{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<a id=\"Content\">HnM RecSys Notebook 9417</a>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": false,
    "tags": []
   },
   "source": [
    "## **<a id=\"Content\">Table of Contents</a>**\n",
    "* [**<span>1. Imports</span>**](#Imports)  \n",
    "* [**<span>2. Helper Functions/Decorators</span>**](#Helper-Functions)\n",
    "* [**<span>5. LightGBM Model</span>**](#LightGBM-Model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# only use last x weeks of transactions data since data is too large\n",
    "def filter_transactions_last_x_weeks(transactions, x = 10):\n",
    "    # Convert date strings to datetime objects\n",
    "    transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "\n",
    "    # Calculate the date x weeks ago from the latest transaction date\n",
    "    latest_date = transactions['t_dat'].max()\n",
    "    cutoff_date = latest_date - timedelta(weeks=x)\n",
    "\n",
    "    # Filter transactions to only include those in the last x weeks\n",
    "    filtered_transactions = transactions.loc[transactions['t_dat'] >= cutoff_date].copy()\n",
    "\n",
    "    return filtered_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_customers_and_articles(customers, articles, filtered_transactions):\n",
    "    # Get unique customer and article IDs from filtered transactions\n",
    "    customer_ids = filtered_transactions['customer_id'].unique()\n",
    "    article_ids = filtered_transactions['article_id'].unique()\n",
    "\n",
    "    # Filter customers and articles to only include those in filtered transactions\n",
    "    customers_filtered = customers.loc[customers['customer_id'].isin(customer_ids)].copy()\n",
    "    articles_filtered = articles.loc[articles['article_id'].isin(article_ids)].copy()\n",
    "\n",
    "    return customers_filtered, articles_filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison of the top GBDT models today. LightGBM is the fastest to train."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|LightGBM|XGBoost|CatBoost|\n",
    "|:----|:----|:----|:----|\n",
    "|Categoricals|Supports categorical features via one-hot encoding|Supports categorical features via one-hot encoding|Automatically handles categorical features using embeddings|\n",
    "|Speed|Very fast training and prediction|Fast training and prediction|Slower than LightGBM and XGBoost|\n",
    "|Handling Bias|Handles unbalanced classes via 'is_unbalance'|Handles unbalanced classes via 'scale_pos_weight'|Automatically handles unbalanced classes|\n",
    "|Handling NaNs|Handles NaN values natively|Requires manual handling of NaNs|Automatically handles NaN values using special category|\n",
    "|Custom Loss|Supports custom loss functions|Supports custom loss functions|Supports custom loss functions|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LightGBM for a ranking problem, we treat this as a binary classification problem where the target variable is whether an item is relevant or not to the user.\n",
    "\n",
    "Alternatively, we can use LightGBM's ranking API, which is designed for ranking problems. Instead of optimizing for accuracy, the ranking API optimizes for ranking metric MAP (MAP support deprecated however). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM imports\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# open user_item_matrix_200\n",
    "with open('user_item_matrix_200.pkl', 'rb') as f:\n",
    "    user_item_matrix = pickle.load(f)\n",
    "\n",
    "# open customer and articels incides map\n",
    "with open('lightgbm/customer_id_indices_map.pkl', 'rb') as f:\n",
    "    customer_id_indices_map = pickle.load(f)\n",
    "\n",
    "with open('lightgbm/article_id_indices_map.pkl', 'rb') as f:\n",
    "    article_id_indices_map = pickle.load(f)\n",
    "\n",
    "# load df from pickle file for time-based split\n",
    "with open('lightgbm/df.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# load final_df from pickle file for clean processing\n",
    "with open('lightgbm/final_df_with_binary_targets.pkl', 'rb') as f:\n",
    "    final_df = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_1</th>\n",
       "      <th>sales_channel_2</th>\n",
       "      <th>quantity</th>\n",
       "      <th>article_engagement_ratio</th>\n",
       "      <th>user_index</th>\n",
       "      <th>item_index</th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>club_member_status</th>\n",
       "      <th>...</th>\n",
       "      <th>garment_group_no_1019.0</th>\n",
       "      <th>garment_group_no_1020.0</th>\n",
       "      <th>garment_group_no_1021.0</th>\n",
       "      <th>garment_group_no_1023.0</th>\n",
       "      <th>garment_group_no_1025.0</th>\n",
       "      <th>index_group_no_1.0</th>\n",
       "      <th>index_group_no_2.0</th>\n",
       "      <th>index_group_no_3.0</th>\n",
       "      <th>index_group_no_4.0</th>\n",
       "      <th>index_group_no_26.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042358</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>11563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050842</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>9899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067810</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>14438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>10307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>10</td>\n",
       "      <td>13608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  sales_channel_1  sales_channel_2  quantity   \n",
       "0  0.042358            False             True       1.0  \\\n",
       "1  0.050842            False             True       1.0   \n",
       "2  0.067810            False             True       1.0   \n",
       "3  0.016937            False             True       1.0   \n",
       "4  0.016937            False             True       1.0   \n",
       "\n",
       "   article_engagement_ratio  user_index  item_index   FN  Active   \n",
       "0                  1.000000           5       11563  1.0     1.0  \\\n",
       "1                  1.000000           5        9899  1.0     1.0   \n",
       "2                  1.000000           5       14438  1.0     1.0   \n",
       "3                  0.500000          10       10307  0.0     0.0   \n",
       "4                  0.166667          10       13608  0.0     0.0   \n",
       "\n",
       "   club_member_status  ...  garment_group_no_1019.0  garment_group_no_1020.0   \n",
       "0                 2.0  ...                    False                    False  \\\n",
       "1                 2.0  ...                    False                    False   \n",
       "2                 2.0  ...                    False                    False   \n",
       "3                 2.0  ...                    False                    False   \n",
       "4                 2.0  ...                    False                    False   \n",
       "\n",
       "   garment_group_no_1021.0  garment_group_no_1023.0  garment_group_no_1025.0   \n",
       "0                    False                    False                    False  \\\n",
       "1                    False                    False                    False   \n",
       "2                    False                    False                    False   \n",
       "3                    False                    False                    False   \n",
       "4                    False                     True                    False   \n",
       "\n",
       "   index_group_no_1.0  index_group_no_2.0  index_group_no_3.0   \n",
       "0                True               False               False  \\\n",
       "1                True               False               False   \n",
       "2                True               False               False   \n",
       "3               False                True               False   \n",
       "4                True               False               False   \n",
       "\n",
       "   index_group_no_4.0  index_group_no_26.0  \n",
       "0               False                False  \n",
       "1               False                False  \n",
       "2               False                False  \n",
       "3               False                False  \n",
       "4               False                False  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_train_test_split(final_df, test_size=0.2):\n",
    "\n",
    "    # Convert days, months, and years columns to datetime object\n",
    "    final_df['date'] = pd.to_datetime(final_df[['day', 'month', 'year']])\n",
    "\n",
    "    # Sort dataframe by date in ascending order\n",
    "    final_df = final_df.sort_values(by='date')\n",
    "\n",
    "    # Calculate cutoff index\n",
    "    cutoff_index = int(len(final_df) * (1-test_size))\n",
    "\n",
    "    # Create train and test dataframes\n",
    "    train_df = final_df[:cutoff_index]\n",
    "    test_df = final_df[cutoff_index:]\n",
    "\n",
    "    # Drop date column from train and test dataframes\n",
    "    train_df = train_df.drop('date', axis=1)\n",
    "    test_df = test_df.drop('date', axis=1)\n",
    "\n",
    "    # split train_df into X_train and y_train\n",
    "    X_train = train_df.drop('target', axis=1)\n",
    "    y_train = train_df['target']\n",
    "\n",
    "    # split test_df into X_test and y_test\n",
    "    X_test = test_df.drop('target', axis=1)\n",
    "    y_test = test_df['target']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6242440, 55)\n",
      "(1560611, 55)\n",
      "(6242440,)\n",
      "(1560611,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = time_based_train_test_split(final_df, test_size=0.2)\n",
    "\n",
    "# drop date column from final_df\n",
    "final_df = final_df.drop('date', axis=1)\n",
    "\n",
    "#print the shape of X_train, X_test, y_train, y_test\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 time-based split to curb data leakage\n",
    "X_train, X_test, y_train, y_test = time_based_train_test_split(final_df, test_size=0.2)\n",
    "final_df = final_df.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import ndcg_score\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.feature_selection import RFECV\n",
    "import joblib\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Define columns to target encode\n",
    "cols_to_encode = ['department_no', 'product_type_no', 'section_no', 'graphical_appearance_no']\n",
    "\n",
    "# Define number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Create KFold object for cross-validation\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform target encoding with cross-validation\n",
    "for col in cols_to_encode:\n",
    "    final_df[f'{col}_te'] = 0\n",
    "    te = TargetEncoder(cols=[col])\n",
    "    for train_idx, val_idx in kf.split(final_df):\n",
    "        te.fit(final_df.iloc[train_idx][[col]], final_df.iloc[train_idx]['target'])\n",
    "        final_df.loc[val_idx, f'{col}_te'] = te.transform(final_df.iloc[val_idx][[col]]).values.flatten()\n",
    "\n",
    "# Define features and target\n",
    "features = final_df.columns.tolist()\n",
    "features.remove('target')\n",
    "target = 'target'\n",
    "\n",
    "# # 80/20 time-based split to curb data leakage\n",
    "# X_train, X_test, y_train, y_test = time_based_train_test_split(final_df, test_size=0.2)\n",
    "# final_df = final_df.drop('date', axis=1)\n",
    "\n",
    "# Group data by user -- so that LightGBM knows which data points belong to each user and can compute the metrics correctly\n",
    "grouped_data_train = X_train.groupby('user_index')\n",
    "grouped_data_test = X_test.groupby('user_index')\n",
    "\n",
    "# Create LightGBM datasets with group query information\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=grouped_data_train.groups.values())\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=grouped_data_test.groups.values())\n",
    "\n",
    "# Define hyperparameters space\n",
    "param_dist = {\n",
    "    'selector__k': stats.randint(10, 50),\n",
    "    'lgbm__learning_rate': stats.uniform(0.01, 0.1),\n",
    "    'lgbm__num_leaves': stats.randint(15, 64),\n",
    "    'lgbm__bagging_fraction': stats.uniform(0.6, 0.4),\n",
    "    'lgbm__feature_fraction': stats.uniform(0.6, 0.4),\n",
    "}\n",
    "\n",
    "# Create a pipeline that includes SelectKBest\n",
    "selector = SelectKBest(score_func=chi2)\n",
    "pipeline = Pipeline(steps=[('selector', selector), ('lgbm', lgb.LGBMClassifier(**params))])\n",
    "\n",
    "# Add num_boost_round parameter to lgbm estimator in pipeline\n",
    "pipeline.named_steps['lgbm'].set_params(num_boost_round=100)\n",
    "\n",
    "# Perform random search on the pipeline\n",
    "scoring = get_scorer('average_precision')\n",
    "clf = RandomizedSearchCV(estimator=pipeline, param_distributions=param_dist, cv=2, scoring=scoring, n_iter=10,\n",
    "                         n_jobs=-1, verbose=2, refit=True)\n",
    "groups = [grouped_data_train.groups[user] for user in grouped_data_train.groups.keys()]\n",
    "groups_flat = np.concatenate(groups)\n",
    "clf.fit(X_train, y_train, groups=groups_flat)\n",
    "\n",
    "# Save the best intermediate model\n",
    "joblib.dump(clf.best_estimator_, 'best_model.pkl')\n",
    "print(f'Best hyperparameters: {clf.best_params_}')\n",
    "print(f'Best map score: {clf.best_score_}')\n",
    "\n",
    "# Save the feature importances to a file\n",
    "feat_importances = pd.Series(clf.best_estimator_.feature_importances_, index=X_train_sel.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh')\n",
    "plt.savefig('lightgbm/feature_importances.png')\n",
    "\n",
    "# Save the selected features\n",
    "selected_features = X_train_sel.columns.tolist()\n",
    "joblib.dump(selected_features, 'lightgbm/selected_features.pkl')\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = clf.best_estimator_.predict(X_test_sel, num_iteration=clf.best_estimator_.best_iteration_)\n",
    "ndcg = ndcg_score(y_test, y_pred, group_scores=True, verbose=1)\n",
    "print(f'NDCG score on test set: {ndcg}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, it can be used to predict the probability of purchase for new user-product pairs, which can be used to generate recommendations for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_popular_products(df, n_products=500):\n",
    "    # Group the dataframe by product and sum the quantity for each product\n",
    "    product_quantities = df.groupby('item_index')['quantity'].sum()\n",
    "    # Sort the products by quantity in descending order and select the top n_products\n",
    "    popular_products = product_quantities.sort_values(ascending=False).index.tolist()[:n_products]\n",
    "    # Filter the dataframe to only include the popular products\n",
    "    df = df[df['item_index'].isin(popular_products)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_1</th>\n",
       "      <th>sales_channel_2</th>\n",
       "      <th>quantity</th>\n",
       "      <th>article_engagement_ratio</th>\n",
       "      <th>user_index</th>\n",
       "      <th>item_index</th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>...</th>\n",
       "      <th>article_preference</th>\n",
       "      <th>item_purchase_frequency</th>\n",
       "      <th>item_avg_price_level</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>recency</th>\n",
       "      <th>frequency</th>\n",
       "      <th>monetary_value</th>\n",
       "      <th>RFM Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.042358</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>11563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042358</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>861</td>\n",
       "      <td>37.391350</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.050842</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>9899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050842</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>861</td>\n",
       "      <td>37.391350</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.067810</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>14438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.067810</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>861</td>\n",
       "      <td>37.391350</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>10307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.016937</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>221</td>\n",
       "      <td>613</td>\n",
       "      <td>17.893959</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>10</td>\n",
       "      <td>13608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.019760</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>221</td>\n",
       "      <td>613</td>\n",
       "      <td>17.893959</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       t_dat     price  sales_channel_1  sales_channel_2  quantity   \n",
       "0 2018-09-20  0.042358            False             True       1.0  \\\n",
       "1 2018-09-20  0.050842            False             True       1.0   \n",
       "2 2018-09-20  0.067810            False             True       1.0   \n",
       "3 2018-09-20  0.016937            False             True       1.0   \n",
       "4 2018-09-20  0.016937            False             True       1.0   \n",
       "\n",
       "   article_engagement_ratio  user_index  item_index   FN  Active  ...   \n",
       "0                  1.000000           5       11563  1.0     1.0  ...  \\\n",
       "1                  1.000000           5        9899  1.0     1.0  ...   \n",
       "2                  1.000000           5       14438  1.0     1.0  ...   \n",
       "3                  0.500000          10       10307  0.0     0.0  ...   \n",
       "4                  0.166667          10       13608  0.0     0.0  ...   \n",
       "\n",
       "   article_preference  item_purchase_frequency  item_avg_price_level  year   \n",
       "0                 0.0                 0.000000              0.042358  2018  \\\n",
       "1                 0.0                 0.000000              0.050842  2018   \n",
       "2                 1.0                 1.000000              0.067810  2018   \n",
       "3                 1.0                 2.500000              0.016937  2018   \n",
       "4                 1.0                 0.666667              0.019760  2018   \n",
       "\n",
       "   month  day  recency  frequency monetary_value  RFM Score  \n",
       "0      9   20        6        861      37.391350        244  \n",
       "1      9   20        6        861      37.391350        244  \n",
       "2      9   20        6        861      37.391350        244  \n",
       "3      9   20      221        613      17.893959        132  \n",
       "4      9   20      221        613      17.893959        132  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X is the input data for the LightGBM model\n",
    "# X has a row for each user-product pair and a binary target indicating whether the user purchased the product or not\n",
    "\n",
    "# Train the LightGBM model on X\n",
    "# lgb_model = lgb.LGBMClassifier(**best_params)\n",
    "# lgb_model.fit(X, y)\n",
    "\n",
    "# Generate candidate products for each user\n",
    "# This can be done using a combination of popular products and user purchase history\n",
    "# Let's assume we have a dictionary 'user_products' that maps each user ID to a list of products they've purchased\n",
    "user_candidates = {}\n",
    "for user_id in user_products:\n",
    "    # Select the 600 most popular products\n",
    "    popular_products = select_popular_products(500)\n",
    "    \n",
    "    # Add user purchase history to candidate list\n",
    "    user_history = user_products[user_id]\n",
    "    candidate_products = list(set(popular_products + user_history))\n",
    "    \n",
    "    # Store candidate products for this user\n",
    "    user_candidates[user_id] = candidate_products\n",
    "\n",
    "# Predict probabilities of purchase for each candidate product for each user\n",
    "user_scores = {}\n",
    "for user_id, candidates in user_candidates.items():\n",
    "    # Create input data for this user\n",
    "    user_data = create_user_data(user_id, candidates)\n",
    "    \n",
    "    # Predict probabilities using the LightGBM model\n",
    "    scores = lgb_model.predict_proba(user_data)[:, 1]\n",
    "    \n",
    "    # Store scores for this user\n",
    "    user_scores[user_id] = scores\n",
    "\n",
    "# Rank candidate products for each user and return top 12 as recommendations\n",
    "recommendations = {}\n",
    "for user_id, scores in user_scores.items():\n",
    "    # Sort candidate products by descending score\n",
    "    candidate_products = user_candidates[user_id]\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_products = [candidate_products[i] for i in sorted_indices]\n",
    "    \n",
    "    # Select top 12 products\n",
    "    top_products = sorted_products[:12]\n",
    "    \n",
    "    # Add user purchase history to top products\n",
    "    top_products += user_products[user_id]\n",
    "    \n",
    "    # Remove duplicates and return as recommendations\n",
    "    recommendations[user_id] = list(set(top_products))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat this as a binary classification problem, we would be ignoring the importance of the ranking of the recommended items and the MAP metric would not be appropriate. Since we are using MAP as the evaluation metric, we should use the LightGBM ranking API instead of the binary classification API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import make_scorer\n",
    "# from sklearn.metrics import average_precision_score\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# target = 'item_index'\n",
    "# features = final_df.columns.tolist()\n",
    "# features.remove(target)\n",
    "\n",
    "# # split the data into training and test sets -- can also do time-based split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(final_df[features], final_df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# # for number of items to rank for each user (group param for ordered ranking)\n",
    "# num_items_per_user = 12\n",
    "# user_indices = X_test.index.unique()\n",
    "# query = [num_items_per_user] * len(user_indices)\n",
    "# query_ids = []\n",
    "# for user_index in user_indices:\n",
    "#     user_indices_repeated = [user_index] * num_items_per_user\n",
    "#     query_ids.extend(user_indices_repeated)\n",
    "\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, group=query_ids)\n",
    "\n",
    "# # MAP@12 metric\n",
    "# def mean_average_precision(y_true, y_score, k=12):\n",
    "#     # get the indices of the top k scores\n",
    "#     top_k_indices = np.argsort(y_score)[::-1][:k]\n",
    "\n",
    "#     # calculate average precision at k\n",
    "#     return average_precision_score(y_true[top_k_indices], y_score[top_k_indices])\n",
    "\n",
    "# # define hyperparameters for tuning\n",
    "# params = {\n",
    "# 'objective': 'lambdarank', #using lightgbm ranking API\n",
    "# 'metric': 'MAP',\n",
    "# 'learning_rate': 0.05,\n",
    "# 'num_leaves': 31,\n",
    "# 'max_depth': 5,\n",
    "# 'min_data_in_leaf': 50,\n",
    "# 'feature_fraction': 0.8,\n",
    "# 'bagging_fraction': 0.8,\n",
    "# 'bagging_freq': 5\n",
    "# }\n",
    "\n",
    "# # create LightGBM model\n",
    "# model = lgb.LGBMRanker()\n",
    "\n",
    "# # perform grid search with cross-validation\n",
    "# param_grid = {\n",
    "# 'num_leaves': [31, 50, 75],\n",
    "# 'max_depth': [5, 7, -1],\n",
    "# 'min_data_in_leaf': [20, 50, 100],\n",
    "# 'feature_fraction': [0.6, 0.8, 1],\n",
    "# 'bagging_fraction': [0.6, 0.8, 1],\n",
    "# 'bagging_freq': [1, 3, 5],\n",
    "# 'lambda_l1': [0, 1, 2],\n",
    "# 'lambda_l2': [0, 1, 2]\n",
    "# }\n",
    "\n",
    "# best_map_score = 0.0\n",
    "# best_model = None\n",
    "\n",
    "# for params_dict in ParameterGrid(param_grid):\n",
    "#     params.update(params_dict)\n",
    "#     model = lgb.train(params, train_data)\n",
    "#     y_pred = model.predict(X_test, group=query)\n",
    "#     map_score = mean_average_precision(y_test, y_pred, k=12)\n",
    "#     if map_score > best_map_score:\n",
    "#         best_map_score = map_score\n",
    "#         best_model = model\n",
    "#         with open(f\"lightgbm/grid_search_model_{map_score:.4f}.pickle\", 'wb') as f:\n",
    "#             pickle.dump(model, f)\n",
    "\n",
    "# # save the best model\n",
    "# if not os.path.exists('lightgbm'):\n",
    "#     os.makedirs('lightgbm')\n",
    "# with open('lightgbm/best_model.pickle', 'wb') as f:\n",
    "#     pickle.dump(best_model, f)\n",
    "\n",
    "# # print the best MAP score\n",
    "# print(f\"Best mean average precision: {best_map_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
