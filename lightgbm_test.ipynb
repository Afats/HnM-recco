{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<a id=\"Content\">HnM RecSys Notebook 9417</a>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": false,
    "tags": []
   },
   "source": [
    "## **<a id=\"Content\">Table of Contents</a>**\n",
    "* [**<span>1. Imports</span>**](#Imports)  \n",
    "* [**<span>2. Helper Functions/Decorators</span>**](#Helper-Functions)\n",
    "* [**<span>5. LightGBM Model</span>**](#LightGBM-Model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison of the top GBDT models today. LightGBM is the fastest to train."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|LightGBM|XGBoost|CatBoost|\n",
    "|:----|:----|:----|:----|\n",
    "|Categoricals|Supports categorical features via one-hot encoding|Supports categorical features via one-hot encoding|Automatically handles categorical features using embeddings|\n",
    "|Speed|Very fast training and prediction|Fast training and prediction|Slower than LightGBM and XGBoost|\n",
    "|Handling Bias|Handles unbalanced classes via 'is_unbalance'|Handles unbalanced classes via 'scale_pos_weight'|Automatically handles unbalanced classes|\n",
    "|Handling NaNs|Handles NaN values natively|Requires manual handling of NaNs|Automatically handles NaN values using special category|\n",
    "|Custom Loss|Supports custom loss functions|Supports custom loss functions|Supports custom loss functions|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# open user_item_matrix_200\n",
    "with open('user_item_matrix_200.pkl', 'rb') as f:\n",
    "    user_item_matrix = pickle.load(f)\n",
    "\n",
    "# open customer and articels incides map\n",
    "with open('lightgbm/customer_id_indices_map.pkl', 'rb') as f:\n",
    "    customer_id_indices_map = pickle.load(f)\n",
    "\n",
    "with open('lightgbm/article_id_indices_map.pkl', 'rb') as f:\n",
    "    article_id_indices_map = pickle.load(f)\n",
    "\n",
    "# load df from pickle file for time-based split\n",
    "with open('lightgbm/df.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# load final_df from pickle file for clean processing\n",
    "with open('lightgbm/final_df_with_binary_targets.pkl', 'rb') as f:\n",
    "    final_df = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_1</th>\n",
       "      <th>sales_channel_2</th>\n",
       "      <th>quantity</th>\n",
       "      <th>article_engagement_ratio</th>\n",
       "      <th>user_index</th>\n",
       "      <th>item_index</th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>club_member_status</th>\n",
       "      <th>...</th>\n",
       "      <th>garment_group_no_1019.0</th>\n",
       "      <th>garment_group_no_1020.0</th>\n",
       "      <th>garment_group_no_1021.0</th>\n",
       "      <th>garment_group_no_1023.0</th>\n",
       "      <th>garment_group_no_1025.0</th>\n",
       "      <th>index_group_no_1.0</th>\n",
       "      <th>index_group_no_2.0</th>\n",
       "      <th>index_group_no_3.0</th>\n",
       "      <th>index_group_no_4.0</th>\n",
       "      <th>index_group_no_26.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042358</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>11563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050842</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>9899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067810</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>14438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>10307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>10</td>\n",
       "      <td>13608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  sales_channel_1  sales_channel_2  quantity   \n",
       "0  0.042358            False             True       1.0  \\\n",
       "1  0.050842            False             True       1.0   \n",
       "2  0.067810            False             True       1.0   \n",
       "3  0.016937            False             True       1.0   \n",
       "4  0.016937            False             True       1.0   \n",
       "\n",
       "   article_engagement_ratio  user_index  item_index   FN  Active   \n",
       "0                  1.000000           5       11563  1.0     1.0  \\\n",
       "1                  1.000000           5        9899  1.0     1.0   \n",
       "2                  1.000000           5       14438  1.0     1.0   \n",
       "3                  0.500000          10       10307  0.0     0.0   \n",
       "4                  0.166667          10       13608  0.0     0.0   \n",
       "\n",
       "   club_member_status  ...  garment_group_no_1019.0  garment_group_no_1020.0   \n",
       "0                 2.0  ...                    False                    False  \\\n",
       "1                 2.0  ...                    False                    False   \n",
       "2                 2.0  ...                    False                    False   \n",
       "3                 2.0  ...                    False                    False   \n",
       "4                 2.0  ...                    False                    False   \n",
       "\n",
       "   garment_group_no_1021.0  garment_group_no_1023.0  garment_group_no_1025.0   \n",
       "0                    False                    False                    False  \\\n",
       "1                    False                    False                    False   \n",
       "2                    False                    False                    False   \n",
       "3                    False                    False                    False   \n",
       "4                    False                     True                    False   \n",
       "\n",
       "   index_group_no_1.0  index_group_no_2.0  index_group_no_3.0   \n",
       "0                True               False               False  \\\n",
       "1                True               False               False   \n",
       "2                True               False               False   \n",
       "3               False                True               False   \n",
       "4                True               False               False   \n",
       "\n",
       "   index_group_no_4.0  index_group_no_26.0  \n",
       "0               False                False  \n",
       "1               False                False  \n",
       "2               False                False  \n",
       "3               False                False  \n",
       "4               False                False  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # target encoding\n",
    "# from category_encoders import TargetEncoder\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# # Define columns to target encode\n",
    "# cols_to_encode = ['department_no', 'product_type_no', 'section_no', 'graphical_appearance_no']\n",
    "\n",
    "# # Define number of folds for cross-validation\n",
    "# n_splits = 5\n",
    "\n",
    "# # Create KFold object for cross-validation\n",
    "# kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# # Perform target encoding with cross-validation\n",
    "# for col in cols_to_encode:\n",
    "#     final_df[f'{col}_te'] = 0\n",
    "#     te = TargetEncoder(cols=[col])\n",
    "#     for train_idx, val_idx in kf.split(final_df):\n",
    "#         te.fit(final_df.iloc[train_idx][[col]], final_df.iloc[train_idx]['target'])\n",
    "#         final_df.loc[val_idx, f'{col}_te'] = te.transform(final_df.iloc[val_idx][[col]]).values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- memory optimizations -------------\n",
    "\n",
    "# reference: https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\n",
    "\n",
    "# iterate through all the columns of a dataframe and reduce the int and float data types to the smallest possible size, ex. customer_id should not be reduced from int64 to a samller value as it would have collisions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"Iterate over all the columns of a DataFrame and modify the data type\n",
    "    to reduce memory usage, handling ordered Categoricals\"\"\"\n",
    "    \n",
    "    # check the memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type == 'category':\n",
    "            if df[col].cat.ordered:\n",
    "                # Convert ordered Categorical to an integer\n",
    "                df[col] = df[col].cat.codes.astype('int16')\n",
    "            else:\n",
    "                # Convert unordered Categorical to a string\n",
    "                df[col] = df[col].astype('str')\n",
    "        \n",
    "        elif col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    # check the memory usage after optimization\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "\n",
    "    # calculate the percentage of the memory usage reduction\n",
    "    mem_reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(\"Memory usage decreased by {:.1f}%\".format(mem_reduction))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1952211, 56)\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# only get top 50 customers by number of total pruchase quantity from final_df\n",
    "\n",
    "# Compute the total quantity for each user_index\n",
    "user_quantity = final_df.groupby('user_index')['quantity'].sum()\n",
    "\n",
    "# Get the top 50 user_indices by total quantity\n",
    "top_50_users = user_quantity.nlargest(50).index\n",
    "\n",
    "# Filter the final_df to include only the data for the top 50 users\n",
    "final_df_top_50 = final_df[final_df['user_index'].isin(top_50_users)].copy()\n",
    "# print the shape of final_df_top_50\n",
    "print(final_df_top_50.shape)\n",
    "\n",
    "print(final_df_top_50['user_index'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 126622, number of negative: 7676429\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.326515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2289\n",
      "[LightGBM] [Info] Number of data points in the train set: 7803051, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016227 -> initscore=-4.104703\n",
      "[LightGBM] [Info] Start training from score -4.104703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy:  1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'candidate_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m user_df \u001b[39m=\u001b[39m final_df[final_df[\u001b[39m'\u001b[39m\u001b[39muser_index\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m user]\n\u001b[0;32m     39\u001b[0m item_indices \u001b[39m=\u001b[39m user_df[\u001b[39m'\u001b[39m\u001b[39mitem_index\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n\u001b[1;32m---> 40\u001b[0m candidate_items \u001b[39m=\u001b[39m candidate_df[candidate_df[\u001b[39m'\u001b[39m\u001b[39mitem_index\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(item_indices)]\n\u001b[0;32m     41\u001b[0m candidate_X \u001b[39m=\u001b[39m candidate_items\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mitem_index\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     42\u001b[0m candidate_y \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(candidate_X)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'candidate_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocessing\n",
    "final_df = final_df.fillna(-1)\n",
    "X = final_df.drop(['target'], axis=1)\n",
    "y = final_df['target']\n",
    "dtrain = lgb.Dataset(X, label=y)\n",
    "\n",
    "# Train-Test split\n",
    "if 'date' in final_df.columns:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=X['date'])\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Model Training\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "}\n",
    "clf = lgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_binary = [1 if x >= 0.5 else 0 for x in y_pred]\n",
    "accuracy = sum(y_pred_binary == y_test) / len(y_test)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Prediction\n",
    "users = final_df['user_index'].unique()\n",
    "predictions = []\n",
    "for user in users:\n",
    "    user_df = final_df[final_df['user_index'] == user]\n",
    "    item_indices = user_df['item_index'].unique()\n",
    "    candidate_items = candidate_df[candidate_df['item_index'].isin(item_indices)]\n",
    "    candidate_X = candidate_items.drop(['item_index'], axis=1)\n",
    "    candidate_y = clf.predict(candidate_X)\n",
    "    candidate_items['prob'] = candidate_y\n",
    "    top_items = candidate_items.sort_values(by=['prob'], ascending=False).iloc[:12]\n",
    "    predictions.append(top_items['item_index'].values)\n",
    "\n",
    "# Output\n",
    "output_df = pd.DataFrame({\n",
    "    'user_index': users,\n",
    "    'item_index_1': [x[0] for x in predictions],\n",
    "    'item_index_2': [x[1] for x in predictions],\n",
    "    'item_index_3': [x[2] for x in predictions]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "import joblib\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = final_df.columns.tolist()\n",
    "features.remove('target')\n",
    "target = 'target'\n",
    "\n",
    "# Group data by user -- so that LightGBM knows which data points belong to each user and can compute the metrics correctly\n",
    "grouped_data_train = X_train.groupby('user_index')\n",
    "grouped_data_test = X_test.groupby('user_index')\n",
    "groups_train = [grouped_data_train.groups[user] for user in grouped_data_train.groups.keys()]\n",
    "groups_train_flat = np.concatenate(groups_train)\n",
    "groups_test = [grouped_data_test.groups[user] for user in grouped_data_test.groups.keys()]\n",
    "\n",
    "# Create LightGBM datasets with group query information\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=grouped_data_train.groups.values())\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=grouped_data_test.groups.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 5 features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RFECV(cv=5, estimator=LGBMClassifier(max_depth=7),\n",
       "      scoring=make_scorer(average_precision_score, needs_proba=True), step=5,\n",
       "      verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RFECV</label><div class=\"sk-toggleable__content\"><pre>RFECV(cv=5, estimator=LGBMClassifier(max_depth=7),\n",
       "      scoring=make_scorer(average_precision_score, needs_proba=True), step=5,\n",
       "      verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(max_depth=7)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(max_depth=7)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RFECV(cv=5, estimator=LGBMClassifier(max_depth=7),\n",
       "      scoring=make_scorer(average_precision_score, needs_proba=True), step=5,\n",
       "      verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "\n",
    "pr_auc_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Feature selection using RFECV\n",
    "selector = RFECV(\n",
    "    estimator=lgb.LGBMClassifier(n_jobs=-1,\n",
    "        num_leaves=31, max_depth=7, learning_rate=0.1\n",
    "    ),\n",
    "    cv=5, scoring=pr_auc_scorer,\n",
    "    verbose=1, step=5\n",
    ")\n",
    "selector.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['price'], dtype='object')\n",
      "[('price', 1), ('article_engagement_ratio', 2), ('user_index', 2), ('item_index', 2), ('item_avg_price_level', 2), ('quantity', 3), ('user_purchase_quant', 3), ('department_no', 3), ('age_diff', 3), ('mean_purchase_age', 3), ('age', 4), ('product_type_no', 4), ('max_purchase_age', 4), ('min_purchase_age', 4), ('item_purchase_frequency', 4), ('sales_channel_1', 5), ('sales_channel_2', 5), ('club_member_status', 5), ('time_diff_days', 5), ('garment_group_no_1013.0', 5), ('FN', 6), ('Active', 6), ('garment_group_no_1012.0', 6), ('garment_group_no_1014.0', 6), ('garment_group_no_1020.0', 6), ('fashion_news_frequency', 7), ('graphical_appearance_no', 7), ('garment_group_no_1007.0', 7), ('index_group_no_2.0', 7), ('index_group_no_3.0', 7), ('section_no', 8), ('garment_group_no_1021.0', 8), ('garment_group_no_1023.0', 8), ('garment_group_no_1025.0', 8), ('index_group_no_1.0', 8), ('garment_group_no_1016.0', 9), ('garment_group_no_1017.0', 9), ('garment_group_no_1018.0', 9), ('garment_group_no_1019.0', 9), ('index_group_no_4.0', 9), ('article_preference', 10), ('garment_group_no_1008.0', 10), ('garment_group_no_1009.0', 10), ('garment_group_no_1010.0', 10), ('garment_group_no_1011.0', 10), ('year', 11), ('month', 11), ('RFM_Score', 11), ('garment_group_no_1006.0', 11), ('index_group_no_26.0', 11), ('day', 12), ('garment_group_no_1001.0', 12), ('garment_group_no_1002.0', 12), ('garment_group_no_1003.0', 12), ('garment_group_no_1005.0', 12)]\n"
     ]
    }
   ],
   "source": [
    "# Get selected features\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "print(selected_features)\n",
    "\n",
    "ranks = selector.ranking_\n",
    "feat_ranks = {feat:rank for feat, rank in zip(X_train.columns, ranks)}\n",
    "sorted_ranks = sorted(feat_ranks.items(), key=lambda x: x[1])\n",
    "print(sorted_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price', 'article_engagement_ratio', 'user_index', 'item_index', 'item_avg_price_level', 'quantity', 'user_purchase_quant', 'department_no', 'age_diff', 'mean_purchase_age', 'age', 'product_type_no', 'max_purchase_age', 'min_purchase_age', 'item_purchase_frequency', 'sales_channel_1', 'sales_channel_2', 'club_member_status', 'time_diff_days', 'garment_group_no_1013.0']\n"
     ]
    }
   ],
   "source": [
    "# # Get selected features\n",
    "# selected_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "# select only the top  x features\n",
    "selected_features = [feat for feat, rank in sorted_ranks if rank <= 5]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LGBMClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.01, 0.001],\n",
       "                         &#x27;max_depth&#x27;: [3, 5, 7], &#x27;num_leaves&#x27;: [31, 63, 127]},\n",
       "             scoring=make_scorer(average_precision_score, needs_proba=True),\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LGBMClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.01, 0.001],\n",
       "                         &#x27;max_depth&#x27;: [3, 5, 7], &#x27;num_leaves&#x27;: [31, 63, 127]},\n",
       "             scoring=make_scorer(average_precision_score, needs_proba=True),\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LGBMClassifier(),\n",
       "             param_grid={'learning_rate': [0.1, 0.01, 0.001],\n",
       "                         'max_depth': [3, 5, 7], 'num_leaves': [31, 63, 127]},\n",
       "             scoring=make_scorer(average_precision_score, needs_proba=True),\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get integer indices of selected features\n",
    "selected_feature_indices = [X_train.columns.get_loc(col) for col in selected_features]\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 63, 127],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "}\n",
    "\n",
    "# Create an instance of the LGBMClassifier\n",
    "lgbm = lgb.LGBMClassifier()\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=5, scoring=pr_auc_scorer, verbose=1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train_scaled[:, selected_feature_indices], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'num_leaves': 31}\n",
      "Best Score: 1.0\n",
      "PR-AUC Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# Define PR-AUC scorer\n",
    "def pr_auc_score(y_true, y_pred):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "# Use PR-AUC scorer to make_scorer object\n",
    "pr_auc_scorer = make_scorer(pr_auc_score)\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = grid_search.best_estimator_.predict_proba(X_test_scaled[:, selected_feature_indices])[:, 1]\n",
    "\n",
    "# Evaluate the model using PR-AUC\n",
    "pr_auc = pr_auc_score(y_test, y_pred)\n",
    "print(\"PR-AUC Score:\", pr_auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, it can be used to predict the probability of purchase for new user-product pairs, which can be used to generate recommendations for users."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat this as a binary classification problem: After training the model, we can then get the probability that each user is likely to purchase an item from a candidate set of items. We can then sort these by descending probability to get the top 12 products as done below. <br>\n",
    "\n",
    "A heuristic apparoach that we use to enhance LighGBM predictions here: <br>\n",
    "1. Get a candidate set of top 500 most popular articles (by total purchase quanitity). <br>\n",
    "2. Include the customer's predicitons to this set. <br>\n",
    "3. Use lightGBM to predict the probability of purchases, and get the top 12. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary 'user_products' that maps each user ID to a list of products they've purchased from the user-item matrix\n",
    "\n",
    "user_products = {}\n",
    "for user_idx in range(user_item_matrix.shape[0]):\n",
    "    purchased_items = list(np.where(user_item_matrix[user_idx, :].toarray()[0] == 1)[0])\n",
    "    user_products[user_idx] = purchased_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class PULearner(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator, hold_out_ratio=0.1):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.hold_out_ratio = hold_out_ratio\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        n_pos = y.sum()\n",
    "        n_unlabeled = y.size - n_pos\n",
    "        n_neg_to_select = int(n_pos / (1 - self.hold_out_ratio)) - n_pos\n",
    "\n",
    "        y_unlabeled = np.zeros(n_unlabeled)\n",
    "        y_hold_out = np.concatenate([np.ones(n_pos), y_unlabeled])\n",
    "\n",
    "        # shuffle the data and split it into the hold-out set and the remaining unlabeled set\n",
    "        idx = np.random.permutation(y.size)\n",
    "        X_hold_out, X_unlabeled = np.array_split(X[idx], [n_pos])\n",
    "        _, y_hold_out = np.array_split(y[idx], [n_pos])\n",
    "        _, y_unlabeled = np.array_split(y_hold_out, [n_pos])\n",
    "        \n",
    "        # fit the base estimator on the unlabeled set\n",
    "        self.base_estimator.fit(X_unlabeled, y_unlabeled)\n",
    "\n",
    "        # predict on the hold-out set and select the most confident negatives\n",
    "        y_hold_out_pred = self.base_estimator.predict_proba(X_hold_out)[:, 1]\n",
    "        y_hold_out_pred_neg = y_hold_out_pred[y_hold_out == 0]\n",
    "        sorted_idx = np.argsort(y_hold_out_pred_neg)[::-1]\n",
    "        neg_idx = sorted_idx[:n_neg_to_select]\n",
    "        neg_mask = np.zeros_like(y_hold_out, dtype=bool)\n",
    "        neg_mask[y_hold_out == 0][neg_idx] = True\n",
    "\n",
    "        # set the hold-out negatives as unlabeled\n",
    "        y_unlabeled[neg_mask] = -1\n",
    "\n",
    "        # fit the base estimator on the new PU set\n",
    "        self.base_estimator.fit(X, y_unlabeled)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        return self.base_estimator.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        return self.base_estimator.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m pu_model \u001b[39m=\u001b[39m PULearner(base_estimator\u001b[39m=\u001b[39mlgb\u001b[39m.\u001b[39mLGBMClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgrid_search\u001b[39m.\u001b[39mbest_params_))\n\u001b[0;32m      4\u001b[0m \u001b[39m# Fit the model on the training set\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m pu_model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_pred \u001b[39m=\u001b[39m pu_model\u001b[39m.\u001b[39mpredict_proba(X_test)[:, \u001b[39m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[38], line 11\u001b[0m, in \u001b[0;36mPULearner.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[1;32m---> 11\u001b[0m     X, y \u001b[39m=\u001b[39m check_X_y(X, y)\n\u001b[0;32m     12\u001b[0m     n_pos \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39msum()\n\u001b[0;32m     13\u001b[0m     n_unlabeled \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39msize \u001b[39m-\u001b[39m n_pos\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN."
     ]
    }
   ],
   "source": [
    "# Initialize the PUClassifier with LightGBM as the base estimator\n",
    "pu_model = PULearner(base_estimator=lgb.LGBMClassifier(**grid_search.best_params_))\n",
    "\n",
    "# Fit the model on the training set\n",
    "pu_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pu_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the performance of the model using AUC-PRC\n",
    "auc_prc = pr_auc_score(y_test, y_pred)\n",
    "print(\"AUC-PRC Score:\", auc_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LightGBM model on X\n",
    "\n",
    "#drop target and columns not in selected_features\n",
    "X = final_df.drop(['target'], axis=1)\n",
    "X = X[selected_features]\n",
    "y = final_df['target']\n",
    "lgb_model = lgb.LGBMClassifier(**grid_search.best_params_)\n",
    "lgb_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns set of most pupular products in the catalog\n",
    "\n",
    "def select_popular_products(df, n_products=500):\n",
    "    # Group the dataframe by user and product and sum the quantity for each group\n",
    "    product_quantities = df.groupby(['user_index', 'item_index'])['quantity'].sum()\n",
    "    # Sort the products by quantity in descending order and select the top n_products\n",
    "    popular_products = product_quantities.groupby('item_index').sum().sort_values(ascending=False).index.tolist()[:n_products]\n",
    "    # return only the unique item_index values\n",
    "    return list(set(popular_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate products for each user\n",
    "# This can be done using a combination of popular products and user purchase history\n",
    "\n",
    "popular_products = select_popular_products(final_df, 500)\n",
    "print(len(popular_products))\n",
    "# print first 10 popular products\n",
    "print(popular_products[:20])\n",
    "\n",
    "for user_id in user_products:\n",
    "    \n",
    "    # Add user purchase history to candidate list\n",
    "    user_history = user_products[user_id]\n",
    "    candidate_products = list(set(popular_products + user_history))\n",
    "    \n",
    "    # Store candidate products for this user\n",
    "    user_candidates[user_id] = candidate_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_df = final_df.copy()\n",
    "\n",
    "test_final_df.groupby(['user_index', 'item_index'])[selected_features].mean().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_data(user_id, candidates, selected_features):\n",
    "    user_data = final_df.groupby(['user_index', 'item_index'])[selected_features].mean().copy()\n",
    "    user_data.fillna(0, inplace=True)\n",
    "    \n",
    "    return user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities of purchase for each candidate product for each user\n",
    "user_scores = {}\n",
    "for user_id, candidates in user_candidates.items():\n",
    "    # Create input data for this user\n",
    "    user_data = create_user_data(user_id, candidates, selected_features)\n",
    "    \n",
    "    # Predict probabilities using the LightGBM model -- slicing for prob of positive class, a.k.a. prob. of purchase\n",
    "    scores = lgb_model.predict_proba(user_data)[:, 1]\n",
    "    \n",
    "    # Store scores for this user\n",
    "    user_scores[user_id] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 2 users and their scores\n",
    "for user_id, scores in list(user_scores.items())[:2]:\n",
    "    print(user_id, scores)\n",
    "    # print number of unique values in scores\n",
    "    print(len(np.unique(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank candidate products for each user and return top 12 as recommendations\n",
    "recommendations = {}\n",
    "for user_id, scores in user_scores.items():\n",
    "    # Sort candidate products by descending score\n",
    "    candidate_products = user_candidates[user_id]\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_products = [candidate_products[i] for i in sorted_indices]\n",
    "    \n",
    "    # Select top 12 products\n",
    "    top_products = sorted_products[:12]\n",
    "    \n",
    "    # Add user purchase history to top products\n",
    "    top_products += user_products[user_id]\n",
    "    \n",
    "    # Remove duplicates and return as recommendations\n",
    "    recommendations[user_id] = list(set(top_products))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using MAP as the evaluation metric, we could also use the LightGBM ranking API instead of the binary classification API. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
