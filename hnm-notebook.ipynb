{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<a id=\"Content\">HnM RecSys Notebook 9417</a>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": false,
    "tags": []
   },
   "source": [
    "## **<a id=\"Content\">Table of Contents</a>**\n",
    "* [**<span>1. Imports</span>**](#Imports)  \n",
    "* [**<span>2. Pre-Processing</span>**](#Pre-Processing)\n",
    "* [**<span>3. Exploratory Data Analysis</span>**](#Exploratory-Data-Analysis)  \n",
    "    * [**<span>3.1 Articles</span>**](#EDA::Articles)  \n",
    "    * [**<span>3.2 Customers</span>**](#EDA::Customers)\n",
    "    * [**<span>3.3 Transactions</span>**](#EDA::Transactions)\n",
    "* [**<span>4. Helper FunctionsDecorators</span>**](#Helper-Functions)\n",
    "* [**<span>5. Models</span>**](#Models) \n",
    "    * [**<span>5.1 Popularity</span>**](#Popularity-Model)   \n",
    "    * [**<span>5.2 ALS</span>**](#Alternating-Least-Squares)  \n",
    "    * [**<span>5.2 GBDT</span>**](#GBDT)  \n",
    "    * [**<span>5.3 SGD/similar</span>**](#SGD)  \n",
    "    * [**<span>5.4 NN</span>**](#NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "# import cudf # switch on P100 GPU for this to work in Kaggle\n",
    "# import cupy as cp\n",
    "\n",
    "# Importing data\n",
    "articles = pd.read_csv('articles.csv')\n",
    "print(articles.head())\n",
    "print(\"--\")\n",
    "customers = pd.read_csv('customers.csv')\n",
    "print(customers.head())\n",
    "print(\"--\")\n",
    "transactions = pd.read_csv(\"transactions_train.csv\")\n",
    "print(transactions.head())\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- empty value stats -------------\n",
    "print(\"Missing values: \")\n",
    "print(customers.isnull().sum())\n",
    "print(\"--\\n\")\n",
    "\n",
    "print(\"FN Newsletter vals: \", customers['FN'].unique())\n",
    "print(\"Active communication vals: \",customers['Active'].unique())\n",
    "print(\"Club member status vals: \", customers['club_member_status'].unique())\n",
    "print(\"Fashion News frequency vals: \", customers['fashion_news_frequency'].unique())\n",
    "print(\"--\\n\")\n",
    "\n",
    "# ---- data cleaning -------------\n",
    "\n",
    "customers['FN'] = customers['FN'].fillna(0)\n",
    "customers['Active'] = customers['Active'].fillna(0)\n",
    "\n",
    "# replace club_member_status missing values with 'LEFT CLUB' --> no members with LEFT CLUB status in data\n",
    "customers['club_member_status'] = customers['club_member_status'].fillna('LEFT CLUB')\n",
    "customers['fashion_news_frequency'] = customers['fashion_news_frequency'].fillna('None')\n",
    "customers['fashion_news_frequency'] = customers['fashion_news_frequency'].replace('NONE', 'None')\n",
    "customers['age'] = customers['age'].fillna(customers['age'].mean())\n",
    "customers['age'] = customers['age'].astype(int)\n",
    "articles['detail_desc'] = articles['detail_desc'].fillna('None')\n",
    "\n",
    "\n",
    "print(\"Customers' Missing values: \")\n",
    "print(customers.isnull().sum())\n",
    "print(\"--\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- memory optimizations -------------\n",
    "\n",
    "# reference: https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\n",
    "\n",
    "# iterate through all the columns of a dataframe and reduce the int and float data types to the smallest possible size, ex. customer_id should not be reduced from int64 to a samller value as it would have collisions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"Iterate over all the columns of a DataFrame and modify the data type\n",
    "    to reduce memory usage, handling ordered Categoricals\"\"\"\n",
    "    \n",
    "    # check the memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type == 'category':\n",
    "            if df[col].cat.ordered:\n",
    "                # Convert ordered Categorical to an integer\n",
    "                df[col] = df[col].cat.codes.astype('int16')\n",
    "            else:\n",
    "                # Convert unordered Categorical to a string\n",
    "                df[col] = df[col].astype('str')\n",
    "        \n",
    "        elif col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    # check the memory usage after optimization\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "\n",
    "    # calculate the percentage of the memory usage reduction\n",
    "    mem_reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(\"Memory usage decreased by {:.1f}%\".format(mem_reduction))\n",
    "    \n",
    "    return df\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Articles Info: \")\n",
    "print(articles.info())\n",
    "print(\"Customer Info: \")\n",
    "print(customers.info())\n",
    "print(\"Transactions Info: \")\n",
    "print(transactions.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print unique values of customer columns\n",
    "print(\"FN Newsletter vals: \", customers['FN'].unique())\n",
    "print(\"Active communication vals: \",customers['Active'].unique())\n",
    "print(\"Club member status vals: \", customers['club_member_status'].unique())\n",
    "print(\"Fashion News frequency vals: \", customers['fashion_news_frequency'].unique())\n",
    "print(\"--\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly convert club_member_status to ordinal values before mem optimization to avoid errors\n",
    "\n",
    "customers['club_member_status'].replace({'LEFT CLUB': 0, 'PRE-CREATE': 1, 'ACTIVE': 2}, inplace=True)\n",
    "customers['club_member_status'] = customers['club_member_status'].astype('int8')\n",
    "print(customers['club_member_status'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- memory optimizations -------------\n",
    "\n",
    "# uses 8 bytes instead of given 64 byte string, reduces mem by 8x, \n",
    "# !!!! have to convert back before merging w/ sample_submissions.csv\n",
    "# convert transactions['customer_id'] to 8 bytes int\n",
    "# transactions['customer_id'] = transactions['customer_id'].astype('int64')\n",
    "transactions['customer_id'] = transactions['customer_id'].apply(lambda x: int(x[-16:], 16)).astype('int64')\n",
    "customers['customer_id'] = customers['customer_id'].apply(lambda x: int(x[-16:], 16)).astype('int64')\n",
    "\n",
    "articles = reduce_mem_usage(articles)\n",
    "customers = reduce_mem_usage(customers)\n",
    "transactions = reduce_mem_usage(transactions)\n",
    "\n",
    "# articles['article_id'] = articles['article_id'].astype('int32')\n",
    "# transactions['article_id'] = transactions['article_id'].astype('int32') \n",
    "# # !!!! ADD LEADING ZERO BACK BEFORE SUBMISSION OF PREDICTIONS TO KAGGLE: \n",
    "# # Ex.: transactions['article_id'] = '0' + transactions.article_id.astype('str')\n",
    "\n",
    "print(\"Articles Info: \")\n",
    "print(articles.info())\n",
    "print(\"Customer Info: \")\n",
    "print(customers.info())\n",
    "print(\"Transactions Info: \")\n",
    "print(transactions.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print unique values of customer columns\n",
    "print(\"FN Newsletter vals: \", customers['FN'].unique())\n",
    "print(\"Active communication vals: \",customers['Active'].unique())\n",
    "print(\"Club member status vals: \", customers['club_member_status'].unique())\n",
    "print(\"Fashion News frequency vals: \", customers['fashion_news_frequency'].unique())\n",
    "print(\"--\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-based splitting strategy\n",
    "\n",
    "def split_train_val_data_and_drop_duplicates(transactions, days=7):\n",
    "    \"\"\"\n",
    "    Splits the transaction training data into a training set and a validation set of 7 days to prevent data leakage.\n",
    "    \"\"\"\n",
    "    \n",
    "    transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "    transactions = transactions.sort_values(by=['t_dat'])\n",
    "    latest_transaction_date = transactions['t_dat'].max()\n",
    "    \n",
    "    training_set = transactions[transactions['t_dat'] < latest_transaction_date - pd.Timedelta(days=days)]\n",
    "    validation_set = transactions[transactions['t_dat'] >= latest_transaction_date - pd.Timedelta(days=days)]\n",
    "    \n",
    "    print(\"Training set size:\", len(training_set))\n",
    "    print(\"Validation set size:\", len(validation_set))\n",
    "    print(\"Last date in training set:\", training_set['t_dat'].max())\n",
    "    print(\"Last date in validation set:\", validation_set['t_dat'].max())\n",
    "\n",
    "    # drop duplicate rows\n",
    "    training_set = training_set.drop_duplicates().copy()\n",
    "    validation_set = validation_set.drop_duplicates().copy()\n",
    "    \n",
    "    return training_set, validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(transactions_df, customers_df, articles_df, customers_col='customer_id', articles_col='article_id'):\n",
    "    \"\"\"\n",
    "    Preprocesses customer and article IDs for use in a sparse matrix.\n",
    "    \n",
    "    Returns:\n",
    "    - transactions_df: the input transaction DataFrame with two additional columns, 'user_index' and 'item_index',\n",
    "                       that map customer and article IDs to their corresponding indices in a sparse matrix\n",
    "    - customer_id_indices_map: a dictionary that maps customer IDs to their corresponding indices\n",
    "    - article_id_indices_map: a dictionary that maps article IDs to their corresponding indices\n",
    "    \"\"\"\n",
    "    # Create a list of unique customer IDs and product IDs\n",
    "    all_customers = customers_df[customers_col].unique().tolist()\n",
    "    all_articles = articles_df[articles_col].unique().tolist()\n",
    "\n",
    "    # Create dicts mapping IDs to their corresponding indices\n",
    "    customer_id_indices_map = {customer_id: i for i, customer_id in enumerate(all_customers)}\n",
    "    article_id_indices_map = {article_id: i for i, article_id in enumerate(all_articles)}\n",
    "\n",
    "    # Map customer and article IDs to their resp. indices in the transaction DataFrame\n",
    "    transactions_df['user_index'] = transactions_df[customers_col].map(customer_id_indices_map)\n",
    "    transactions_df['item_index'] = transactions_df[articles_col].map(article_id_indices_map)\n",
    "\n",
    "    return transactions_df, all_customers, all_articles, customer_id_indices_map, article_id_indices_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# binary purchase interaction user-item matrix\n",
    "\n",
    "def create_user_item_matrix(transactions):\n",
    "\n",
    "    # Get unique user and item indices in asc. order\n",
    "    user_indices = np.arange(transactions['user_index'].nunique())\n",
    "    item_indices = np.arange(transactions['item_index'].nunique())\n",
    "\n",
    "    # Create a dictionary mapping user and item indices to matrix indices\n",
    "    user_index_dict = dict(zip(sorted(transactions['user_index'].unique()), user_indices))\n",
    "    item_index_dict = dict(zip(sorted(transactions['item_index'].unique()), item_indices))\n",
    "\n",
    "    # Create arrays of row indices, column indices, and data for the sparse matrix\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = [] # purchased 1 or 0\n",
    "\n",
    "    # Iterate over all possible combinations of user and item indices\n",
    "    for user_index in user_indices:\n",
    "        for item_index in item_indices:\n",
    "            # Get the corresponding matrix indices for the user and item indices\n",
    "            matrix_user_index = user_index_dict.get(user_index)\n",
    "            matrix_item_index = item_index_dict.get(item_index)\n",
    "            # Get the corresponding interaction value from the transactions dataframe\n",
    "            interaction = transactions.loc[(transactions['user_index'] == user_index) & \n",
    "                                            (transactions['item_index'] == item_index), 'quantity'].values\n",
    "            # Append the row index, column index, and interaction value to the corresponding arrays\n",
    "            rows.append(matrix_user_index)\n",
    "            cols.append(matrix_item_index)\n",
    "            data.append(1 if len(interaction) > 0 else 0)\n",
    "\n",
    "    # Create the sparse matrix using the row, column, and data arrays\n",
    "    user_item_matrix = csr_matrix((data, (rows, cols)), shape=(len(user_indices), len(item_indices)))\n",
    "\n",
    "    return user_item_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "# calculate total number of transaction weeks in tranactions data\n",
    "transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "\n",
    "# Compute the minimum and maximum date values\n",
    "min_date = transactions['t_dat'].min()\n",
    "max_date = transactions['t_dat'].max()\n",
    "\n",
    "# Compute the number of weeks between the minimum and maximum date values\n",
    "num_weeks = ceil((max_date - min_date).days / 7)\n",
    "\n",
    "print(f\"Total number of transaction weeks: {num_weeks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# only use last x weeks of transactions data since data is too large\n",
    "def filter_transactions_last_x_weeks(transactions, x = 10):\n",
    "    # Convert date strings to datetime objects\n",
    "    transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "\n",
    "    # Calculate the date x weeks ago from the latest transaction date\n",
    "    latest_date = transactions['t_dat'].max()\n",
    "    cutoff_date = latest_date - timedelta(weeks=x)\n",
    "\n",
    "    # Filter transactions to only include those in the last x weeks\n",
    "    filtered_transactions = transactions.loc[transactions['t_dat'] >= cutoff_date].copy()\n",
    "\n",
    "    return filtered_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_customers_and_articles(customers, articles, filtered_transactions):\n",
    "    # Get unique customer and article IDs from filtered transactions\n",
    "    customer_ids = filtered_transactions['customer_id'].unique()\n",
    "    article_ids = filtered_transactions['article_id'].unique()\n",
    "\n",
    "    # Filter customers and articles to only include those in filtered transactions\n",
    "    customers_filtered = customers.loc[customers['customer_id'].isin(customer_ids)].copy()\n",
    "    articles_filtered = articles.loc[articles['article_id'].isin(article_ids)].copy()\n",
    "\n",
    "    return customers_filtered, articles_filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|LightGBM|XGBoost|CatBoost|\n",
    "|:----|:----|:----|:----|\n",
    "|Categoricals|Supports categorical features via one-hot encoding|Supports categorical features via one-hot encoding|Automatically handles categorical features using embeddings|\n",
    "|Speed|Very fast training and prediction|Fast training and prediction|Slower than LightGBM and XGBoost|\n",
    "|Handling Bias|Handles unbalanced classes via 'is_unbalance'|Handles unbalanced classes via 'scale_pos_weight'|Automatically handles unbalanced classes|\n",
    "|Handling NaNs|Handles NaN values natively|Requires manual handling of NaNs|Automatically handles NaN values using special category|\n",
    "|Custom Loss|Supports custom loss functions|Supports custom loss functions|Supports custom loss functions|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LightGBM for a ranking problem, we could also treat this as a binary classification problem where the target variable is whether an item is relevant or not to the user.\n",
    "\n",
    "OR, use LightGBM's ranking API, which is designed for ranking problems. Instead of optimizing for accuracy, the ranking API optimizes for ranking metric MAP (deprecated). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM imports\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 200 customers by number of transactions\n",
    "top_customers = transactions['customer_id'].value_counts().head(200).index.tolist()\n",
    "\n",
    "# print num of transactions for the 200th customer\n",
    "print(transactions['customer_id'].value_counts().sort_values(ascending=False).iloc[199])\n",
    "\n",
    "# only get articles that were purchased by top 200 customers at least once in articles df\n",
    "articles_top_200 = articles[articles['article_id'].isin(transactions[transactions['customer_id'].isin(top_customers)]['article_id'].unique())]\n",
    "\n",
    "# only get 200 customers in customers df\n",
    "customers_top_200 = customers[customers['customer_id'].isin(top_customers)]\n",
    "\n",
    "articles = articles_top_200.copy()\n",
    "customers = customers_top_200.copy()\n",
    "transactions = transactions[transactions['customer_id'].isin(top_customers)].copy()\n",
    "transactions = transactions.drop_duplicates().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transactions.isnull().sum())\n",
    "print(customers.isnull().sum())\n",
    "print(articles.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(transactions))\n",
    "print(len(customers))\n",
    "print(len(articles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with uninformative article data\n",
    "\n",
    "articles = articles.drop(columns=['product_code', 'prod_name', 'product_type_name', 'product_group_name', 'graphical_appearance_name', 'department_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name', 'detail_desc'])\n",
    "articles = articles.drop(columns=[col for col in articles.columns if 'colour_' in col or 'perceived_' in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns are left to capture any potential patterns in the other columns, such as how certain index codes or sections might be associated with higher or lower sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define mapping for fashion_news_frequency feature\n",
    "fashion_news_freq_mapping = {'None': 0, 'Monthly': 1, 'Regularly': 2}\n",
    "\n",
    "# label encode fashion_news_frequency feature\n",
    "le = LabelEncoder()\n",
    "customers['fashion_news_frequency'] = customers['fashion_news_frequency'].map(fashion_news_freq_mapping)\n",
    "customers['fashion_news_frequency'] = le.fit_transform(customers['fashion_news_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers.drop(['postal_code'], axis=1)\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: encode nominal categorical features\n",
    "ohe = OneHotEncoder()\n",
    "# One-hot encode sales_channel_id feature\n",
    "sales_channel_ohe = pd.get_dummies(transactions['sales_channel_id'], prefix='sales_channel')\n",
    "transactions = pd.concat([transactions, sales_channel_ohe], axis=1)\n",
    "\n",
    "# Drop the original sales_channel_id feature\n",
    "transactions.drop('sales_channel_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean unique values of sales_channel_id after encoding\n",
    "print(transactions['sales_channel_1'].unique())\n",
    "print(transactions['sales_channel_2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 't_dat' column to datetime format\n",
    "transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "\n",
    "# Group by customer ID and find the first and last transaction dates\n",
    "first_trans_dates = transactions.groupby('customer_id')['t_dat'].min().reset_index()\n",
    "last_trans_dates = transactions.groupby('customer_id')['t_dat'].max().reset_index()\n",
    "\n",
    "customer_purchase_engagement = pd.merge(first_trans_dates, last_trans_dates, on='customer_id', suffixes=('_first', '_last'))\n",
    "# Create a new feature by calculating the time difference in days between first and last transactions\n",
    "customer_purchase_engagement['time_diff_days'] = (customer_purchase_engagement['t_dat_last'] - customer_purchase_engagement['t_dat_first']).dt.days\n",
    "# Drop the original first and last transaction date columns\n",
    "customer_purchase_engagement.drop(['t_dat_first', 't_dat_last'], axis=1, inplace=True)\n",
    "customer_purchase_engagement.head()\n",
    "\n",
    "# Merge the customer_purchase_engagement dataframe with the customers dataframe\n",
    "customers = pd.merge(customers, customer_purchase_engagement, on='customer_id', how='left')\n",
    "customers.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above `time_diff_days` feature can potentially provide insights into a customer's engagement by looking at the gap in the number of days between the last purchase and the current date. <br> The assumption is, The larger the gap, the less engaged the customer is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the transaction dataframe with the customers dataframe\n",
    "merged = pd.merge(transactions, customers, on='customer_id', how='inner')\n",
    "\n",
    "# Calculate the mean age for each article\n",
    "item_mean_age = merged.groupby('article_id')['age'].mean()\n",
    "\n",
    "# Calculate the difference between every user's age and the mean age of users who have purchased a particular item\n",
    "merged['age_diff'] = merged['age'] - merged['article_id'].map(item_mean_age)\n",
    "\n",
    "# Group by article and take the mean of age_diff\n",
    "article_age_diff = merged.groupby('article_id')['age_diff'].mean()\n",
    "\n",
    "# Append the age difference feature to the articles dataframe\n",
    "articles['age_diff'] = articles['article_id'].map(article_age_diff)\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean age_diff for every article. It can be useful for predicting whether a user will buy an item based on their age and the age of other users who have already bought the same item. \n",
    "\n",
    "Intuituion behind the `age_diff` feature:\n",
    "\n",
    "Let's say we have a dataset of customers who made transactions for a particular item with article_id = 123. Here is an example of how we can calculate the age_diff feature: <br>\n",
    "Assume that the mean age of all customers who bought the item with article_id = 123 is 40 years old <br>\n",
    "Customer A made a transaction for item with article_id = 123 and their age is 35. The age_diff feature for this transaction would be -5. (35 - 40). <br>\n",
    "Customer B made a transaction for item with article_id = 123 and their age is 50. The age_diff feature for this transaction would be 10. (50 - 40). <br>\n",
    "Customer C made a transaction for item with article_id = 123 and their age is 40. The age_diff feature for this transaction would be 0. (40 - 40). <br>\n",
    "So, the age_diff feature measures the difference between the age of each customer who bought a specific item and the average age of all customers who bought that item. <br>\n",
    "\n",
    "Therefore, the age_diff is the mean of all these individual age_diff values for each customer who bought the item with article_id = 123. age_diff = -1.66 for this example<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean, max, and min age for each item\n",
    "item_mean_age = merged.groupby('article_id')['age'].mean()\n",
    "item_max_age = merged.groupby('article_id')['age'].max()\n",
    "item_min_age = merged.groupby('article_id')['age'].min()\n",
    "\n",
    "# Merge the features back into the articles dataframe\n",
    "articles = articles.merge(item_mean_age, on='article_id', how='left')\n",
    "articles = articles.merge(item_max_age, on='article_id', how='left')\n",
    "articles = articles.merge(item_min_age, on='article_id', how='left')\n",
    "\n",
    "# Rename the columns to make them more descriptive\n",
    "articles = articles.rename(columns={'age_x': 'mean_purchase_age', 'age_y': 'max_purchase_age', 'age': 'min_purchase_age'})\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuituion behind the `*_purchase_age` feature:\n",
    "\n",
    "Additional age features to capture more information about the age of the customers who bought the respective articles. The gbdt might be able to learn more complex patterns from these features. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate purchased item count for each user\n",
    "transactions['quantity'] = 1\n",
    "user_item_count = transactions.groupby(['customer_id', 'article_id'])['quantity'].sum().reset_index()\n",
    "\n",
    "# Calculate total item count for each article\n",
    "total_item_count = transactions.groupby('article_id')['quantity'].sum().reset_index()\n",
    "total_item_count.columns = ['article_id', 'total_items']\n",
    "\n",
    "user_item_count = pd.merge(user_item_count, total_item_count, on='article_id', how='left')\n",
    "\n",
    "# Calculate ratio of purchased item count and total item count\n",
    "user_item_count['article_engagement_ratio'] = user_item_count['quantity'] / user_item_count['total_items']\n",
    "\n",
    "\n",
    "transactions = pd.merge(transactions, user_item_count[['customer_id', 'article_id', 'article_engagement_ratio']], on=['customer_id', 'article_id'], how='left')\n",
    "transactions['quantity'] = user_item_count['quantity']\n",
    "\n",
    "# fill missing values with 0\n",
    "transactions['quantity'] = transactions['quantity'].fillna(0)\n",
    "transactions.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition behind feature: <br>\n",
    "\n",
    "`article_engagement_ratio`: The feature is ratio of one user's purchased item count and the item's total purchase count. This serves to measure how engaged a user is with a particular item, which can be useful for predicting whether a user will buy similar items. <br>\n",
    "Can also be used to measure how popular an item is, and can be used to potentially diversify recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions, all_customers, all_articles, customer_id_indices_map, article_id_indices_map = preprocess_data(transactions, customers, articles)\n",
    "\n",
    "print(\"Total num of customers: \", len(all_customers))\n",
    "print(\"Total num of articles: \", len(all_articles))\n",
    "print(\"Customer ID mapping: \", list(customer_id_indices_map.items())[:5])\n",
    "print(\"Article ID mapping: \", list(article_id_indices_map.items())[:5])\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user item matrix -- rows are users, columns are items, doesnt need article and customer data\n",
    "\n",
    "user_item_matrix = create_user_item_matrix(transactions)\n",
    "user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_item_matrix[:10, :10].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.als import AlternatingLeastSquares\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# from the als_strat1_hyperparam_log \n",
    "# Create ALS model with default parameters\n",
    "alpha = 25\n",
    "als_model = AlternatingLeastSquares(factors=55, iterations=20, regularization=0.18)\n",
    "\n",
    "# Fit model to user-item matrix\n",
    "als_model.fit(user_item_matrix*alpha)\n",
    "\n",
    "# Latent factors matrices\n",
    "item_factors = als_model.item_factors\n",
    "user_factors = als_model.user_factors\n",
    "\n",
    "# item-item cosine similarity \n",
    "item_similarities = cosine_similarity(item_factors, dense_output=False)\n",
    "# user-user cosine similarity\n",
    "user_similarities = cosine_similarity(user_factors, dense_output=False)\n",
    "\n",
    "k = 5\n",
    "# Get top-k most similar items for each item\n",
    "top_k_similar_items = item_similarities.argsort()[:, -k-1:-1]\n",
    "# Get top-k most similar user for each user\n",
    "top_k_similar_users = user_similarities.argsort()[:, -k-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item_similarities.shape)\n",
    "print(user_similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important: add user_index and item_index to customers and articles respectively\n",
    "customers['user_index'] = customers['customer_id'].map(customer_id_indices_map)\n",
    "articles['item_index'] = articles['article_id'].map(article_id_indices_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indices = np.arange(transactions['user_index'].nunique())\n",
    "item_indices = np.arange(transactions['item_index'].nunique())\n",
    "user_index_dict = dict(zip(sorted(transactions['user_index'].unique()), user_indices))\n",
    "item_index_dict = dict(zip(sorted(transactions['item_index'].unique()), item_indices))\n",
    "\n",
    "# print first 5 key and values in the dictionary\n",
    "print(list(user_index_dict.items())[:5])\n",
    "print(list(item_index_dict.items())[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from user IDs to matrix indices\n",
    "user_indices = np.arange(transactions['user_index'].nunique())\n",
    "item_indices = np.arange(transactions['item_index'].nunique())\n",
    "user_index_dict = dict(zip(sorted(transactions['user_index'].unique()), user_indices))\n",
    "item_index_dict = dict(zip(sorted(transactions['item_index'].unique()), item_indices))\n",
    "\n",
    "\n",
    "# Create features for each user based on the average quantity purchased by similar customers\n",
    "for i in range(len(customers)):\n",
    "    customer_id = customers.loc[i, 'user_index']\n",
    "    # Get the matrix index for the current customer\n",
    "    customer_idx = user_index_dict.get(customer_id, -1)\n",
    "    \n",
    "    if customer_idx != -1:\n",
    "        similar_user_indexxs = top_k_similar_users[customer_idx]\n",
    "        # Compute the mean of the user-item matrix for the similar users\n",
    "        user_feature = user_item_matrix[similar_user_indexxs, :].mean(axis=0).A1\n",
    "        \n",
    "        # Store the mean in the customers DataFrame\n",
    "        customers.loc[i, 'user_purchase_quant'] = user_feature.mean()\n",
    "    else:\n",
    "        # If the current customer is not in the user-item matrix, set the feature to NaN\n",
    "        customers.loc[i, 'user_purchase_quant'] = np.nan\n",
    "\n",
    "# Print the head of the customers DataFrame\n",
    "customers.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition behind feature: <br>\n",
    "\n",
    "`user_purchase_quant`: Gets the average quantity of items purchased by the k most similar customers to that customer. It looks at what other customers who are similar to this customer have bought and calculates the average amount of each item they bought. This feature can be used to predict what items a customer is likely to buy in the future based on what similar customers have bought in the past. This feature aims to capture purchase behaviours of a customer.<br>\n",
    "\n",
    "For example, if a customer typically buys a lot of bomber jackets, and the top k most similar customers to that customer also tend to buy a lot of bomber jackets, then the average quantity of bomber jackets purchased by those similar customers could be a good predictor of how much the original customer is likely to purchase in the future. This however assumes that the k most similar customers have similar purchase behaviours to the customers in question, and on its own is not a strong feature.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary feature for each item that indicates whether or not a customer has bought that item,\n",
    "# based on whether other customers who bought similar items also tended to buy that item.\n",
    "for i in range(len(articles)):\n",
    "    item_id = articles.loc[i, 'item_index']\n",
    "    # Get the matrix index for the current item\n",
    "    item_idx = item_index_dict.get(item_id, -1)\n",
    "    \n",
    "    if item_idx != -1:\n",
    "        # List of (column) indices of similar items\n",
    "        similar_items = top_k_similar_items[item_idx]\n",
    "        \n",
    "        # Find the customers who have purchased the current item\n",
    "        customer_indices = np.where(user_item_matrix[:, item_idx].toarray()[:, 0] == 1)[0]\n",
    "        \n",
    "        # Binary vector representing the customer's purchases for the similar items\n",
    "        customer_purchases = user_item_matrix[customer_indices, :][:, similar_items].toarray()\n",
    "        article_preference = np.any(customer_purchases, axis=0)\n",
    "        \n",
    "        # Set article_preference to 1 if any customer has purchased the item, 0 otherwise\n",
    "        articles.loc[i, 'article_preference'] = int(np.any(article_preference))\n",
    "    else:\n",
    "        articles.loc[i, 'article_preference'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[articles['article_preference'] == 0].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition behind feature: <br>\n",
    "\n",
    "`article_preference`: Binary feature for each item that indicates whether or not a customer has bought that item, based on whether other customers who bought similar items also tended to buy that item. <br>\n",
    "\n",
    "This feature can be useful for a fashion-based recommender system because it captures the idea that customers who have similar tastes or preferences tend to buy similar items. For example, if a customer has a history of buying shirts and other customers who bought similar shorts also tended to buy a specific pair of jeans, then the binary feature for those jeans would be set to 1 for that customer, indicating that they are likely to be interested in those shoes. The binary feature can then be used as a predictor for which items to recommend to the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature for each item based on the total number of times it was purchased by similar customers\n",
    "for i in range(len(articles)):\n",
    "    item_id = articles.loc[i, 'item_index']\n",
    "    item_idx = item_index_dict.get(item_id, -1)\n",
    "    \n",
    "    if item_idx != -1:\n",
    "    \n",
    "        similar_items = top_k_similar_items[item_idx]\n",
    "        \n",
    "        # Find the customers who have purchased the current item\n",
    "        customer_indices = np.where(user_item_matrix[:, item_idx].toarray()[:, 0] == 1)[0]\n",
    "        \n",
    "        # Compute the mean of the user-item matrix for the similar items\n",
    "        item_feature = user_item_matrix[customer_indices, :][:, similar_items].sum() / len(customer_indices)\n",
    "        articles.loc[i, 'item_purchase_frequency'] = item_feature.mean()\n",
    "        \n",
    "    else:\n",
    "        articles.loc[i, 'item_purchase_frequency'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition behind feature: <br>\n",
    "\n",
    "`item_purchase_frequency`: It gives an estimate of how frequently an item is being bought by customers who have similar purchase histories. This feature is useful because it can provide insights into purchasing patterns and identify popular items that are often bought together. <br> It can potentially help identify popular items among customer groups, and the lightgbm model can potentially use this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(transactions, articles[['item_index']], on='item_index', how='left')\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge transactions with articles on item_index to access the price of each article\n",
    "merged_df = pd.merge(transactions, articles[['item_index']], on='item_index', how='left')\n",
    "\n",
    "# Compute the average price levels of all articles purchased by similar customers who have purchased this particular article in the past\n",
    "for i in range(len(articles)):\n",
    "    item_id = articles.loc[i, 'item_index']\n",
    "    item_idx = item_index_dict.get(item_id, -1)\n",
    "    \n",
    "    if item_idx != -1:\n",
    "        similar_items = top_k_similar_items[item_idx]\n",
    "        \n",
    "        # Find the customers who have purchased the current item\n",
    "        customer_indices = np.where(user_item_matrix[:, item_idx].toarray()[:, 0] == 1)[0]\n",
    "        \n",
    "        # Compute the average price levels of all articles purchased by similar customers who have purchased this particular article in the past\n",
    "        item_feature = merged_df.loc[(merged_df['item_index'] == item_id) & (merged_df['user_index'].isin(customer_indices)), 'price'].mean()\n",
    "        articles.loc[i, 'item_avg_price_level'] = item_feature\n",
    "    else:\n",
    "        articles.loc[i, 'item_avg_price_level'] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition behind feature: <br>\n",
    "\n",
    "`item_avg_price_level`: Calculates the average price levels of all articles purchased by similar customers who have purchased this particular article in the past.  <br> It provides information on the typical price level of articles that are purchased together with a given article, as indicated by the purchasing patterns of similar customers. \n",
    "\n",
    "For example, if customers who frequently purchase articles A also tend to purchase higher-priced article, then the \"item_avg_price_level\" feature for article A would be relatively high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print num of unique articles['graphical_appearance_no']\n",
    "print(articles['graphical_appearance_no'].nunique())\n",
    "print(articles['product_type_no'].nunique())\n",
    "print(articles['department_no'].nunique())\n",
    "print(articles['index_code'].nunique())\n",
    "print(articles['index_group_no'].nunique()) \n",
    "print(articles['section_no'].nunique())\n",
    "print(articles['garment_group_no'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already dropped for articles\n",
    "# articles_final_df = articles.drop(['article_id'], axis=1).copy()\n",
    "customers_final_df = customers.drop(['customer_id'], axis=1).copy()\n",
    "transactions_final_df = transactions.drop(['article_id', 'customer_id'], axis=1).copy()\n",
    "\n",
    "# Merge transactions with customers\n",
    "df = pd.merge(transactions_final_df, customers_final_df, on='user_index', how='left')\n",
    "\n",
    "# Merge resulting dataframe with articles_final_df usually\n",
    "df = pd.merge(df, articles, on='item_index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exttracting time-based features\n",
    "\n",
    "df['t_dat'] = pd.to_datetime(df['t_dat'])\n",
    "df['year'] = df['t_dat'].dt.year\n",
    "df['month'] = df['t_dat'].dt.month\n",
    "df['day'] = df['t_dat'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM features, reference: https://www.geeksforgeeks.org/rfm-analysis-analysis-using-python/\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate recency\n",
    "last_purchase_date = transactions.groupby('user_index')['t_dat'].max().reset_index()\n",
    "last_purchase_date.columns = ['user_index', 'last_purchase_date']\n",
    "last_purchase_date['recency'] = (last_purchase_date['last_purchase_date'].max() - last_purchase_date['last_purchase_date']).dt.days\n",
    "last_purchase_date.drop('last_purchase_date', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Calculate Frequency\n",
    "frequency = transactions.groupby('user_index')['t_dat'].count().reset_index()\n",
    "frequency.columns = ['user_index', 'frequency']\n",
    "\n",
    "# Calculate Monetary Value\n",
    "monetary_value = transactions.groupby('user_index')['price'].sum().reset_index()\n",
    "monetary_value.columns = ['user_index', 'monetary_value']\n",
    "\n",
    "# Merge all RFM features into a single DataFrame\n",
    "rfm = last_purchase_date[['user_index', 'recency']].merge(frequency, on='user_index').merge(monetary_value, on='user_index')\n",
    "\n",
    "# Calculate RFM Scores\n",
    "quantiles = rfm.quantile(q=[0.25, 0.5, 0.75])\n",
    "quantiles = quantiles.to_dict()\n",
    "\n",
    "def rfm_segmenter(x, quantiles):\n",
    "    if x <= quantiles['recency'][0.25]:\n",
    "        return 4\n",
    "    elif x <= quantiles['recency'][0.50]:\n",
    "        return 3\n",
    "    elif x <= quantiles['recency'][0.75]: \n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "rfm['R'] = rfm['recency'].apply(rfm_segmenter, args=(quantiles,))\n",
    "\n",
    "def f_segmenter(x, quantiles):\n",
    "    if x <= quantiles['frequency'][0.25]:\n",
    "        return 1\n",
    "    elif x <= quantiles['frequency'][0.50]:\n",
    "        return 2\n",
    "    elif x <= quantiles['frequency'][0.75]: \n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "rfm['F'] = rfm['frequency'].apply(f_segmenter, args=(quantiles,))\n",
    "\n",
    "def m_segmenter(x, quantiles):\n",
    "    if x <= quantiles['monetary_value'][0.25]:\n",
    "        return 1\n",
    "    elif x <= quantiles['monetary_value'][0.50]:\n",
    "        return 2\n",
    "    elif x <= quantiles['monetary_value'][0.75]: \n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "rfm['M'] = rfm['monetary_value'].apply(m_segmenter, args=(quantiles,))\n",
    "\n",
    "# Calculate RFM Score\n",
    "rfm['RFM Score'] = rfm['R'].map(str) + rfm['F'].map(str) + rfm['M'].map(str)\n",
    "rfm = rfm.drop(['R', 'F', 'M'], axis=1)\n",
    "\n",
    "# Display sample of RFM DataFrame\n",
    "print(rfm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge rfm with df on user_index\n",
    "\n",
    "df = pd.merge(df, rfm, on='user_index', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df.drop(['t_dat'], axis=1).copy()\n",
    "# print all column names in final_df\n",
    "print(final_df.columns)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['RFM Score'] = pd.to_numeric(final_df['RFM Score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame as a pickle file\n",
    "df.to_pickle('lightgbm/df.pkl')\n",
    "final_df.to_pickle('lightgbm/final_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print final_df shape\n",
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indices, item_indices = user_item_matrix.get_shape()\n",
    "\n",
    "print('Number of users: %d' % user_indices)\n",
    "print('Number of items: %d' % item_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()\n",
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assume you have a CSR matrix called user_item_matrix\n",
    "# Save the matrix as a pickle file\n",
    "with open('user_item_matrix_200.pkl', 'wb') as f:\n",
    "    pickle.dump(user_item_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load user_item_matrix from pickle file\n",
    "\n",
    "with open('user_item_matrix_200.pkl', 'rb') as f:\n",
    "    user_item_matrix = pickle.load(f)\n",
    "\n",
    "user_item_matrix = user_item_matrix.toarray()\n",
    "\n",
    "# extract indices of non-zero elements\n",
    "user_purchased_indices, item_purchased_indices = user_item_matrix.nonzero()\n",
    "\n",
    "print('user_purchased_indices: ', user_indices)\n",
    "print('item_purchased_indices: ', item_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to hold the dummy data\n",
    "\n",
    "# load final_df from pickle file for clean processing\n",
    "with open('lightgbm/final_df.pkl', 'rb') as f:\n",
    "    final_df = pickle.load(f)\n",
    "\n",
    "dummy_data = []\n",
    "\n",
    "# get the unique user and item indices from final_df\n",
    "users = final_df['user_index'].unique()\n",
    "items = final_df['item_index'].unique()\n",
    "\n",
    "# final_df.shape[1]\n",
    "# loop through all possible user-item pairs\n",
    "for user in users:\n",
    "    for item in items:\n",
    "        # check if the user-item pair has an interaction in the sparse matrix\n",
    "        if user_item_matrix[user, item] == 1:\n",
    "            # if it does, set the target of the corresponding row in final_df to 1\n",
    "            final_df.loc[(final_df['user_index'] == user) & (final_df['item_index'] == item), 'target'] = 1\n",
    "        else:\n",
    "            # if it doesn't, add a row to the dummy data with target = 0 and stub values for other columns\n",
    "            dummy_data.append([user, item] + [np.nan] * (final_df.shape[1] - 2))\n",
    "\n",
    "\n",
    "dummy_df = pd.DataFrame(dummy_data, columns=['user_index', 'item_index'] + list(final_df.columns.drop(['user_index', 'item_index'])))\n",
    "dummy_df['target'] = 0  # set target to 0 for the dummy data\n",
    "final_df = pd.concat([final_df, dummy_df], ignore_index=True)\n",
    "\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop recency\tfrequency\tmonetary_value columns\n",
    "final_df = final_df.drop(['recency', 'frequency', 'monetary_value'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode garment_group_no and index_group_no columns\n",
    "one_hot_cols = ['garment_group_no', 'index_group_no']\n",
    "final_df = pd.get_dummies(final_df, columns=one_hot_cols)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame as a pickle file\n",
    "final_df.to_pickle('lightgbm/final_df_with_binary_targets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the article_id_indices_map and user_id_indices_map as pickle files\n",
    "with open('lightgbm/article_id_indices_map.pkl', 'wb') as f:\n",
    "    pickle.dump(article_id_indices_map, f)\n",
    "with open('lightgbm/customer_id_indices_map.pkl', 'wb') as f:\n",
    "    pickle.dump(customer_id_indices_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename RFM_Score to RFM_Score\n",
    "final_df.rename(columns={'RFM Score': 'RFM_Score'}, inplace=True)\n",
    "# convert sales_channel_ 1 to to boolean\n",
    "final_df['sales_channel_1'] = final_df['sales_channel_1'].astype('bool')\n",
    "final_df['sales_channel_2'] = final_df['sales_channel_2'].astype('bool')\n",
    "final_df.to_pickle('lightgbm/final_df_with_binary_targets.pkl')\n",
    "# final_df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load final_df from pickle file for clean processing\n",
    "with open('lightgbm/final_df_with_binary_targets.pkl', 'rb') as f:\n",
    "    final_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = final_df.groupby('user_index')\n",
    "grouped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df from pickle file for clean processing\n",
    "with open('lightgbm/df.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_train_test_split(final_df, test_size=0.2):\n",
    "\n",
    "    # Convert days, months, and years columns to datetime object\n",
    "    final_df['date'] = pd.to_datetime(final_df[['day', 'month', 'year']])\n",
    "\n",
    "    # Sort dataframe by date in ascending order\n",
    "    final_df = final_df.sort_values(by='date')\n",
    "\n",
    "    # Calculate cutoff index\n",
    "    cutoff_index = int(len(final_df) * (1-test_size))\n",
    "\n",
    "    # Create train and test dataframes\n",
    "    train_df = final_df[:cutoff_index]\n",
    "    test_df = final_df[cutoff_index:]\n",
    "\n",
    "    # Drop date column from train and test dataframes\n",
    "    train_df = train_df.drop('date', axis=1)\n",
    "    test_df = test_df.drop('date', axis=1)\n",
    "\n",
    "    # split train_df into X_train and y_train\n",
    "    X_train = train_df.drop('target', axis=1)\n",
    "    y_train = train_df['target']\n",
    "\n",
    "    # split test_df into X_test and y_test\n",
    "    X_test = test_df.drop('target', axis=1)\n",
    "    y_test = test_df['target']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = time_based_train_test_split(final_df, test_size=0.2)\n",
    "\n",
    "# drop date column from final_df\n",
    "final_df = final_df.drop('date', axis=1)\n",
    "\n",
    "#print the shape of X_train, X_test, y_train, y_test\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_pickle('lightgbm/final_df_with_binary_targets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "with open('lightgbm/final_df_with_binary_targets.pkl', 'rb') as f:\n",
    "    final_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "6242440\n",
      "6242440\n",
      "6242440\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# 80/20 time-based split to curb data leakage\n",
    "X_train, X_test, y_train, y_test = time_based_train_test_split(final_df, test_size=0.2)\n",
    "final_df = final_df.drop('date', axis=1)\n",
    "\n",
    "# print(len(X_train))\n",
    "# print(len(y_train))\n",
    "# print(len(X_test))\n",
    "# print(len(y_test))\n",
    "\n",
    "grouped_data_train = X_train.groupby('user_index')\n",
    "grouped_data_test = X_test.groupby('user_index')\n",
    "groups = [grouped_data_train.groups[user] for user in grouped_data_train.groups.keys()]\n",
    "groups_flat = np.concatenate(groups)\n",
    "print(X_train['user_index'].nunique())\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(groups_flat))\n",
    "print(len(grouped_data_train.groups.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 405 candidates, totalling 810 fits\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 47.6 MiB for an array with shape (2, 3121220) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 678, in _fit_and_score\n    X_train, y_train = _safe_split(estimator, X, y, train)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\metaestimators.py\", line 233, in _safe_split\n    X_subset = _safe_indexing(X, indices)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\__init__.py\", line 354, in _safe_indexing\n    return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\__init__.py\", line 196, in _pandas_indexing\n    return X.take(key, axis=axis)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py\", line 3909, in take\n    return self._take(indices, axis)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py\", line 3932, in _take\n    new_data = self._mgr.take(\n               ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py\", line 963, in take\n    return self.reindex_indexer(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py\", line 747, in reindex_indexer\n    new_blocks = [\n                 ^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py\", line 748, in <listcomp>\n    blk.take_nd(\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\blocks.py\", line 945, in take_nd\n    new_values = algos.take_nd(\n                 ^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\array_algos\\take.py\", line 117, in take_nd\n    return _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\array_algos\\take.py\", line 157, in _take_nd_ndarray\n    out = np.empty(out_shape, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 47.6 MiB for an array with shape (2, 3121220) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m groups \u001b[39m=\u001b[39m [grouped_data_train\u001b[39m.\u001b[39mgroups[user] \u001b[39mfor\u001b[39;00m user \u001b[39min\u001b[39;00m grouped_data_train\u001b[39m.\u001b[39mgroups\u001b[39m.\u001b[39mkeys()]\n\u001b[0;32m     73\u001b[0m groups_flat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(groups)\n\u001b[1;32m---> 74\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train, groups\u001b[39m=\u001b[39;49mgroups_flat)\n\u001b[0;32m     76\u001b[0m \u001b[39m# Save the best model\u001b[39;00m\n\u001b[0;32m     77\u001b[0m joblib\u001b[39m.\u001b[39mdump(clf\u001b[39m.\u001b[39mbest_estimator_, \u001b[39m'\u001b[39m\u001b[39mbest_model.pkl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 47.6 MiB for an array with shape (2, 3121220) and data type float64"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import ndcg_score\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.feature_selection import RFECV\n",
    "import joblib\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define columns to target encode\n",
    "cols_to_encode = ['department_no', 'product_type_no', 'section_no', 'graphical_appearance_no']\n",
    "\n",
    "# Define number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Create KFold object for cross-validation\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform target encoding with cross-validation\n",
    "for col in cols_to_encode:\n",
    "    final_df[f'{col}_te'] = 0\n",
    "    te = TargetEncoder(cols=[col])\n",
    "    for train_idx, val_idx in kf.split(final_df):\n",
    "        te.fit(final_df.iloc[train_idx][[col]], final_df.iloc[train_idx]['target'])\n",
    "        final_df.loc[val_idx, f'{col}_te'] = te.transform(final_df.iloc[val_idx][[col]]).values.flatten()\n",
    "\n",
    "# Define features and target\n",
    "features = final_df.columns.tolist()\n",
    "features.remove('target')\n",
    "target = 'target'\n",
    "\n",
    "# # 80/20 time-based split to curb data leakage\n",
    "# X_train, X_test, y_train, y_test = time_based_train_test_split(final_df, test_size=0.2)\n",
    "# final_df = final_df.drop('date', axis=1)\n",
    "\n",
    "# Group data by user -- so that LightGBM knows which data points belong to each user and can compute the metrics correctly\n",
    "grouped_data_train = X_train.groupby('user_index')\n",
    "grouped_data_test = X_test.groupby('user_index')\n",
    "\n",
    "# Create LightGBM datasets with group query information\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=grouped_data_train.groups.values())\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=grouped_data_test.groups.values())\n",
    "\n",
    "# Define hyperparameters\n",
    "params = {'objective': 'binary',\n",
    "          'boosting_type': 'gbdt',\n",
    "          'metric': 'map',\n",
    "          'num_leaves': 31,\n",
    "          'learning_rate': 0.05,\n",
    "          'feature_fraction': 0.9,\n",
    "          'bagging_fraction': 0.8,\n",
    "          'bagging_freq': 5,\n",
    "          'early_stopping_rounds': 10,\n",
    "          'verbose': 1}\n",
    "          \n",
    "grid_params = {\n",
    "    'selector__k': [10, 25, 40], \n",
    "    'lgbm__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'lgbm__num_leaves': [15, 31, 63],\n",
    "    'lgbm__bagging_fraction': [0.6, 0.8, 1.0],\n",
    "    'lgbm__feature_fraction': [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Create a pipeline that includes SelectKBest\n",
    "selector = SelectKBest(score_func=chi2)\n",
    "pipeline = Pipeline(steps=[('selector', selector), ('lgbm', lgb.LGBMClassifier(**params))])\n",
    "\n",
    "# Add num_boost_round parameter to lgbm estimator in pipeline\n",
    "pipeline.named_steps['lgbm'].set_params(num_boost_round=100)\n",
    "\n",
    "# Perform grid search on the pipeline\n",
    "scoring = get_scorer('average_precision')\n",
    "clf = GridSearchCV(estimator=pipeline, param_grid=grid_params, cv=2, scoring=scoring, n_jobs=-1, verbose=2)\n",
    "groups = [grouped_data_train.groups[user] for user in grouped_data_train.groups.keys()]\n",
    "groups_flat = np.concatenate(groups)\n",
    "clf.fit(X_train, y_train, groups=groups_flat)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(clf.best_estimator_, 'best_model.pkl')\n",
    "print(f'Best hyperparameters: {clf.best_params_}')\n",
    "print(f'Best map score: {clf.best_score_}')\n",
    "\n",
    "# Save the feature importances to a file\n",
    "feat_importances = pd.Series(clf.best_estimator_.feature_importances_, index=X_train_sel.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh')\n",
    "plt.savefig('lightgbm/feature_importances.png')\n",
    "\n",
    "# Save the selected features\n",
    "selected_features = X_train_sel.columns.tolist()\n",
    "joblib.dump(selected_features, 'lightgbm/selected_features.pkl')\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = clf.best_estimator_.predict(X_test_sel, num_iteration=clf.best_estimator_.best_iteration_)\n",
    "ndcg = ndcg_score(y_test, y_pred, group_scores=True, verbose=1)\n",
    "print(f'NDCG score on test set: {ndcg}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, it can be used to predict the probability of purchase for new user-product pairs, which can be used to generate recommendations for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X is the input data for the LightGBM model\n",
    "# X has a row for each user-product pair and a binary target indicating whether the user purchased the product or not\n",
    "\n",
    "# Train the LightGBM model on X\n",
    "# lgb_model = lgb.LGBMClassifier(**best_params)\n",
    "# lgb_model.fit(X, y)\n",
    "\n",
    "# Generate candidate products for each user\n",
    "# This can be done using a combination of popular products and user purchase history\n",
    "# Let's assume we have a dictionary 'user_products' that maps each user ID to a list of products they've purchased\n",
    "user_candidates = {}\n",
    "for user_id in user_products:\n",
    "    # Select the 600 most popular products\n",
    "    popular_products = select_popular_products(600)\n",
    "    \n",
    "    # Add user purchase history to candidate list\n",
    "    user_history = user_products[user_id]\n",
    "    candidate_products = list(set(popular_products + user_history))\n",
    "    \n",
    "    # Store candidate products for this user\n",
    "    user_candidates[user_id] = candidate_products\n",
    "\n",
    "# Predict probabilities of purchase for each candidate product for each user\n",
    "user_scores = {}\n",
    "for user_id, candidates in user_candidates.items():\n",
    "    # Create input data for this user\n",
    "    user_data = create_user_data(user_id, candidates)\n",
    "    \n",
    "    # Predict probabilities using the LightGBM model\n",
    "    scores = lgb_model.predict_proba(user_data)[:, 1]\n",
    "    \n",
    "    # Store scores for this user\n",
    "    user_scores[user_id] = scores\n",
    "\n",
    "# Rank candidate products for each user and return top 12 as recommendations\n",
    "recommendations = {}\n",
    "for user_id, scores in user_scores.items():\n",
    "    # Sort candidate products by descending score\n",
    "    candidate_products = user_candidates[user_id]\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_products = [candidate_products[i] for i in sorted_indices]\n",
    "    \n",
    "    # Select top 12 products\n",
    "    top_products = sorted_products[:12]\n",
    "    \n",
    "    # Add user purchase history to top products\n",
    "    top_products += user_products[user_id]\n",
    "    \n",
    "    # Remove duplicates and return as recommendations\n",
    "    recommendations[user_id] = list(set(top_products))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat this as a binary classification problem, we would be ignoring the importance of the ranking of the recommended items and the MAP metric would not be appropriate. Since we are using MAP as the evaluation metric, we should use the LightGBM ranking API instead of the binary classification API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import make_scorer\n",
    "# from sklearn.metrics import average_precision_score\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# target = 'item_index'\n",
    "# features = final_df.columns.tolist()\n",
    "# features.remove(target)\n",
    "\n",
    "# # split the data into training and test sets -- can also do time-based split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(final_df[features], final_df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# # for number of items to rank for each user (group param for ordered ranking)\n",
    "# num_items_per_user = 12\n",
    "# user_indices = X_test.index.unique()\n",
    "# query = [num_items_per_user] * len(user_indices)\n",
    "# query_ids = []\n",
    "# for user_index in user_indices:\n",
    "#     user_indices_repeated = [user_index] * num_items_per_user\n",
    "#     query_ids.extend(user_indices_repeated)\n",
    "\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, group=query_ids)\n",
    "\n",
    "# # MAP@12 metric\n",
    "# def mean_average_precision(y_true, y_score, k=12):\n",
    "#     # get the indices of the top k scores\n",
    "#     top_k_indices = np.argsort(y_score)[::-1][:k]\n",
    "\n",
    "#     # calculate average precision at k\n",
    "#     return average_precision_score(y_true[top_k_indices], y_score[top_k_indices])\n",
    "\n",
    "# # define hyperparameters for tuning\n",
    "# params = {\n",
    "# 'objective': 'lambdarank', #using lightgbm ranking API\n",
    "# 'metric': 'MAP',\n",
    "# 'learning_rate': 0.05,\n",
    "# 'num_leaves': 31,\n",
    "# 'max_depth': 5,\n",
    "# 'min_data_in_leaf': 50,\n",
    "# 'feature_fraction': 0.8,\n",
    "# 'bagging_fraction': 0.8,\n",
    "# 'bagging_freq': 5\n",
    "# }\n",
    "\n",
    "# # create LightGBM model\n",
    "# model = lgb.LGBMRanker()\n",
    "\n",
    "# # perform grid search with cross-validation\n",
    "# param_grid = {\n",
    "# 'num_leaves': [31, 50, 75],\n",
    "# 'max_depth': [5, 7, -1],\n",
    "# 'min_data_in_leaf': [20, 50, 100],\n",
    "# 'feature_fraction': [0.6, 0.8, 1],\n",
    "# 'bagging_fraction': [0.6, 0.8, 1],\n",
    "# 'bagging_freq': [1, 3, 5],\n",
    "# 'lambda_l1': [0, 1, 2],\n",
    "# 'lambda_l2': [0, 1, 2]\n",
    "# }\n",
    "\n",
    "# best_map_score = 0.0\n",
    "# best_model = None\n",
    "\n",
    "# for params_dict in ParameterGrid(param_grid):\n",
    "#     params.update(params_dict)\n",
    "#     model = lgb.train(params, train_data)\n",
    "#     y_pred = model.predict(X_test, group=query)\n",
    "#     map_score = mean_average_precision(y_test, y_pred, k=12)\n",
    "#     if map_score > best_map_score:\n",
    "#         best_map_score = map_score\n",
    "#         best_model = model\n",
    "#         with open(f\"lightgbm/grid_search_model_{map_score:.4f}.pickle\", 'wb') as f:\n",
    "#             pickle.dump(model, f)\n",
    "\n",
    "# # save the best model\n",
    "# if not os.path.exists('lightgbm'):\n",
    "#     os.makedirs('lightgbm')\n",
    "# with open('lightgbm/best_model.pickle', 'wb') as f:\n",
    "#     pickle.dump(best_model, f)\n",
    "\n",
    "# # print the best MAP score\n",
    "# print(f\"Best mean average precision: {best_map_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "\n",
    "\n",
    "\n",
    "- Lightgbm training with training with MAP as eval metric, grid search for hyperparams (ref. kaggle for starting params) (in built train test split? or by dates?)\n",
    "- Lightgbm recommendation example\n",
    "  \n",
    "\n",
    "- Baseline model evalutaion for top 200 (same train test split as Lightgbm)\n",
    "\n",
    "- Redo ALS for top 200 (same train test split as Lightgbm, import user_item_matrix)\n",
    "\n",
    "\n",
    "- comparison of ALS and Lightgbm and baseline model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
