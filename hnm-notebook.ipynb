{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<a id=\"Content\">HnM RecSys Notebook 9417</a>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": false,
    "tags": []
   },
   "source": [
    "## **<a id=\"Content\">Table of Contents</a>**\n",
    "* [**<span>1. Imports</span>**](#Imports)  \n",
    "* [**<span>2. Pre-Processing</span>**](#Pre-Processing)\n",
    "* [**<span>3. Exploratory Data Analysis</span>**](#Exploratory-Data-Analysis)  \n",
    "    * [**<span>3.1 Articles</span>**](#EDA::Articles)  \n",
    "    * [**<span>3.2 Customers</span>**](#EDA::Customers)\n",
    "    * [**<span>3.3 Transactions</span>**](#EDA::Transactions)\n",
    "* [**<span>4. Helper FunctionsDecorators</span>**](#Helper-Functions)\n",
    "* [**<span>5. Models</span>**](#Models) \n",
    "    * [**<span>5.1 Popularity</span>**](#Popularity-Model)   \n",
    "    * [**<span>5.2 ALS</span>**](#Alternating-Least-Squares)  \n",
    "    * [**<span>5.2 GBDT</span>**](#GBDT)  \n",
    "    * [**<span>5.3 SGD/similar</span>**](#SGD)  \n",
    "    * [**<span>5.4 NN</span>**](#NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "# import cudf # switch on P100 GPU for this to work in Kaggle\n",
    "# import cupy as cp\n",
    "\n",
    "# Importing data\n",
    "articles = pd.read_csv('articles.csv')\n",
    "print(articles.head())\n",
    "print(\"--\")\n",
    "customers = pd.read_csv('customers.csv')\n",
    "print(customers.head())\n",
    "print(\"--\")\n",
    "transactions = pd.read_csv(\"transactions_train.csv\")\n",
    "print(transactions.head())\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- empty value stats -------------\n",
    "print(\"Missing values: \")\n",
    "print(customers.isnull().sum())\n",
    "print(\"--\\n\")\n",
    "\n",
    "print(\"FN Newsletter vals: \", customers['FN'].unique())\n",
    "print(\"Active communication vals: \",customers['Active'].unique())\n",
    "print(\"Club member status vals: \", customers['club_member_status'].unique())\n",
    "print(\"Fashion News frequency vals: \", customers['fashion_news_frequency'].unique())\n",
    "print(\"--\\n\")\n",
    "\n",
    "# ---- data cleaning -------------\n",
    "\n",
    "customers['FN'] = customers['FN'].fillna(0)\n",
    "customers['Active'] = customers['Active'].fillna(0)\n",
    "\n",
    "# replace club_member_status missing values with 'LEFT CLUB' --> no members with LEFT CLUB status in data\n",
    "customers['club_member_status'] = customers['club_member_status'].fillna('LEFT CLUB')\n",
    "customers['fashion_news_frequency'] = customers['fashion_news_frequency'].fillna('None')\n",
    "customers['fashion_news_frequency'] = customers['fashion_news_frequency'].replace('NONE', 'None')\n",
    "customers['age'] = customers['age'].fillna(customers['age'].mean())\n",
    "customers['age'] = customers['age'].astype(int)\n",
    "articles['detail_desc'] = articles['detail_desc'].fillna('None')\n",
    "\n",
    "\n",
    "print(\"Customers' Missing values: \")\n",
    "print(customers.isnull().sum())\n",
    "print(\"--\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- memory optimizations -------------\n",
    "\n",
    "# reference: https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\n",
    "\n",
    "# iterate through all the columns of a dataframe and reduce the int and float data types to the smallest possible size, ex. customer_id should not be reduced from int64 to a samller value as it would have collisions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"Iterate over all the columns of a DataFrame and modify the data type\n",
    "    to reduce memory usage, handling ordered Categoricals\"\"\"\n",
    "    \n",
    "    # check the memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type == 'category':\n",
    "            if df[col].cat.ordered:\n",
    "                # Convert ordered Categorical to an integer\n",
    "                df[col] = df[col].cat.codes.astype('int16')\n",
    "            else:\n",
    "                # Convert unordered Categorical to a string\n",
    "                df[col] = df[col].astype('str')\n",
    "        \n",
    "        elif col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    # check the memory usage after optimization\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "\n",
    "    # calculate the percentage of the memory usage reduction\n",
    "    mem_reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(\"Memory usage decreased by {:.1f}%\".format(mem_reduction))\n",
    "    \n",
    "    return df\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Articles Info: \")\n",
    "print(articles.info())\n",
    "print(\"Customer Info: \")\n",
    "print(customers.info())\n",
    "print(\"Transactions Info: \")\n",
    "print(transactions.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print unique values of customer columns\n",
    "print(\"FN Newsletter vals: \", customers['FN'].unique())\n",
    "print(\"Active communication vals: \",customers['Active'].unique())\n",
    "print(\"Club member status vals: \", customers['club_member_status'].unique())\n",
    "print(\"Fashion News frequency vals: \", customers['fashion_news_frequency'].unique())\n",
    "print(\"--\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly convert club_member_status to ordinal values before mem optimization to avoid errors\n",
    "\n",
    "customers['club_member_status'].replace({'LEFT CLUB': 0, 'PRE-CREATE': 1, 'ACTIVE': 2}, inplace=True)\n",
    "customers['club_member_status'] = customers['club_member_status'].astype('int8')\n",
    "print(customers['club_member_status'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- memory optimizations -------------\n",
    "\n",
    "# uses 8 bytes instead of given 64 byte string, reduces mem by 8x, \n",
    "# !!!! have to convert back before merging w/ sample_submissions.csv\n",
    "# convert transactions['customer_id'] to 8 bytes int\n",
    "# transactions['customer_id'] = transactions['customer_id'].astype('int64')\n",
    "transactions['customer_id'] = transactions['customer_id'].apply(lambda x: int(x[-16:], 16)).astype('int64')\n",
    "customers['customer_id'] = customers['customer_id'].apply(lambda x: int(x[-16:], 16)).astype('int64')\n",
    "\n",
    "articles = reduce_mem_usage(articles)\n",
    "customers = reduce_mem_usage(customers)\n",
    "transactions = reduce_mem_usage(transactions)\n",
    "\n",
    "# articles['article_id'] = articles['article_id'].astype('int32')\n",
    "# transactions['article_id'] = transactions['article_id'].astype('int32') \n",
    "# # !!!! ADD LEADING ZERO BACK BEFORE SUBMISSION OF PREDICTIONS TO KAGGLE: \n",
    "# # Ex.: transactions['article_id'] = '0' + transactions.article_id.astype('str')\n",
    "\n",
    "print(\"Articles Info: \")\n",
    "print(articles.info())\n",
    "print(\"Customer Info: \")\n",
    "print(customers.info())\n",
    "print(\"Transactions Info: \")\n",
    "print(transactions.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print unique values of customer columns\n",
    "print(\"FN Newsletter vals: \", customers['FN'].unique())\n",
    "print(\"Active communication vals: \",customers['Active'].unique())\n",
    "print(\"Club member status vals: \", customers['club_member_status'].unique())\n",
    "print(\"Fashion News frequency vals: \", customers['fashion_news_frequency'].unique())\n",
    "print(\"--\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-based splitting strategy\n",
    "\n",
    "def split_train_val_data_and_drop_duplicates(transactions, days=7):\n",
    "    \"\"\"\n",
    "    Splits the transaction training data into a training set and a validation set of 7 days to prevent data leakage.\n",
    "    \"\"\"\n",
    "    \n",
    "    transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "    transactions = transactions.sort_values(by=['t_dat'])\n",
    "    latest_transaction_date = transactions['t_dat'].max()\n",
    "    \n",
    "    training_set = transactions[transactions['t_dat'] < latest_transaction_date - pd.Timedelta(days=days)]\n",
    "    validation_set = transactions[transactions['t_dat'] >= latest_transaction_date - pd.Timedelta(days=days)]\n",
    "    \n",
    "    print(\"Training set size:\", len(training_set))\n",
    "    print(\"Validation set size:\", len(validation_set))\n",
    "    print(\"Last date in training set:\", training_set['t_dat'].max())\n",
    "    print(\"Last date in validation set:\", validation_set['t_dat'].max())\n",
    "\n",
    "    # drop duplicate rows\n",
    "    training_set = training_set.drop_duplicates().copy()\n",
    "    validation_set = validation_set.drop_duplicates().copy()\n",
    "    \n",
    "    return training_set, validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(transactions_df, customers_df, articles_df, customers_col='customer_id', articles_col='article_id'):\n",
    "    \"\"\"\n",
    "    Preprocesses customer and article IDs for use in a sparse matrix.\n",
    "    \n",
    "    Returns:\n",
    "    - transactions_df: the input transaction DataFrame with two additional columns, 'user_index' and 'item_index',\n",
    "                       that map customer and article IDs to their corresponding indices in a sparse matrix\n",
    "    - customer_id_indices_map: a dictionary that maps customer IDs to their corresponding indices\n",
    "    - article_id_indices_map: a dictionary that maps article IDs to their corresponding indices\n",
    "    \"\"\"\n",
    "    # Create a list of unique customer IDs and product IDs\n",
    "    all_customers = customers_df[customers_col].unique().tolist()\n",
    "    all_articles = articles_df[articles_col].unique().tolist()\n",
    "\n",
    "    # Create dicts mapping IDs to their corresponding indices\n",
    "    customer_id_indices_map = {customer_id: i for i, customer_id in enumerate(all_customers)}\n",
    "    article_id_indices_map = {article_id: i for i, article_id in enumerate(all_articles)}\n",
    "\n",
    "    # Map customer and article IDs to their resp. indices in the transaction DataFrame\n",
    "    transactions_df['user_index'] = transactions_df[customers_col].map(customer_id_indices_map)\n",
    "    transactions_df['item_index'] = transactions_df[articles_col].map(article_id_indices_map)\n",
    "\n",
    "    return transactions_df, all_customers, all_articles, customer_id_indices_map, article_id_indices_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# binary purchase interaction user-item matrix\n",
    "\n",
    "def create_user_item_matrix(transactions):\n",
    "\n",
    "    # Get unique user and item indices in asc. order\n",
    "    user_indices = np.arange(transactions['user_index'].nunique())\n",
    "    item_indices = np.arange(transactions['item_index'].nunique())\n",
    "\n",
    "    # Create a dictionary mapping user and item indices to matrix indices\n",
    "    user_index_dict = dict(zip(sorted(transactions['user_index'].unique()), user_indices))\n",
    "    item_index_dict = dict(zip(sorted(transactions['item_index'].unique()), item_indices))\n",
    "\n",
    "    # Create arrays of row indices, column indices, and data for the sparse matrix\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = [] # purchased 1 or 0\n",
    "\n",
    "    # Iterate over all possible combinations of user and item indices\n",
    "    for user_index in user_indices:\n",
    "        for item_index in item_indices:\n",
    "            # Get the corresponding matrix indices for the user and item indices\n",
    "            matrix_user_index = user_index_dict.get(user_index)\n",
    "            matrix_item_index = item_index_dict.get(item_index)\n",
    "            # Get the corresponding interaction value from the transactions dataframe\n",
    "            interaction = transactions.loc[(transactions['user_index'] == user_index) & \n",
    "                                            (transactions['item_index'] == item_index), 'quantity'].values\n",
    "            # Append the row index, column index, and interaction value to the corresponding arrays\n",
    "            rows.append(matrix_user_index)\n",
    "            cols.append(matrix_item_index)\n",
    "            data.append(1 if len(interaction) > 0 else 0)\n",
    "\n",
    "    # Create the sparse matrix using the row, column, and data arrays\n",
    "    user_item_matrix = csr_matrix((data, (rows, cols)), shape=(len(user_indices), len(item_indices)))\n",
    "\n",
    "    return user_item_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "# calculate total number of transaction weeks in tranactions data\n",
    "transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "\n",
    "# Compute the minimum and maximum date values\n",
    "min_date = transactions['t_dat'].min()\n",
    "max_date = transactions['t_dat'].max()\n",
    "\n",
    "# Compute the number of weeks between the minimum and maximum date values\n",
    "num_weeks = ceil((max_date - min_date).days / 7)\n",
    "\n",
    "print(f\"Total number of transaction weeks: {num_weeks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# only use last x weeks of transactions data since data is too large\n",
    "def filter_transactions_last_x_weeks(transactions, x = 10):\n",
    "    # Convert date strings to datetime objects\n",
    "    transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "\n",
    "    # Calculate the date x weeks ago from the latest transaction date\n",
    "    latest_date = transactions['t_dat'].max()\n",
    "    cutoff_date = latest_date - timedelta(weeks=x)\n",
    "\n",
    "    # Filter transactions to only include those in the last x weeks\n",
    "    filtered_transactions = transactions.loc[transactions['t_dat'] >= cutoff_date].copy()\n",
    "\n",
    "    return filtered_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_customers_and_articles(customers, articles, filtered_transactions):\n",
    "    # Get unique customer and article IDs from filtered transactions\n",
    "    customer_ids = filtered_transactions['customer_id'].unique()\n",
    "    article_ids = filtered_transactions['article_id'].unique()\n",
    "\n",
    "    # Filter customers and articles to only include those in filtered transactions\n",
    "    customers_filtered = customers.loc[customers['customer_id'].isin(customer_ids)].copy()\n",
    "    articles_filtered = articles.loc[articles['article_id'].isin(article_ids)].copy()\n",
    "\n",
    "    return customers_filtered, articles_filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|LightGBM|XGBoost|CatBoost|\n",
    "|:----|:----|:----|:----|\n",
    "|Categoricals|Supports categorical features via one-hot encoding|Supports categorical features via one-hot encoding|Automatically handles categorical features using embeddings|\n",
    "|Speed|Very fast training and prediction|Fast training and prediction|Slower than LightGBM and XGBoost|\n",
    "|Handling Bias|Handles unbalanced classes via 'is_unbalance'|Handles unbalanced classes via 'scale_pos_weight'|Automatically handles unbalanced classes|\n",
    "|Handling NaNs|Handles NaN values natively|Requires manual handling of NaNs|Automatically handles NaN values using special category|\n",
    "|Custom Loss|Supports custom loss functions|Supports custom loss functions|Supports custom loss functions|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LightGBM for a ranking problem, we could also treat this as a binary classification problem where the target variable is whether an item is relevant or not to the user.\n",
    "\n",
    "OR, use LightGBM's ranking API, which is designed for ranking problems. Instead of optimizing for accuracy, the ranking API optimizes for ranking metric MAP (deprecated). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM imports\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 200 customers by number of transactions\n",
    "top_customers = transactions['customer_id'].value_counts().head(200).index.tolist()\n",
    "\n",
    "# print num of transactions for the 200th customer\n",
    "print(transactions['customer_id'].value_counts().sort_values(ascending=False).iloc[199])\n",
    "\n",
    "# only get articles that were purchased by top 200 customers at least once in articles df\n",
    "articles_top_200 = articles[articles['article_id'].isin(transactions[transactions['customer_id'].isin(top_customers)]['article_id'].unique())]\n",
    "\n",
    "# only get 200 customers in customers df\n",
    "customers_top_200 = customers[customers['customer_id'].isin(top_customers)]\n",
    "\n",
    "articles = articles_top_200.copy()\n",
    "customers = customers_top_200.copy()\n",
    "transactions = transactions[transactions['customer_id'].isin(top_customers)].copy()\n",
    "transactions = transactions.drop_duplicates().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transactions.isnull().sum())\n",
    "print(customers.isnull().sum())\n",
    "print(articles.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(transactions))\n",
    "print(len(customers))\n",
    "print(len(articles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with uninformative article data\n",
    "\n",
    "articles = articles.drop(columns=['product_code', 'prod_name', 'product_type_name', 'product_group_name', 'graphical_appearance_name', 'department_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name', 'detail_desc'])\n",
    "articles = articles.drop(columns=[col for col in articles.columns if 'colour_' in col or 'perceived_' in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns are left to capture any potential patterns in the other columns, such as how certain index codes or sections might be associated with higher or lower sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define mapping for fashion_news_frequency feature\n",
    "fashion_news_freq_mapping = {'None': 0, 'Monthly': 1, 'Regularly': 2}\n",
    "\n",
    "# label encode fashion_news_frequency feature\n",
    "le = LabelEncoder()\n",
    "customers['fashion_news_frequency'] = customers['fashion_news_frequency'].map(fashion_news_freq_mapping)\n",
    "customers['fashion_news_frequency'] = le.fit_transform(customers['fashion_news_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers.drop(['postal_code'], axis=1)\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: encode nominal categorical features\n",
    "ohe = OneHotEncoder()\n",
    "# One-hot encode sales_channel_id feature\n",
    "sales_channel_ohe = pd.get_dummies(transactions['sales_channel_id'], prefix='sales_channel')\n",
    "transactions = pd.concat([transactions, sales_channel_ohe], axis=1)\n",
    "\n",
    "# Drop the original sales_channel_id feature\n",
    "transactions.drop('sales_channel_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean unique values of sales_channel_id after encoding\n",
    "print(transactions['sales_channel_1'].unique())\n",
    "print(transactions['sales_channel_2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 't_dat' column to datetime format\n",
    "transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "\n",
    "# Group by customer ID and find the first and last transaction dates\n",
    "first_trans_dates = transactions.groupby('customer_id')['t_dat'].min().reset_index()\n",
    "last_trans_dates = transactions.groupby('customer_id')['t_dat'].max().reset_index()\n",
    "\n",
    "customer_purchase_engagement = pd.merge(first_trans_dates, last_trans_dates, on='customer_id', suffixes=('_first', '_last'))\n",
    "# Create a new feature by calculating the time difference in days between first and last transactions\n",
    "customer_purchase_engagement['time_diff_days'] = (customer_purchase_engagement['t_dat_last'] - customer_purchase_engagement['t_dat_first']).dt.days\n",
    "# Drop the original first and last transaction date columns\n",
    "customer_purchase_engagement.drop(['t_dat_first', 't_dat_last'], axis=1, inplace=True)\n",
    "customer_purchase_engagement.head()\n",
    "\n",
    "# Merge the customer_purchase_engagement dataframe with the customers dataframe\n",
    "customers = pd.merge(customers, customer_purchase_engagement, on='customer_id', how='left')\n",
    "customers.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above `time_diff_days` feature can potentially provide insights into a customer's engagement by looking at the gap in the number of days between the last purchase and the current date. <br> The assumption is, The larger the gap, the less engaged the customer is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the transaction dataframe with the customers dataframe\n",
    "merged = pd.merge(transactions, customers, on='customer_id', how='inner')\n",
    "\n",
    "# Calculate the mean age for each article\n",
    "item_mean_age = merged.groupby('article_id')['age'].mean()\n",
    "\n",
    "# Calculate the difference between every user's age and the mean age of users who have purchased a particular item\n",
    "merged['age_diff'] = merged['age'] - merged['article_id'].map(item_mean_age)\n",
    "\n",
    "# Group by article and take the mean of age_diff\n",
    "article_age_diff = merged.groupby('article_id')['age_diff'].mean()\n",
    "\n",
    "# Append the age difference feature to the articles dataframe\n",
    "articles['age_diff'] = articles['article_id'].map(article_age_diff)\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean age_diff for every article. It can be useful for predicting whether a user will buy an item based on their age and the age of other users who have already bought the same item. \n",
    "\n",
    "Intuituion behind the `age_diff` feature:\n",
    "\n",
    "Let's say we have a dataset of customers who made transactions for a particular item with article_id = 123. Here is an example of how we can calculate the age_diff feature: <br>\n",
    "Assume that the mean age of all customers who bought the item with article_id = 123 is 40 years old <br>\n",
    "Customer A made a transaction for item with article_id = 123 and their age is 35. The age_diff feature for this transaction would be -5. (35 - 40). <br>\n",
    "Customer B made a transaction for item with article_id = 123 and their age is 50. The age_diff feature for this transaction would be 10. (50 - 40). <br>\n",
    "Customer C made a transaction for item with article_id = 123 and their age is 40. The age_diff feature for this transaction would be 0. (40 - 40). <br>\n",
    "So, the age_diff feature measures the difference between the age of each customer who bought a specific item and the average age of all customers who bought that item. <br>\n",
    "\n",
    "Therefore, the age_diff is the mean of all these individual age_diff values for each customer who bought the item with article_id = 123. age_diff = -1.66 for this example<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean, max, and min age for each item\n",
    "item_mean_age = merged.groupby('article_id')['age'].mean()\n",
    "item_max_age = merged.groupby('article_id')['age'].max()\n",
    "item_min_age = merged.groupby('article_id')['age'].min()\n",
    "\n",
    "# Merge the features back into the articles dataframe\n",
    "articles = articles.merge(item_mean_age, on='article_id', how='left')\n",
    "articles = articles.merge(item_max_age, on='article_id', how='left')\n",
    "articles = articles.merge(item_min_age, on='article_id', how='left')\n",
    "\n",
    "# Rename the columns to make them more descriptive\n",
    "articles = articles.rename(columns={'age_x': 'mean_purchase_age', 'age_y': 'max_purchase_age', 'age': 'min_purchase_age'})\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuituion behind the `*_purchase_age` feature:\n",
    "\n",
    "Additional age features to capture more information about the age of the customers who bought the respective articles. The gbdt might be able to learn more complex patterns from these features. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate purchased item count for each user\n",
    "transactions['quantity'] = 1\n",
    "user_item_count = transactions.groupby(['customer_id', 'article_id'])['quantity'].sum().reset_index()\n",
    "\n",
    "# Calculate total item count for each article\n",
    "total_item_count = transactions.groupby('article_id')['quantity'].sum().reset_index()\n",
    "total_item_count.columns = ['article_id', 'total_items']\n",
    "\n",
    "user_item_count = pd.merge(user_item_count, total_item_count, on='article_id', how='left')\n",
    "\n",
    "# Calculate ratio of purchased item count and total item count\n",
    "user_item_count['article_engagement_ratio'] = user_item_count['quantity'] / user_item_count['total_items']\n",
    "\n",
    "\n",
    "transactions = pd.merge(transactions, user_item_count[['customer_id', 'article_id', 'article_engagement_ratio']], on=['customer_id', 'article_id'], how='left')\n",
    "transactions['quantity'] = user_item_count['quantity']\n",
    "\n",
    "# fill missing values with 0\n",
    "transactions['quantity'] = transactions['quantity'].fillna(0)\n",
    "transactions.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition behind feature: <br>\n",
    "\n",
    "`article_engagement_ratio`: The feature is ratio of one user's purchased item count and the item's total purchase count. This serves to measure how engaged a user is with a particular item, which can be useful for predicting whether a user will buy similar items. <br>\n",
    "Can also be used to measure how popular an item is, and can be used to potentially diversify recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions, all_customers, all_articles, customer_id_indices_map, article_id_indices_map = preprocess_data(transactions, customers, articles)\n",
    "\n",
    "print(\"Total num of customers: \", len(all_customers))\n",
    "print(\"Total num of articles: \", len(all_articles))\n",
    "print(\"Customer ID mapping: \", list(customer_id_indices_map.items())[:5])\n",
    "print(\"Article ID mapping: \", list(article_id_indices_map.items())[:5])\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user item matrix -- rows are users, columns are items, doesnt need article and customer data\n",
    "\n",
    "user_item_matrix = create_user_item_matrix(transactions)\n",
    "user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_item_matrix[:10, :10].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.als import AlternatingLeastSquares\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# from the als_strat1_hyperparam_log \n",
    "# Create ALS model with default parameters\n",
    "alpha = 25\n",
    "als_model = AlternatingLeastSquares(factors=55, iterations=20, regularization=0.18)\n",
    "\n",
    "# Fit model to user-item matrix\n",
    "als_model.fit(user_item_matrix*alpha)\n",
    "\n",
    "# Latent factors matrices\n",
    "item_factors = als_model.item_factors\n",
    "user_factors = als_model.user_factors\n",
    "\n",
    "# item-item cosine similarity \n",
    "item_similarities = cosine_similarity(item_factors, dense_output=False)\n",
    "# user-user cosine similarity\n",
    "user_similarities = cosine_similarity(user_factors, dense_output=False)\n",
    "\n",
    "k = 5\n",
    "# Get top-k most similar items for each item\n",
    "top_k_similar_items = item_similarities.argsort()[:, -k-1:-1]\n",
    "# Get top-k most similar user for each user\n",
    "top_k_similar_users = user_similarities.argsort()[:, -k-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item_similarities.shape)\n",
    "print(user_similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important: add user_index and item_index to customers and articles respectively\n",
    "customers['user_index'] = customers['customer_id'].map(customer_id_indices_map)\n",
    "articles['item_index'] = articles['article_id'].map(article_id_indices_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indices = np.arange(transactions['user_index'].nunique())\n",
    "item_indices = np.arange(transactions['item_index'].nunique())\n",
    "user_index_dict = dict(zip(sorted(transactions['user_index'].unique()), user_indices))\n",
    "item_index_dict = dict(zip(sorted(transactions['item_index'].unique()), item_indices))\n",
    "\n",
    "# print first 5 key and values in the dictionary\n",
    "print(list(user_index_dict.items())[:5])\n",
    "print(list(item_index_dict.items())[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from user IDs to matrix indices\n",
    "user_indices = np.arange(transactions['user_index'].nunique())\n",
    "item_indices = np.arange(transactions['item_index'].nunique())\n",
    "user_index_dict = dict(zip(sorted(transactions['user_index'].unique()), user_indices))\n",
    "item_index_dict = dict(zip(sorted(transactions['item_index'].unique()), item_indices))\n",
    "\n",
    "\n",
    "# Create features for each user based on the average quantity purchased by similar customers\n",
    "for i in range(len(customers)):\n",
    "    customer_id = customers.loc[i, 'user_index']\n",
    "    # Get the matrix index for the current customer\n",
    "    customer_idx = user_index_dict.get(customer_id, -1)\n",
    "    \n",
    "    if customer_idx != -1:\n",
    "        similar_user_indexxs = top_k_similar_users[customer_idx]\n",
    "        # Compute the mean of the user-item matrix for the similar users\n",
    "        user_feature = user_item_matrix[similar_user_indexxs, :].mean(axis=0).A1\n",
    "        \n",
    "        # Store the mean in the customers DataFrame\n",
    "        customers.loc[i, 'user_purchase_quant'] = user_feature.mean()\n",
    "    else:\n",
    "        # If the current customer is not in the user-item matrix, set the feature to NaN\n",
    "        customers.loc[i, 'user_purchase_quant'] = np.nan\n",
    "\n",
    "# Print the head of the customers DataFrame\n",
    "customers.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition behind feature: <br>\n",
    "\n",
    "`user_purchase_quant`: Gets the average quantity of items purchased by the k most similar customers to that customer. It looks at what other customers who are similar to this customer have bought and calculates the average amount of each item they bought. This feature can be used to predict what items a customer is likely to buy in the future based on what similar customers have bought in the past. This feature aims to capture purchase behaviours of a customer.<br>\n",
    "\n",
    "For example, if a customer typically buys a lot of bomber jackets, and the top k most similar customers to that customer also tend to buy a lot of bomber jackets, then the average quantity of bomber jackets purchased by those similar customers could be a good predictor of how much the original customer is likely to purchase in the future. This however assumes that the k most similar customers have similar purchase behaviours to the customers in question, and on its own is not a strong feature.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary feature for each item that indicates whether or not a customer has bought that item,\n",
    "# based on whether other customers who bought similar items also tended to buy that item.\n",
    "for i in range(len(articles)):\n",
    "    item_id = articles.loc[i, 'item_index']\n",
    "    # Get the matrix index for the current item\n",
    "    item_idx = item_index_dict.get(item_id, -1)\n",
    "    \n",
    "    if item_idx != -1:\n",
    "        # List of (column) indices of similar items\n",
    "        similar_items = top_k_similar_items[item_idx]\n",
    "        \n",
    "        # Find the customers who have purchased the current item\n",
    "        customer_indices = np.where(user_item_matrix[:, item_idx].toarray()[:, 0] == 1)[0]\n",
    "        \n",
    "        # Binary vector representing the customer's purchases for the similar items\n",
    "        customer_purchases = user_item_matrix[customer_indices, :][:, similar_items].toarray()\n",
    "        article_preference = np.any(customer_purchases, axis=0)\n",
    "        \n",
    "        # Set article_preference to 1 if any customer has purchased the item, 0 otherwise\n",
    "        articles.loc[i, 'article_preference'] = int(np.any(article_preference))\n",
    "    else:\n",
    "        articles.loc[i, 'article_preference'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[articles['article_preference'] == 0].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition behind feature: <br>\n",
    "\n",
    "`article_preference`: Binary feature for each item that indicates whether or not a customer has bought that item, based on whether other customers who bought similar items also tended to buy that item. <br>\n",
    "\n",
    "This feature can be useful for a fashion-based recommender system because it captures the idea that customers who have similar tastes or preferences tend to buy similar items. For example, if a customer has a history of buying shirts and other customers who bought similar shorts also tended to buy a specific pair of jeans, then the binary feature for those jeans would be set to 1 for that customer, indicating that they are likely to be interested in those shoes. The binary feature can then be used as a predictor for which items to recommend to the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature for each item based on the total number of times it was purchased by similar customers\n",
    "for i in range(len(articles)):\n",
    "    item_id = articles.loc[i, 'item_index']\n",
    "    item_idx = item_index_dict.get(item_id, -1)\n",
    "    \n",
    "    if item_idx != -1:\n",
    "    \n",
    "        similar_items = top_k_similar_items[item_idx]\n",
    "        \n",
    "        # Find the customers who have purchased the current item\n",
    "        customer_indices = np.where(user_item_matrix[:, item_idx].toarray()[:, 0] == 1)[0]\n",
    "        \n",
    "        # Compute the mean of the user-item matrix for the similar items\n",
    "        item_feature = user_item_matrix[customer_indices, :][:, similar_items].sum() / len(customer_indices)\n",
    "        articles.loc[i, 'item_purchase_frequency'] = item_feature.mean()\n",
    "        \n",
    "    else:\n",
    "        articles.loc[i, 'item_purchase_frequency'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition behind feature: <br>\n",
    "\n",
    "`item_purchase_frequency`: It gives an estimate of how frequently an item is being bought by customers who have similar purchase histories. This feature is useful because it can provide insights into purchasing patterns and identify popular items that are often bought together. <br> It can potentially help identify popular items among customer groups, and the lightgbm model can potentially use this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(transactions, articles[['item_index']], on='item_index', how='left')\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge transactions with articles on item_index to access the price of each article\n",
    "merged_df = pd.merge(transactions, articles[['item_index']], on='item_index', how='left')\n",
    "\n",
    "# Compute the average price levels of all articles purchased by similar customers who have purchased this particular article in the past\n",
    "for i in range(len(articles)):\n",
    "    item_id = articles.loc[i, 'item_index']\n",
    "    item_idx = item_index_dict.get(item_id, -1)\n",
    "    \n",
    "    if item_idx != -1:\n",
    "        similar_items = top_k_similar_items[item_idx]\n",
    "        \n",
    "        # Find the customers who have purchased the current item\n",
    "        customer_indices = np.where(user_item_matrix[:, item_idx].toarray()[:, 0] == 1)[0]\n",
    "        \n",
    "        # Compute the average price levels of all articles purchased by similar customers who have purchased this particular article in the past\n",
    "        item_feature = merged_df.loc[(merged_df['item_index'] == item_id) & (merged_df['user_index'].isin(customer_indices)), 'price'].mean()\n",
    "        articles.loc[i, 'item_avg_price_level'] = item_feature\n",
    "    else:\n",
    "        articles.loc[i, 'item_avg_price_level'] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition behind feature: <br>\n",
    "\n",
    "`item_avg_price_level`: Calculates the average price levels of all articles purchased by similar customers who have purchased this particular article in the past.  <br> It provides information on the typical price level of articles that are purchased together with a given article, as indicated by the purchasing patterns of similar customers. \n",
    "\n",
    "For example, if customers who frequently purchase articles A also tend to purchase higher-priced article, then the \"item_avg_price_level\" feature for article A would be relatively high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print num of unique articles['graphical_appearance_no']\n",
    "print(articles['graphical_appearance_no'].nunique())\n",
    "print(articles['product_type_no'].nunique())\n",
    "print(articles['department_no'].nunique())\n",
    "print(articles['index_code'].nunique())\n",
    "print(articles['index_group_no'].nunique()) \n",
    "print(articles['section_no'].nunique())\n",
    "print(articles['garment_group_no'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already dropped for articles\n",
    "# articles_final_df = articles.drop(['article_id'], axis=1).copy()\n",
    "customers_final_df = customers.drop(['customer_id'], axis=1).copy()\n",
    "transactions_final_df = transactions.drop(['article_id', 'customer_id'], axis=1).copy()\n",
    "\n",
    "# Merge transactions with customers\n",
    "df = pd.merge(transactions_final_df, customers_final_df, on='user_index', how='left')\n",
    "\n",
    "# Merge resulting dataframe with articles_final_df usually\n",
    "df = pd.merge(df, articles, on='item_index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exttracting time-based features\n",
    "\n",
    "df['t_dat'] = pd.to_datetime(df['t_dat'])\n",
    "df['year'] = df['t_dat'].dt.year\n",
    "df['month'] = df['t_dat'].dt.month\n",
    "df['day'] = df['t_dat'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM features, reference: https://www.geeksforgeeks.org/rfm-analysis-analysis-using-python/\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate recency\n",
    "last_purchase_date = transactions.groupby('user_index')['t_dat'].max().reset_index()\n",
    "last_purchase_date.columns = ['user_index', 'last_purchase_date']\n",
    "last_purchase_date['recency'] = (last_purchase_date['last_purchase_date'].max() - last_purchase_date['last_purchase_date']).dt.days\n",
    "last_purchase_date.drop('last_purchase_date', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Calculate Frequency\n",
    "frequency = transactions.groupby('user_index')['t_dat'].count().reset_index()\n",
    "frequency.columns = ['user_index', 'frequency']\n",
    "\n",
    "# Calculate Monetary Value\n",
    "monetary_value = transactions.groupby('user_index')['price'].sum().reset_index()\n",
    "monetary_value.columns = ['user_index', 'monetary_value']\n",
    "\n",
    "# Merge all RFM features into a single DataFrame\n",
    "rfm = last_purchase_date[['user_index', 'recency']].merge(frequency, on='user_index').merge(monetary_value, on='user_index')\n",
    "\n",
    "# Calculate RFM Scores\n",
    "quantiles = rfm.quantile(q=[0.25, 0.5, 0.75])\n",
    "quantiles = quantiles.to_dict()\n",
    "\n",
    "def rfm_segmenter(x, quantiles):\n",
    "    if x <= quantiles['recency'][0.25]:\n",
    "        return 4\n",
    "    elif x <= quantiles['recency'][0.50]:\n",
    "        return 3\n",
    "    elif x <= quantiles['recency'][0.75]: \n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "rfm['R'] = rfm['recency'].apply(rfm_segmenter, args=(quantiles,))\n",
    "\n",
    "def f_segmenter(x, quantiles):\n",
    "    if x <= quantiles['frequency'][0.25]:\n",
    "        return 1\n",
    "    elif x <= quantiles['frequency'][0.50]:\n",
    "        return 2\n",
    "    elif x <= quantiles['frequency'][0.75]: \n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "rfm['F'] = rfm['frequency'].apply(f_segmenter, args=(quantiles,))\n",
    "\n",
    "def m_segmenter(x, quantiles):\n",
    "    if x <= quantiles['monetary_value'][0.25]:\n",
    "        return 1\n",
    "    elif x <= quantiles['monetary_value'][0.50]:\n",
    "        return 2\n",
    "    elif x <= quantiles['monetary_value'][0.75]: \n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "rfm['M'] = rfm['monetary_value'].apply(m_segmenter, args=(quantiles,))\n",
    "\n",
    "# Calculate RFM Score\n",
    "rfm['RFM Score'] = rfm['R'].map(str) + rfm['F'].map(str) + rfm['M'].map(str)\n",
    "rfm = rfm.drop(['R', 'F', 'M'], axis=1)\n",
    "\n",
    "# Display sample of RFM DataFrame\n",
    "print(rfm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge rfm with df on user_index\n",
    "\n",
    "df = pd.merge(df, rfm, on='user_index', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df.drop(['t_dat'], axis=1).copy()\n",
    "# print all column names in final_df\n",
    "print(final_df.columns)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['RFM Score'] = pd.to_numeric(final_df['RFM Score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame as a pickle file\n",
    "df.to_pickle('lightgbm/df.pkl')\n",
    "final_df.to_pickle('lightgbm/final_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print final_df shape\n",
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indices, item_indices = user_item_matrix.get_shape()\n",
    "\n",
    "print('Number of users: %d' % user_indices)\n",
    "print('Number of items: %d' % item_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()\n",
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assume you have a CSR matrix called user_item_matrix\n",
    "# Save the matrix as a pickle file\n",
    "with open('user_item_matrix_200.pkl', 'wb') as f:\n",
    "    pickle.dump(user_item_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load user_item_matrix from pickle file\n",
    "\n",
    "with open('user_item_matrix_200.pkl', 'rb') as f:\n",
    "    user_item_matrix = pickle.load(f)\n",
    "\n",
    "user_item_matrix = user_item_matrix.toarray()\n",
    "\n",
    "# extract indices of non-zero elements\n",
    "user_purchased_indices, item_purchased_indices = user_item_matrix.nonzero()\n",
    "\n",
    "print('user_purchased_indices: ', user_indices)\n",
    "print('item_purchased_indices: ', item_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to hold the dummy data\n",
    "\n",
    "# load final_df from pickle file for clean processing\n",
    "with open('lightgbm/final_df.pkl', 'rb') as f:\n",
    "    final_df = pickle.load(f)\n",
    "\n",
    "dummy_data = []\n",
    "\n",
    "# get the unique user and item indices from final_df\n",
    "users = final_df['user_index'].unique()\n",
    "items = final_df['item_index'].unique()\n",
    "\n",
    "# final_df.shape[1]\n",
    "# loop through all possible user-item pairs\n",
    "for user in users:\n",
    "    for item in items:\n",
    "        # check if the user-item pair has an interaction in the sparse matrix\n",
    "        if user_item_matrix[user, item] == 1:\n",
    "            # if it does, set the target of the corresponding row in final_df to 1\n",
    "            final_df.loc[(final_df['user_index'] == user) & (final_df['item_index'] == item), 'target'] = 1\n",
    "        else:\n",
    "            # if it doesn't, add a row to the dummy data with target = 0 and stub values for other columns\n",
    "            dummy_data.append([user, item] + [np.nan] * (final_df.shape[1] - 2))\n",
    "\n",
    "\n",
    "dummy_df = pd.DataFrame(dummy_data, columns=['user_index', 'item_index'] + list(final_df.columns.drop(['user_index', 'item_index'])))\n",
    "dummy_df['target'] = 0  # set target to 0 for the dummy data\n",
    "final_df = pd.concat([final_df, dummy_df], ignore_index=True)\n",
    "\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop recency\tfrequency\tmonetary_value columns\n",
    "final_df = final_df.drop(['recency', 'frequency', 'monetary_value'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode garment_group_no and index_group_no columns\n",
    "one_hot_cols = ['garment_group_no', 'index_group_no']\n",
    "final_df = pd.get_dummies(final_df, columns=one_hot_cols)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame as a pickle file\n",
    "final_df.to_pickle('lightgbm/final_df_with_binary_targets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the article_id_indices_map and user_id_indices_map as pickle files\n",
    "with open('lightgbm/article_id_indices_map.pkl', 'wb') as f:\n",
    "    pickle.dump(article_id_indices_map, f)\n",
    "with open('lightgbm/customer_id_indices_map.pkl', 'wb') as f:\n",
    "    pickle.dump(customer_id_indices_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename RFM_Score to RFM_Score\n",
    "final_df.rename(columns={'RFM Score': 'RFM_Score'}, inplace=True)\n",
    "# convert sales_channel_ 1 to to boolean\n",
    "final_df['sales_channel_1'] = final_df['sales_channel_1'].astype('bool')\n",
    "final_df['sales_channel_2'] = final_df['sales_channel_2'].astype('bool')\n",
    "final_df.to_pickle('lightgbm/final_df_with_binary_targets.pkl')\n",
    "# final_df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load final_df from pickle file for clean processing\n",
    "with open('lightgbm/final_df_with_binary_targets.pkl', 'rb') as f:\n",
    "    final_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'sales_channel_1', 'sales_channel_2', 'quantity',\n",
       "       'article_engagement_ratio', 'user_index', 'item_index', 'FN', 'Active',\n",
       "       'club_member_status', 'fashion_news_frequency', 'age', 'time_diff_days',\n",
       "       'user_purchase_quant', 'product_type_no', 'graphical_appearance_no',\n",
       "       'department_no', 'section_no', 'age_diff', 'mean_purchase_age',\n",
       "       'max_purchase_age', 'min_purchase_age', 'article_preference',\n",
       "       'item_purchase_frequency', 'item_avg_price_level', 'year', 'month',\n",
       "       'day', 'RFM Score', 'target', 'garment_group_no_1001.0',\n",
       "       'garment_group_no_1002.0', 'garment_group_no_1003.0',\n",
       "       'garment_group_no_1005.0', 'garment_group_no_1006.0',\n",
       "       'garment_group_no_1007.0', 'garment_group_no_1008.0',\n",
       "       'garment_group_no_1009.0', 'garment_group_no_1010.0',\n",
       "       'garment_group_no_1011.0', 'garment_group_no_1012.0',\n",
       "       'garment_group_no_1013.0', 'garment_group_no_1014.0',\n",
       "       'garment_group_no_1016.0', 'garment_group_no_1017.0',\n",
       "       'garment_group_no_1018.0', 'garment_group_no_1019.0',\n",
       "       'garment_group_no_1020.0', 'garment_group_no_1021.0',\n",
       "       'garment_group_no_1023.0', 'garment_group_no_1025.0',\n",
       "       'index_group_no_1.0', 'index_group_no_2.0', 'index_group_no_3.0',\n",
       "       'index_group_no_4.0', 'index_group_no_26.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_1</th>\n",
       "      <th>sales_channel_2</th>\n",
       "      <th>quantity</th>\n",
       "      <th>article_engagement_ratio</th>\n",
       "      <th>user_index</th>\n",
       "      <th>item_index</th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>club_member_status</th>\n",
       "      <th>...</th>\n",
       "      <th>garment_group_no_1019.0</th>\n",
       "      <th>garment_group_no_1020.0</th>\n",
       "      <th>garment_group_no_1021.0</th>\n",
       "      <th>garment_group_no_1023.0</th>\n",
       "      <th>garment_group_no_1025.0</th>\n",
       "      <th>index_group_no_1.0</th>\n",
       "      <th>index_group_no_2.0</th>\n",
       "      <th>index_group_no_3.0</th>\n",
       "      <th>index_group_no_4.0</th>\n",
       "      <th>index_group_no_26.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>11563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>9899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>14438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>10307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>10</td>\n",
       "      <td>13608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      price sales_channel_1 sales_channel_2  quantity   \n",
       "0  0.042358             0.0             1.0       1.0  \\\n",
       "1  0.050842             0.0             1.0       1.0   \n",
       "2  0.067810             0.0             1.0       1.0   \n",
       "3  0.016937             0.0             1.0       1.0   \n",
       "4  0.016937             0.0             1.0       1.0   \n",
       "\n",
       "   article_engagement_ratio  user_index  item_index   FN  Active   \n",
       "0                  1.000000           5       11563  1.0     1.0  \\\n",
       "1                  1.000000           5        9899  1.0     1.0   \n",
       "2                  1.000000           5       14438  1.0     1.0   \n",
       "3                  0.500000          10       10307  0.0     0.0   \n",
       "4                  0.166667          10       13608  0.0     0.0   \n",
       "\n",
       "   club_member_status  ...  garment_group_no_1019.0  garment_group_no_1020.0   \n",
       "0                 2.0  ...                    False                    False  \\\n",
       "1                 2.0  ...                    False                    False   \n",
       "2                 2.0  ...                    False                    False   \n",
       "3                 2.0  ...                    False                    False   \n",
       "4                 2.0  ...                    False                    False   \n",
       "\n",
       "   garment_group_no_1021.0  garment_group_no_1023.0  garment_group_no_1025.0   \n",
       "0                    False                    False                    False  \\\n",
       "1                    False                    False                    False   \n",
       "2                    False                    False                    False   \n",
       "3                    False                    False                    False   \n",
       "4                    False                     True                    False   \n",
       "\n",
       "   index_group_no_1.0  index_group_no_2.0  index_group_no_3.0   \n",
       "0                True               False               False  \\\n",
       "1                True               False               False   \n",
       "2                True               False               False   \n",
       "3               False                True               False   \n",
       "4                True               False               False   \n",
       "\n",
       "   index_group_no_4.0  index_group_no_26.0  \n",
       "0               False                False  \n",
       "1               False                False  \n",
       "2               False                False  \n",
       "3               False                False  \n",
       "4               False                False  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_1</th>\n",
       "      <th>sales_channel_2</th>\n",
       "      <th>quantity</th>\n",
       "      <th>article_engagement_ratio</th>\n",
       "      <th>user_index</th>\n",
       "      <th>item_index</th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>club_member_status</th>\n",
       "      <th>...</th>\n",
       "      <th>garment_group_no_1019.0</th>\n",
       "      <th>garment_group_no_1020.0</th>\n",
       "      <th>garment_group_no_1021.0</th>\n",
       "      <th>garment_group_no_1023.0</th>\n",
       "      <th>garment_group_no_1025.0</th>\n",
       "      <th>index_group_no_1.0</th>\n",
       "      <th>index_group_no_2.0</th>\n",
       "      <th>index_group_no_3.0</th>\n",
       "      <th>index_group_no_4.0</th>\n",
       "      <th>index_group_no_26.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042358</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>11563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050842</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>9899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067810</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>14438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>10307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>10</td>\n",
       "      <td>13608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67829</th>\n",
       "      <td>0.019821</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>146</td>\n",
       "      <td>29230</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67830</th>\n",
       "      <td>0.045746</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>146</td>\n",
       "      <td>27217</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67831</th>\n",
       "      <td>0.038116</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>146</td>\n",
       "      <td>23690</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67832</th>\n",
       "      <td>0.015236</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>146</td>\n",
       "      <td>349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67833</th>\n",
       "      <td>0.030487</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>146</td>\n",
       "      <td>1875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          price  sales_channel_1  sales_channel_2  quantity   \n",
       "0      0.042358            False             True       1.0  \\\n",
       "1      0.050842            False             True       1.0   \n",
       "2      0.067810            False             True       1.0   \n",
       "3      0.016937            False             True       1.0   \n",
       "4      0.016937            False             True       1.0   \n",
       "...         ...              ...              ...       ...   \n",
       "67829  0.019821            False             True       2.0   \n",
       "67830  0.045746            False             True       1.0   \n",
       "67831  0.038116            False             True       2.0   \n",
       "67832  0.015236            False             True       1.0   \n",
       "67833  0.030487            False             True       1.0   \n",
       "\n",
       "       article_engagement_ratio  user_index  item_index   FN  Active   \n",
       "0                      1.000000           5       11563  1.0     1.0  \\\n",
       "1                      1.000000           5        9899  1.0     1.0   \n",
       "2                      1.000000           5       14438  1.0     1.0   \n",
       "3                      0.500000          10       10307  0.0     0.0   \n",
       "4                      0.166667          10       13608  0.0     0.0   \n",
       "...                         ...         ...         ...  ...     ...   \n",
       "67829                  0.100000         146       29230  1.0     1.0   \n",
       "67830                  0.062500         146       27217  1.0     1.0   \n",
       "67831                  0.125000         146       23690  1.0     1.0   \n",
       "67832                  0.062500         146         349  1.0     1.0   \n",
       "67833                  0.125000         146        1875  1.0     1.0   \n",
       "\n",
       "       club_member_status  ...  garment_group_no_1019.0   \n",
       "0                     2.0  ...                    False  \\\n",
       "1                     2.0  ...                    False   \n",
       "2                     2.0  ...                    False   \n",
       "3                     2.0  ...                    False   \n",
       "4                     2.0  ...                    False   \n",
       "...                   ...  ...                      ...   \n",
       "67829                 2.0  ...                    False   \n",
       "67830                 2.0  ...                    False   \n",
       "67831                 2.0  ...                    False   \n",
       "67832                 2.0  ...                    False   \n",
       "67833                 2.0  ...                    False   \n",
       "\n",
       "       garment_group_no_1020.0  garment_group_no_1021.0   \n",
       "0                        False                    False  \\\n",
       "1                        False                    False   \n",
       "2                        False                    False   \n",
       "3                        False                    False   \n",
       "4                        False                    False   \n",
       "...                        ...                      ...   \n",
       "67829                    False                    False   \n",
       "67830                    False                    False   \n",
       "67831                     True                    False   \n",
       "67832                    False                    False   \n",
       "67833                    False                    False   \n",
       "\n",
       "       garment_group_no_1023.0  garment_group_no_1025.0  index_group_no_1.0   \n",
       "0                        False                    False                True  \\\n",
       "1                        False                    False                True   \n",
       "2                        False                    False                True   \n",
       "3                        False                    False               False   \n",
       "4                         True                    False                True   \n",
       "...                        ...                      ...                 ...   \n",
       "67829                    False                    False                True   \n",
       "67830                    False                    False                True   \n",
       "67831                    False                    False                True   \n",
       "67832                    False                    False                True   \n",
       "67833                    False                    False               False   \n",
       "\n",
       "       index_group_no_2.0  index_group_no_3.0  index_group_no_4.0   \n",
       "0                   False               False               False  \\\n",
       "1                   False               False               False   \n",
       "2                   False               False               False   \n",
       "3                    True               False               False   \n",
       "4                   False               False               False   \n",
       "...                   ...                 ...                 ...   \n",
       "67829               False               False               False   \n",
       "67830               False               False               False   \n",
       "67831               False               False               False   \n",
       "67832               False               False               False   \n",
       "67833                True               False               False   \n",
       "\n",
       "       index_group_no_26.0  \n",
       "0                    False  \n",
       "1                    False  \n",
       "2                    False  \n",
       "3                    False  \n",
       "4                    False  \n",
       "...                    ...  \n",
       "67829                False  \n",
       "67830                False  \n",
       "67831                False  \n",
       "67832                False  \n",
       "67833                False  \n",
       "\n",
       "[1000 rows x 56 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data = final_df.groupby('user_index')\n",
    "grouped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of duplicate rows in final_df\n",
    "final_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 101390, number of negative: 6141050\n",
      "[LightGBM] [Info] Total groups: 200, total data: 6242440\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2284\n",
      "[LightGBM] [Info] Number of data points in the train set: 6242440, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016242 -> initscore=-4.103777\n",
      "[LightGBM] [Info] Start training from score -4.103777\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "average_precision_score() got an unexpected keyword argument 'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     user_y_pred \u001b[39m=\u001b[39m y_pred[user_mask]\n\u001b[0;32m     45\u001b[0m     user_item_indices \u001b[39m=\u001b[39m X_test[user_mask][\u001b[39m'\u001b[39m\u001b[39mitem_index\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 47\u001b[0m     user_map_score \u001b[39m=\u001b[39m average_precision_score(user_y_test, user_y_pred, average\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmicro\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m     48\u001b[0m                                               pos_label\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, k\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m)\n\u001b[0;32m     49\u001b[0m     map_score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m user_map_score\n\u001b[0;32m     51\u001b[0m map_score \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_test[\u001b[39m'\u001b[39m\u001b[39muser_index\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique())\n",
      "\u001b[1;31mTypeError\u001b[0m: average_precision_score() got an unexpected keyword argument 'k'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "features = final_df.columns.tolist()\n",
    "features.remove('target')\n",
    "target = 'target'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_df[features], final_df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Group data by user -- so that LightGBM knows which data points belong to each user and can compute the metrics correctly\n",
    "grouped_data_train = X_train.groupby('user_index')\n",
    "grouped_data_test = X_test.groupby('user_index')\n",
    "\n",
    "# Create LightGBM datasets with gorup query information\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=grouped_data_train.size())\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=grouped_data_test.size())\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "params = {'objective': 'binary',\n",
    "          'boosting_type': 'gbdt',\n",
    "          'metric': 'map',\n",
    "          'num_leaves': 31,\n",
    "          'learning_rate': 0.05,\n",
    "          'feature_fraction': 0.9,\n",
    "          'bagging_fraction': 0.8,\n",
    "          'bagging_freq': 5,\n",
    "          'verbose': 1}\n",
    "\n",
    "# Train the model\n",
    "clf = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "# Group the test data by user ID and pass the group information to the LightGBM dataset\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=X_test['user_index'])\n",
    "\n",
    "y_pred = clf.predict(X_test, num_iteration=clf.best_iteration)\n",
    "\n",
    "# Compute MAP@12\n",
    "map_score = 0\n",
    "for user_index in X_test['user_index'].unique():\n",
    "    user_mask = (X_test['user_index'] == user_index)\n",
    "    user_y_test = y_test[user_mask]\n",
    "    user_y_pred = y_pred[user_mask]\n",
    "    user_item_indices = X_test[user_mask]['item_index']\n",
    "    \n",
    "    user_map_score = average_precision_score(user_y_test, user_y_pred, average='micro', \n",
    "                                              pos_label=1, k=12)\n",
    "    map_score += user_map_score\n",
    "\n",
    "map_score /= len(X_test['user_index'].unique())\n",
    "print(f'Mean average precision: {map_score:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, it can be used to predict the probability of purchase for new user-product pairs, which can be used to generate recommendations for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X is the input data for the LightGBM model\n",
    "# X has a row for each user-product pair and a binary target indicating whether the user purchased the product or not\n",
    "\n",
    "# Train the LightGBM model on X\n",
    "# lgb_model = lgb.LGBMClassifier(**best_params)\n",
    "# lgb_model.fit(X, y)\n",
    "\n",
    "# Generate candidate products for each user\n",
    "# This can be done using a combination of popular products and user purchase history\n",
    "# Let's assume we have a dictionary 'user_products' that maps each user ID to a list of products they've purchased\n",
    "user_candidates = {}\n",
    "for user_id in user_products:\n",
    "    # Select the 600 most popular products\n",
    "    popular_products = select_popular_products(600)\n",
    "    \n",
    "    # Add user purchase history to candidate list\n",
    "    user_history = user_products[user_id]\n",
    "    candidate_products = list(set(popular_products + user_history))\n",
    "    \n",
    "    # Store candidate products for this user\n",
    "    user_candidates[user_id] = candidate_products\n",
    "\n",
    "# Predict probabilities of purchase for each candidate product for each user\n",
    "user_scores = {}\n",
    "for user_id, candidates in user_candidates.items():\n",
    "    # Create input data for this user\n",
    "    user_data = create_user_data(user_id, candidates)\n",
    "    \n",
    "    # Predict probabilities using the LightGBM model\n",
    "    scores = lgb_model.predict_proba(user_data)[:, 1]\n",
    "    \n",
    "    # Store scores for this user\n",
    "    user_scores[user_id] = scores\n",
    "\n",
    "# Rank candidate products for each user and return top 12 as recommendations\n",
    "recommendations = {}\n",
    "for user_id, scores in user_scores.items():\n",
    "    # Sort candidate products by descending score\n",
    "    candidate_products = user_candidates[user_id]\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_products = [candidate_products[i] for i in sorted_indices]\n",
    "    \n",
    "    # Select top 12 products\n",
    "    top_products = sorted_products[:12]\n",
    "    \n",
    "    # Add user purchase history to top products\n",
    "    top_products += user_products[user_id]\n",
    "    \n",
    "    # Remove duplicates and return as recommendations\n",
    "    recommendations[user_id] = list(set(top_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'query_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'query_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 31\u001b[0m\n\u001b[0;32m     22\u001b[0m param_grid \u001b[39m=\u001b[39m {\n\u001b[0;32m     23\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_leaves\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m31\u001b[39m, \u001b[39m63\u001b[39m, \u001b[39m127\u001b[39m],\n\u001b[0;32m     24\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m7\u001b[39m],\n\u001b[0;32m     25\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.05\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m0.2\u001b[39m],\n\u001b[0;32m     26\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcolsample_bytree\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.5\u001b[39m, \u001b[39m1.0\u001b[39m]\n\u001b[0;32m     27\u001b[0m }\n\u001b[0;32m     29\u001b[0m \u001b[39m# Define the query information\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# Assume each sample belongs to a query specified by the 'query_id' column\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m query_ids \u001b[39m=\u001b[39m X_train_encoded[\u001b[39m'\u001b[39;49m\u001b[39mquery_id\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m     33\u001b[0m \u001b[39m# Perform grid search with cross-validation using query information\u001b[39;00m\n\u001b[0;32m     34\u001b[0m cv_results \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'query_id'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "# def map_12_score(y_true, y_pred):\n",
    "#     # Assume y_true is a list of true labels for each query\n",
    "#     # and y_pred is a list of predicted scores for each query\n",
    "#     return np.mean([average_precision_score(t[:12], p[:12]) for t, p in zip(y_true, y_pred)])\n",
    "\n",
    "# # perform train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(final_df.drop('target', axis=1), final_df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # define target encoding encoder -- thse columns ahve a lot of values, ohe will create a lot of columns, label encoding will create unwanted bias to higher values\n",
    "# encoder = ce.TargetEncoder(cols=['department_no', 'product_type_no', 'section_no', 'graphical_appearance_no'])\n",
    "\n",
    "# # fit and transform the encoder on training data only\n",
    "# X_train_encoded = encoder.fit_transform(X_train, y_train)\n",
    "\n",
    "# # Define the LightGBM model and parameter grid\n",
    "# lgb_model = lgb.LGBMClassifier()\n",
    "# param_grid = {\n",
    "#     'num_leaves': [31, 63, 127],\n",
    "#     'max_depth': [-1, 3, 7],\n",
    "#     'learning_rate': [0.05, 0.1, 0.2],\n",
    "#     'colsample_bytree': [0.5, 1.0]\n",
    "# }\n",
    "\n",
    "# # Define the query information\n",
    "# # Assume each sample belongs to a query specified by the 'query_id' column\n",
    "# query_ids = X_train_encoded['query_id'].values\n",
    "\n",
    "# # Perform grid search with cross-validation using query information\n",
    "# cv_results = {}\n",
    "# best_score = -1\n",
    "# for num_leaves in param_grid['num_leaves']:\n",
    "#     for max_depth in param_grid['max_depth']:\n",
    "#         for learning_rate in param_grid['learning_rate']:\n",
    "#             for colsample_bytree in param_grid['colsample_bytree']:\n",
    "#                 params = {\n",
    "#                     'num_leaves': num_leaves,\n",
    "#                     'max_depth': max_depth,\n",
    "#                     'learning_rate': learning_rate,\n",
    "#                     'colsample_bytree': colsample_bytree,\n",
    "#                     'objective': 'binary',\n",
    "#                     'metric': 'None'  # We'll use our custom scorer instead since in-built metrics need query information\n",
    "#                 }\n",
    "#                 lgb_model.set_params(**params)\n",
    "#                 cv_score = cross_val_score(lgb_model, X_train_encoded, y_train,\n",
    "#                                            groups=query_ids, cv=5,\n",
    "#                                            scoring=make_scorer(map_score, needs_proba=True))\n",
    "#                 mean_map = np.mean(cv_score)\n",
    "#                 if mean_map > best_score:\n",
    "#                     best_score = mean_map\n",
    "#                     best_params = params\n",
    "#                     print(f\"New best hyperparameters found: {best_params}, mean average precision: {best_score}\")\n",
    "#                     # save best model so far -- optimized for lightgbm instead of .pkl\n",
    "#                     lgb_model.booster_.save_model('lightgbm/best_model.txt')\n",
    "#                 cv_results[str(params)] = mean_map\n",
    "\n",
    "# print(f'Best parameters: {best_params}')\n",
    "# print(f'Best MAP score: {best_score}')\n",
    "# print('CV results:')\n",
    "# for params, score in cv_results.items():\n",
    "#     print(f'{params}: {score}')\n",
    "\n",
    "# # re-fit the encoder on the full training data (w/o cross-validation) and transform the test data\n",
    "# encoder.fit(X_train, y_train)\n",
    "# X_train_encoded = encoder.transform(X_train)\n",
    "# X_test_encoded = encoder.transform(X_test)\n",
    "\n",
    "# # train final model with best hyperparameters on the encoded training data and save to disk\n",
    "# lgb_model = lgb.LGBMClassifier(**best_params)\n",
    "# lgb_model.fit(X_train_encoded, y_train)\n",
    "# lgb_model.booster_.save_model('lightgbm/best_model.txt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat this as a binary classification problem, we would be ignoring the importance of the ranking of the recommended items and the MAP metric would not be appropriate. Since we are using MAP as the evaluation metric, we should use the LightGBM ranking API instead of the binary classification API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import make_scorer\n",
    "# from sklearn.metrics import average_precision_score\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# target = 'item_index'\n",
    "# features = final_df.columns.tolist()\n",
    "# features.remove(target)\n",
    "\n",
    "# # split the data into training and test sets -- can also do time-based split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(final_df[features], final_df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# # for number of items to rank for each user (group param for ordered ranking)\n",
    "# num_items_per_user = 12\n",
    "# user_indices = X_test.index.unique()\n",
    "# query = [num_items_per_user] * len(user_indices)\n",
    "# query_ids = []\n",
    "# for user_index in user_indices:\n",
    "#     user_indices_repeated = [user_index] * num_items_per_user\n",
    "#     query_ids.extend(user_indices_repeated)\n",
    "\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, group=query_ids)\n",
    "\n",
    "# # MAP@12 metric\n",
    "# def mean_average_precision(y_true, y_score, k=12):\n",
    "#     # get the indices of the top k scores\n",
    "#     top_k_indices = np.argsort(y_score)[::-1][:k]\n",
    "\n",
    "#     # calculate average precision at k\n",
    "#     return average_precision_score(y_true[top_k_indices], y_score[top_k_indices])\n",
    "\n",
    "# # define hyperparameters for tuning\n",
    "# params = {\n",
    "# 'objective': 'lambdarank', #using lightgbm ranking API\n",
    "# 'metric': 'MAP',\n",
    "# 'learning_rate': 0.05,\n",
    "# 'num_leaves': 31,\n",
    "# 'max_depth': 5,\n",
    "# 'min_data_in_leaf': 50,\n",
    "# 'feature_fraction': 0.8,\n",
    "# 'bagging_fraction': 0.8,\n",
    "# 'bagging_freq': 5\n",
    "# }\n",
    "\n",
    "# # create LightGBM model\n",
    "# model = lgb.LGBMRanker()\n",
    "\n",
    "# # perform grid search with cross-validation\n",
    "# param_grid = {\n",
    "# 'num_leaves': [31, 50, 75],\n",
    "# 'max_depth': [5, 7, -1],\n",
    "# 'min_data_in_leaf': [20, 50, 100],\n",
    "# 'feature_fraction': [0.6, 0.8, 1],\n",
    "# 'bagging_fraction': [0.6, 0.8, 1],\n",
    "# 'bagging_freq': [1, 3, 5],\n",
    "# 'lambda_l1': [0, 1, 2],\n",
    "# 'lambda_l2': [0, 1, 2]\n",
    "# }\n",
    "\n",
    "# best_map_score = 0.0\n",
    "# best_model = None\n",
    "\n",
    "# for params_dict in ParameterGrid(param_grid):\n",
    "#     params.update(params_dict)\n",
    "#     model = lgb.train(params, train_data)\n",
    "#     y_pred = model.predict(X_test, group=query)\n",
    "#     map_score = mean_average_precision(y_test, y_pred, k=12)\n",
    "#     if map_score > best_map_score:\n",
    "#         best_map_score = map_score\n",
    "#         best_model = model\n",
    "#         with open(f\"lightgbm/grid_search_model_{map_score:.4f}.pickle\", 'wb') as f:\n",
    "#             pickle.dump(model, f)\n",
    "\n",
    "# # save the best model\n",
    "# if not os.path.exists('lightgbm'):\n",
    "#     os.makedirs('lightgbm')\n",
    "# with open('lightgbm/best_model.pickle', 'wb') as f:\n",
    "#     pickle.dump(best_model, f)\n",
    "\n",
    "# # print the best MAP score\n",
    "# print(f\"Best mean average precision: {best_map_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "\n",
    "\n",
    "\n",
    "- Lightgbm training with training with MAP as eval metric, grid search for hyperparams (ref. kaggle for starting params) (in built train test split? or by dates?)\n",
    "- Lightgbm recommendation example\n",
    "  \n",
    "\n",
    "- Baseline model evalutaion for top 200 (same train test split as Lightgbm)\n",
    "\n",
    "- Redo ALS for top 200 (same train test split as Lightgbm, import user_item_matrix)\n",
    "\n",
    "\n",
    "- comparison of ALS and Lightgbm and baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
