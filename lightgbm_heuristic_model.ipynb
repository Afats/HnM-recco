{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<a id=\"Content\">HnM RecSys Notebook 9417</a>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": false,
    "tags": []
   },
   "source": [
    "## **<a id=\"Content\">Table of Contents</a>**\n",
    "* [**<span>1. Imports</span>**](#Imports)  \n",
    "* [**<span>2. Helper Functions/Decorators</span>**](#Helper-Functions)\n",
    "* [**<span>5. LightGBM Model</span>**](#LightGBM-Model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# only use last x weeks of transactions data since data is too large\n",
    "def filter_transactions_last_x_weeks(transactions, x = 10):\n",
    "    # Convert date strings to datetime objects\n",
    "    transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "\n",
    "    # Calculate the date x weeks ago from the latest transaction date\n",
    "    latest_date = transactions['t_dat'].max()\n",
    "    cutoff_date = latest_date - timedelta(weeks=x)\n",
    "\n",
    "    # Filter transactions to only include those in the last x weeks\n",
    "    filtered_transactions = transactions.loc[transactions['t_dat'] >= cutoff_date].copy()\n",
    "\n",
    "    return filtered_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_customers_and_articles(customers, articles, filtered_transactions):\n",
    "    # Get unique customer and article IDs from filtered transactions\n",
    "    customer_ids = filtered_transactions['customer_id'].unique()\n",
    "    article_ids = filtered_transactions['article_id'].unique()\n",
    "\n",
    "    # Filter customers and articles to only include those in filtered transactions\n",
    "    customers_filtered = customers.loc[customers['customer_id'].isin(customer_ids)].copy()\n",
    "    articles_filtered = articles.loc[articles['article_id'].isin(article_ids)].copy()\n",
    "\n",
    "    return customers_filtered, articles_filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison of the top GBDT models today. LightGBM is the fastest to train."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|LightGBM|XGBoost|CatBoost|\n",
    "|:----|:----|:----|:----|\n",
    "|Categoricals|Supports categorical features via one-hot encoding|Supports categorical features via one-hot encoding|Automatically handles categorical features using embeddings|\n",
    "|Speed|Very fast training and prediction|Fast training and prediction|Slower than LightGBM and XGBoost|\n",
    "|Handling Bias|Handles unbalanced classes via 'is_unbalance'|Handles unbalanced classes via 'scale_pos_weight'|Automatically handles unbalanced classes|\n",
    "|Handling NaNs|Handles NaN values natively|Requires manual handling of NaNs|Automatically handles NaN values using special category|\n",
    "|Custom Loss|Supports custom loss functions|Supports custom loss functions|Supports custom loss functions|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LightGBM for a ranking problem, we treat this as a binary classification problem where the target variable is whether an item is relevant or not to the user.\n",
    "\n",
    "Alternatively, we can use LightGBM's ranking API, which is designed for ranking problems. Instead of optimizing for accuracy, the ranking API optimizes for ranking metric MAP (MAP support deprecated however). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM imports\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# open user_item_matrix_200\n",
    "with open('user_item_matrix_200.pkl', 'rb') as f:\n",
    "    user_item_matrix = pickle.load(f)\n",
    "\n",
    "# open customer and articels incides map\n",
    "with open('lightgbm/customer_id_indices_map.pkl', 'rb') as f:\n",
    "    customer_id_indices_map = pickle.load(f)\n",
    "\n",
    "with open('lightgbm/article_id_indices_map.pkl', 'rb') as f:\n",
    "    article_id_indices_map = pickle.load(f)\n",
    "\n",
    "# load df from pickle file for time-based split\n",
    "with open('lightgbm/df.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# load final_df from pickle file for clean processing\n",
    "with open('lightgbm/final_df_with_binary_targets.pkl', 'rb') as f:\n",
    "    final_df = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows where target is 0 (no purchase)\n",
    "final_df = final_df[final_df['target'] == 1].copy()\n",
    "\n",
    "# Convert days, months, and years columns to datetime object\n",
    "final_df['date'] = pd.to_datetime(final_df[['day', 'month', 'year']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target encoding\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define columns to target encode\n",
    "cols_to_encode = ['department_no', 'product_type_no', 'section_no', 'graphical_appearance_no']\n",
    "\n",
    "# Define number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Create KFold object for cross-validation\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform target encoding with cross-validation\n",
    "for col in cols_to_encode:\n",
    "    final_df[f'{col}_te'] = 0\n",
    "    te = TargetEncoder(cols=[col])\n",
    "    for train_idx, val_idx in kf.split(final_df):\n",
    "        te.fit(final_df.iloc[train_idx][[col]], final_df.iloc[train_idx]['target'])\n",
    "        final_df.loc[val_idx, f'{col}_te'] = te.transform(final_df.iloc[val_idx][[col]]).values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- memory optimizations -------------\n",
    "\n",
    "# reference: https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\n",
    "\n",
    "# iterate through all the columns of a dataframe and reduce the int and float data types to the smallest possible size, ex. customer_id should not be reduced from int64 to a samller value as it would have collisions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"Iterate over all the columns of a DataFrame and modify the data type\n",
    "    to reduce memory usage, handling ordered Categoricals\"\"\"\n",
    "    \n",
    "    # check the memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type == 'category':\n",
    "            if df[col].cat.ordered:\n",
    "                # Convert ordered Categorical to an integer\n",
    "                df[col] = df[col].cat.codes.astype('int16')\n",
    "            else:\n",
    "                # Convert unordered Categorical to a string\n",
    "                df[col] = df[col].astype('str')\n",
    "        \n",
    "        elif col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    # check the memory usage after optimization\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "\n",
    "    # calculate the percentage of the memory usage reduction\n",
    "    mem_reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(\"Memory usage decreased by {:.1f}%\".format(mem_reduction))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_train_test_split(final_df, test_size=0.2):\n",
    "\n",
    "    # Sort dataframe by date in ascending order\n",
    "    final_df = final_df.sort_values(by='date')\n",
    "\n",
    "    # Calculate cutoff index\n",
    "    cutoff_index = int(len(final_df) * (1-test_size))\n",
    "\n",
    "    # Create train and test dataframes\n",
    "    train_df = final_df[:cutoff_index]\n",
    "    test_df = final_df[cutoff_index:]\n",
    "\n",
    "    # Drop date column from train and test dataframes\n",
    "    train_df = train_df.drop('date', axis=1)\n",
    "    test_df = test_df.drop('date', axis=1)\n",
    "\n",
    "    # split train_df into X_train and y_train\n",
    "    X_train = train_df.drop('target', axis=1)\n",
    "    y_train = train_df['target']\n",
    "\n",
    "    # split test_df into X_test and y_test\n",
    "    X_test = test_df.drop('target', axis=1)\n",
    "    y_test = test_df['target']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 25.70 MB\n",
      "Memory usage after optimization is: 12.17 MB\n",
      "Memory usage decreased by 52.6%\n",
      "Memory usage of dataframe is 6.42 MB\n",
      "Memory usage after optimization is: 3.04 MB\n",
      "Memory usage decreased by 52.6%\n",
      "(101297, 59)\n",
      "(25325, 59)\n",
      "(101297,)\n",
      "(25325,)\n"
     ]
    }
   ],
   "source": [
    "# 80/20 time-based split to curb data leakage\n",
    "# X_train, X_test, y_train, y_test = time_based_train_test_split(final_df, test_size=0.2)\n",
    "# final_df_top_50 = final_df_top_50.drop('date', axis=1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_df.drop(['target'], axis=1), final_df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# drop the date column from X_train, X_test\n",
    "X_train = X_train.drop('date', axis=1)\n",
    "X_test = X_test.drop('date', axis=1)\n",
    "\n",
    "# redcue memory usage\n",
    "X_train = reduce_mem_usage(X_train)\n",
    "X_test = reduce_mem_usage(X_test)\n",
    "\n",
    "# print the shape of X_train, X_test, y_train, y_test\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "import joblib\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = final_df.columns.tolist()\n",
    "features.remove('target')\n",
    "target = 'target'\n",
    "\n",
    "# Group data by user -- so that LightGBM knows which data points belong to each user and can compute the metrics correctly\n",
    "grouped_data_train = X_train.groupby('user_index')\n",
    "grouped_data_test = X_test.groupby('user_index')\n",
    "groups_train = [grouped_data_train.groups[user] for user in grouped_data_train.groups.keys()]\n",
    "groups_train_flat = np.concatenate(groups_train)\n",
    "groups_test = [grouped_data_test.groups[user] for user in grouped_data_test.groups.keys()]\n",
    "\n",
    "# Create LightGBM datasets with group query information\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=grouped_data_train.groups.values())\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=grouped_data_test.groups.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Contains only one class\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[7]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[8]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[9]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[10]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[11]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[12]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[13]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[14]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[15]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[16]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[17]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[18]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[19]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[20]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[21]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[22]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[23]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[24]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[25]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[26]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[27]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[28]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[29]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[30]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[31]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[32]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[33]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[34]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[35]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[36]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[37]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[38]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[39]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[40]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[41]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[42]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[43]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[44]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[45]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[46]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[47]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[48]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[49]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[50]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[51]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n"
     ]
    }
   ],
   "source": [
    "# Define the LightGBM dataset for training and validation\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Define the parameters for the LightGBM model\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Train the LightGBM model\n",
    "num_rounds = 1000\n",
    "lgb_model = lgb.train(params, train_data, num_rounds, valid_sets=[train_data, val_data], early_stopping_rounds=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, it can be used to predict the probability of purchase for new user-product pairs, which can be used to generate recommendations for users."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat this as a binary classification problem: After training the model, we can then get the probability that each user is likely to purchase an item from a candidate set of items. We can then sort these by descending probability to get the top 12 products as done below. <br>\n",
    "\n",
    "A heuristic apparoach that we use to enhance LighGBM predictions here: <br>\n",
    "1. Get a candidate set of top 500 most popular articles (by total purchase quanitity). <br>\n",
    "2. Include the customer's predicitons to this set. <br>\n",
    "3. Use lightGBM to predict the probability of purchases, and get the top 12. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary 'user_products' that maps each user ID to a list of products they've purchased from the user-item matrix\n",
    "\n",
    "user_products = {}\n",
    "for user_idx in range(user_item_matrix.shape[0]):\n",
    "    purchased_items = list(np.where(user_item_matrix[user_idx, :].toarray()[0] == 1)[0])\n",
    "    user_products[user_idx] = purchased_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns set of most pupular products in the catalog\n",
    "\n",
    "def select_popular_products(df, n_products=500):\n",
    "    # Group the dataframe by user and product and sum the quantity for each group\n",
    "    product_quantities = df.groupby(['user_index', 'item_index'])['quantity'].sum()\n",
    "    # Sort the products by quantity in descending order and select the top n_products\n",
    "    popular_products = product_quantities.groupby('item_index').sum().sort_values(ascending=False).index.tolist()[:n_products]\n",
    "    # return only the unique item_index values\n",
    "    return list(set(popular_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "[0, 1, 28674, 28675, 28676, 7, 6154, 14352, 14354, 6171, 2077, 45, 16432, 49, 16433, 16434, 20531, 22581, 16437, 8247]\n"
     ]
    }
   ],
   "source": [
    "# Generate candidate products for each user\n",
    "# This can be done using a combination of popular products and user purchase history\n",
    "\n",
    "popular_products = select_popular_products(final_df, 500)\n",
    "print(len(popular_products))\n",
    "# print first 10 popular products\n",
    "print(popular_products[:20])\n",
    "\n",
    "user_candidates = {}\n",
    "for user_id in user_products:\n",
    "    \n",
    "    # Add user purchase history to candidate list\n",
    "    user_history = user_products[user_id]\n",
    "    candidate_products = list(set(popular_products + user_history))\n",
    "    \n",
    "    # Store candidate products (dataframes) for this user\n",
    "    user_articles = final_df[final_df['item_index'].isin(candidate_products)]\n",
    "    user_article_info = user_articles.groupby('item_index').first().reset_index()\n",
    "    user_article_info = user_article_info.drop(['date', 'target'], axis=1)\n",
    "    user_candidates[user_id] = user_article_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check where columns of X_test and user_candidates[0] dont match\n",
    "for col in user_candidates[0].columns:\n",
    "    if col not in X_test.columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommended items for user 0:\n",
      "[38856 11466 12649 12643 12621 12564 12507 12493 12395 12315 12287 12257]\n",
      "Top recommended items for user 1:\n",
      "[37234  8419  7161  7120  7082  7080  7033  6961  6925  6895  6876  6874]\n",
      "Top recommended items for user 2:\n",
      "[38511 13926 13844 13840 13838 13802 13655 13610 13543 13542 13541 13418]\n",
      "Top recommended items for user 3:\n",
      "[38068 10366 10219 10209 10207 10087  9982  9952  9943  9758  9701  9579]\n",
      "Top recommended items for user 4:\n",
      "[38886 14132 14596 14568 14520 14419 14360 14354 14352 14280 14219 14197]\n",
      "Top recommended items for user 5:\n",
      "[38821 15100 14966 14967 14999 15024 15062 15064 15084 15142 14948 15155]\n",
      "Top recommended items for user 6:\n",
      "[38649 11975 11822 11821 11645 11552 11470 11280 11219 11210 11130 11105]\n",
      "Top recommended items for user 7:\n",
      "[38562 38072 12621 12507 12396 12394 12202 12077 12029 12014 12012 12010]\n",
      "Top recommended items for user 8:\n",
      "[38853 12014 12643 12621 12616 12507 12310 12279 12275 12265 12202 12062]\n",
      "Top recommended items for user 9:\n",
      "[38815 11408 11701 11645 11564 11552 11487 11422 11413 11412 11411 11410]\n",
      "Top recommended items for user 10:\n",
      "[37234 10684 10628 10625 10622 10619 10617 10616 10575 10377 10371 10370]\n",
      "Top recommended items for user 11:\n",
      "[38779 13762 14062 14035 14008 14007 13998 13967 13966 13939 13933 13926]\n",
      "Top recommended items for user 12:\n",
      "[38801 11219 11070 11078 11093 11105 11130 11210 11227 11066 11280 11286]\n",
      "Top recommended items for user 13:\n",
      "[38889 14419 14159 14165 14166 14179 14219 14352 14354 14439 15980 14568]\n",
      "Top recommended items for user 14:\n",
      "[38080  9384 10122  9952  9943  9833  9832  9778  9685  9585  9584  9579]\n",
      "Top recommended items for user 15:\n",
      "[38908 14631 14949 14948 14947 14945 14941 14939 14937 14894 14807 14727]\n",
      "Top recommended items for user 16:\n",
      "[37607  8582  8544  8528  8457  8456  8450  8419  8274  8248  8247  7992]\n",
      "Top recommended items for user 17:\n",
      "[37762 11130 11821 11780 11645 11643 11552 11351 11280 11219 11210 11199]\n",
      "Top recommended items for user 18:\n",
      "[37758 11054 11395 11280 11219 11210 11177 11130 11111 11105 11093 11062]\n",
      "Top recommended items for user 19:\n",
      "[38708 11062 10785 10811 10880 10950 11032 11059 11093 10686 11105 11111]\n",
      "Top recommended items for user 20:\n",
      "[38524 10616 10371 10370 10368 10366 10319 10318 10309 10209 10207 10060]\n",
      "Top recommended items for user 21:\n",
      "[38495 12905 13415 13414 13381 13367 13283 13280 13243 13207 13169 13166]\n",
      "Top recommended items for user 22:\n",
      "[38904 15527 14807 14785 14783 14739 14631 14617 14608 14597 14596 14574]\n",
      "Top recommended items for user 23:\n",
      "[38797 12996 13542 13541 13524 13523 13418 13417 13415 13414 13409 13381]\n",
      "Top recommended items for user 24:\n",
      "[38395 10371 10368 10366 10364 10297 10209 10207 10159 10141 10081 10076]\n",
      "Top recommended items for user 25:\n",
      "[38911 13823 13543 13610 13655 13692 13703 13711 13802 13824 14062 13862]\n",
      "Top recommended items for user 26:\n",
      "[38878 12774 14012 14008 14007 14001 13969 13966 13933 13926 13895 13894]\n",
      "Top recommended items for user 27:\n",
      "[38746 12643 13313 13303 13294 13189 12997 12925 12774 12654 12653 12649]\n",
      "Top recommended items for user 28:\n",
      "[38869 13580 13954 13951 13940 13933 13926 13913 13885 13883 13802 13749]\n",
      "Top recommended items for user 29:\n",
      "[38843 14557 14948 14947 14945 14941 14939 14937 14807 14648 14631 14610]\n",
      "Top recommended items for user 30:\n",
      "[38570 10314 10625 10619 10617 10616 10600 10558 10481 10378 10371 10370]\n",
      "Top recommended items for user 31:\n",
      "[38293 11210 11130 11108 11105 11099 11093 11062 11059 11005 11002 10983]\n",
      "Top recommended items for user 32:\n",
      "[38812 11389 12643 12621 12599 12507 12202 12026 12014 12012 12010 12009]\n",
      "Top recommended items for user 33:\n",
      "[38582 11105 11749 11707 11661 11646 11645 11552 11281 11280 11242 11219]\n",
      "Top recommended items for user 34:\n",
      "[37654 13543 13541 13520 13425 13418 13417 13415 13414 13397 13392 13381]\n",
      "Top recommended items for user 35:\n",
      "[38774 12524 12398 12316 12202 12014 12012 12010 12009 12008 11997 11979]\n",
      "Top recommended items for user 36:\n",
      "[38885 13908 13802 13848 13872 13873 13876 13877 13926 13381 13933 13966]\n",
      "Top recommended items for user 37:\n",
      "[38223 11645 12012 12010 12009 12008 11997 11975 11925 11907 11864 11822]\n",
      "Top recommended items for user 38:\n",
      "[38761 12621 13381 13200 12991 12935 12925 12921 12774 12682 12654 12653]\n",
      "Top recommended items for user 39:\n",
      "[38542 13381 13291 13112 12965 12925 12921 12774 12654 12653 12649 12643]\n",
      "Top recommended items for user 40:\n",
      "[38415 10497 10007 10009 10013 10035 10065 10173 10174 10207 10209 10299]\n",
      "Top recommended items for user 41:\n",
      "[38766 13686 14074 14062 14035 14008 14007 13991 13982 13966 13933 13926]\n",
      "Top recommended items for user 42:\n",
      "[38647  8813 10209 10207 10146 10129 10030  9952  9943  9886  9885  9794]\n",
      "Top recommended items for user 43:\n",
      "[38918 14572 14967 14966 14949 14948 14947 14945 14941 14939 14937 14807]\n",
      "Top recommended items for user 44:\n",
      "[37234  6975  6927  6874  6863  6860  6734  6725  6600  6597  6581  6579]\n",
      "Top recommended items for user 45:\n",
      "[38050 12649 13944 13933 13926 13863 13861 13802 13790 13655 13610 13543]\n",
      "Top recommended items for user 46:\n",
      "[38615  9952 11219 11210 11130 11120 11106 11105 11093 11062 11059 10950]\n",
      "Top recommended items for user 47:\n",
      "[38890 12009 12505 12488 12474 12441 12248 12202 12082 12080 12014 12012]\n",
      "Top recommended items for user 48:\n",
      "[38898 12659 13541 13418 13417 13415 13414 13391 13381 13358 13286 13234]\n",
      "Top recommended items for user 49:\n",
      "[38727 12435 12714 12674 12672 12666 12654 12653 12649 12643 12621 12581]\n",
      "Top recommended items for user 50:\n",
      "[38903 13074 12621 12643 12649 12653 12654 12678 12679 12680 12681 12774]\n",
      "Top recommended items for user 51:\n",
      "[38872 10614 10950 10811 10791 10788 10739 10686 10684 10647 10641 10625]\n",
      "Top recommended items for user 52:\n",
      "[38869 14219 14597 14596 14568 14552 14543 14477 14428 14419 14354 14352]\n",
      "Top recommended items for user 53:\n",
      "[38857 11210 11822 11821 11645 11559 11552 11516 11483 11478 11389 11375]\n",
      "Top recommended items for user 54:\n",
      "[38673  9952 10371 10370 10368 10366 10365 10333 10209 10207 10194 10108]\n",
      "Top recommended items for user 55:\n",
      "[38900 12711 13541 13418 13417 13415 13414 13408 13381 13320 13223 13009]\n",
      "Top recommended items for user 56:\n",
      "[38754 14132 13541 13542 13543 13610 13655 13738 13802 13812 13871 13926]\n",
      "Top recommended items for user 57:\n",
      "[37541 11997 11822 11821 11736 11645 11552 11499 11487 11485 11480 11478]\n",
      "Top recommended items for user 58:\n",
      "[38641 11645 12202 12022 12014 12012 12010 12009 12008 11997 11975 11933]\n",
      "Top recommended items for user 59:\n",
      "[38785 12774 12658 12654 12653 12650 12649 12647 12646 12643 12621 12507]\n",
      "Top recommended items for user 60:\n",
      "[38434 11737 12116 12014 12012 12010 12009 12008 11997 11975 11930 11822]\n",
      "Top recommended items for user 61:\n",
      "[38463 12653 11573 11552 11521 11483 11480 11424 11338 11337 11280 11219]\n",
      "Top recommended items for user 62:\n",
      "[38912 14966 14132 14062 14036 14035 14008 14007 14006 13980 13966 13933]\n",
      "Top recommended items for user 63:\n",
      "[38259  9109  8939  8849  8669  8616  8582  8575  8568  8567  8544  8528]\n",
      "Top recommended items for user 64:\n",
      "[38823 13414 11552 11540 11374 11354 11331 11280 11220 11219 11210 11130]\n",
      "Top recommended items for user 65:\n",
      "[37599 11130 11105 11093 11062 11059 10950 10930 10921 10811 10772 10739]\n",
      "Top recommended items for user 66:\n",
      "[38908 14631 14597 14596 14568 14547 14446 14419 14417 14415 14413 14354]\n",
      "Top recommended items for user 67:\n",
      "[38554 10370 10866 10811 10739 10686 10684 10625 10619 10617 10616 10568]\n",
      "Top recommended items for user 68:\n",
      "[38556 14419 15192 15064 15062 14999 14967 14966 14949 14948 14947 14945]\n",
      "Top recommended items for user 69:\n",
      "[37234  9314 10076  9995  9952  9943  9782  9779  9765  9639  9638  9629]\n",
      "Top recommended items for user 70:\n",
      "[38907 12643 12507 12464 12202 12137 12014 12012 12010 12009 12008 11997]\n",
      "Top recommended items for user 71:\n",
      "[38869 13476 13412 13414 13415 13417 13418 13440 13532 13387 13541 13542]\n",
      "Top recommended items for user 72:\n",
      "[38859 14216 14673 14807 14937 14939 14941 14945 14947 14948 14949 14960]\n",
      "Top recommended items for user 73:\n",
      "[38891 15192 15197 15201 15212 15218 15233 15291 15306 15350 15482 15529]\n",
      "Top recommended items for user 74:\n",
      "[38789 13539 13812 13811 13810 13802 13730 13728 13685 13655 13610 13609]\n",
      "Top recommended items for user 75:\n",
      "[38824 11093 12012 12010 12009 12008 11997 11975 11940 11870 11822 11821]\n",
      "Top recommended items for user 76:\n",
      "[38540 14132 12653 12649 12643 12621 12507 12502 12479 12474 12422 12368]\n",
      "Top recommended items for user 77:\n",
      "[38913 16326 15937 15981 16118 16142 16199 16212 16214 16239 16329 17696]\n",
      "Top recommended items for user 78:\n",
      "[38753 13007 13542 13541 13493 13418 13417 13415 13414 13381 13045 13044]\n",
      "Top recommended items for user 79:\n",
      "[38910 14671 15482 15363 15360 15351 15291 15233 15212 15204 15201 15197]\n",
      "Top recommended items for user 80:\n",
      "[38866 14484 14035 14062 14117 14132 14135 14155 14165 14166 14219 14297]\n",
      "Top recommended items for user 81:\n",
      "[38895 15398 14356 14382 14385 14390 14392 14396 14419 14568 14570 14596]\n",
      "Top recommended items for user 82:\n",
      "[38808 11131 11947 11822 11821 11645 11552 11518 11393 11280 11219 11210]\n",
      "Top recommended items for user 83:\n",
      "[37234 10370 10366 10209 10207 10184  9952  9943  9829  9579  9513  9441]\n",
      "Top recommended items for user 84:\n",
      "[38839 13993 13417 13418 13541 13542 13543 13606 13610 13655 13802 13926]\n",
      "Top recommended items for user 85:\n",
      "[38811 14568 14419 14387 14354 14352 14286 14219 14180 14166 14165 14132]\n",
      "Top recommended items for user 86:\n",
      "[38746 12925 13802 13655 13610 13543 13542 13541 13418 13417 13415 13414]\n",
      "Top recommended items for user 87:\n",
      "[38864 14035 14937 14894 14893 14807 14763 14715 14638 14637 14631 14608]\n",
      "Top recommended items for user 88:\n",
      "[38866 15482 14596 14568 14494 14419 14379 14378 14375 14361 14354 14352]\n",
      "Top recommended items for user 89:\n",
      "[38861 10857 10697 10724 10739 10785 10811 10851 10860 10686 10880 10950]\n",
      "Top recommended items for user 90:\n",
      "[38913 15212 15062 15064 15192 15193 15197 15201 15233 14999 15291 15307]\n",
      "Top recommended items for user 91:\n",
      "[38828 14311 14165 14166 14205 14213 14219 14220 14319 14132 14322 14352]\n",
      "Top recommended items for user 92:\n",
      "[38680 11230 11975 11822 11821 11656 11645 11580 11552 11413 11412 11280]\n",
      "Top recommended items for user 93:\n",
      "[38736 10232 10208 10207 10122  9994  9952  9943  9808  9774  9643  9579]\n",
      "Top recommended items for user 94:\n",
      "[38878 11210 11822 11821 11751 11749 11670 11645 11566 11552 11413 11280]\n",
      "Top recommended items for user 95:\n",
      "[38880 14638 14999 14989 14967 14966 14949 14948 14947 14945 14941 14939]\n",
      "Top recommended items for user 96:\n",
      "[38891 21811 14352 14354 14370 14419 14509 14568 14596 14597 14608 14611]\n",
      "Top recommended items for user 97:\n",
      "[38640 12654 13926 13802 13655 13610 13543 13542 13541 13418 13417 13415]\n",
      "Top recommended items for user 98:\n",
      "[38586 12139 12006 12008 12009 12010 12012 12014 12138 12141 11975 12195]\n",
      "Top recommended items for user 99:\n",
      "[38516 11645 11280 11437 11441 11445 11552 11606 11646 11975 11747 11778]\n",
      "Top recommended items for user 100:\n",
      "[38863 12008 12649 12643 12621 12507 12202 12014 12012 12010 12009 11998]\n",
      "Top recommended items for user 101:\n",
      "[38880 14937 14354 14355 14419 14430 14431 14468 14568 14596 14597 14608]\n",
      "Top recommended items for user 102:\n",
      "[38762 12042 12012 12010 12009 12008 12003 11997 11975 11970 11822 11821]\n",
      "Top recommended items for user 103:\n",
      "[37234  5330  7956  7949  7850  7738  7737  7526  7326  7307  7153  7152]\n",
      "Top recommended items for user 104:\n",
      "[38557 10235 10619 10617 10616 10494 10423 10390 10371 10370 10368 10366]\n",
      "Top recommended items for user 105:\n",
      "[38898 11550 12925 12774 12714 12654 12653 12649 12643 12621 12507 12470]\n",
      "Top recommended items for user 106:\n",
      "[38588 12925 13542 13541 13418 13417 13415 13414 13381 13052 13051 13003]\n",
      "Top recommended items for user 107:\n",
      "[38000 14352 13190 13166 13056 12933 12925 12774 12759 12654 12653 12649]\n",
      "Top recommended items for user 108:\n",
      "[38632 12014 12753 12752 12751 12654 12653 12649 12643 12621 12507 12202]\n",
      "Top recommended items for user 109:\n",
      "[38860 13543 11552 11645 11672 11821 11822 11899 11900 11901 11975 11983]\n",
      "Top recommended items for user 110:\n",
      "[38354 12202 12014 12012 12010 12009 12008 11997 11975 11931 11928 11842]\n",
      "Top recommended items for user 111:\n",
      "[38820 12008 11975 11925 11924 11882 11881 11880 11822 11821 11645 11552]\n",
      "Top recommended items for user 112:\n",
      "[38745 14967 14062 14035 14024 14008 14007 13966 13933 13926 13802 13655]\n",
      "Top recommended items for user 113:\n",
      "[38903 12280 13716 13655 13610 13543 13542 13541 13453 13420 13418 13417]\n",
      "Top recommended items for user 114:\n",
      "[37234 10207 10616 10371 10370 10368 10366 10209 10164  9849 10135 10107]\n",
      "Top recommended items for user 115:\n",
      "[38576  9109 10207 10162  9952  9943  9851  9827  9726  9645  9622  9579]\n",
      "Top recommended items for user 116:\n",
      "[38809 12925 13417 13415 13414 13381 13371 13306 13297 13214 13190 13058]\n",
      "Top recommended items for user 117:\n",
      "[38700  8282 10368 10366 10209 10207 10044  9952  9943  9579  9458  9440]\n",
      "Top recommended items for user 118:\n",
      "[38659 13577 13933 13926 13888 13844 13842 13840 13802 13655 13610 13584]\n",
      "Top recommended items for user 119:\n",
      "[37386 10617 10920 10829 10828 10817 10811 10745 10739 10696 10686 10684]\n",
      "Top recommended items for user 120:\n",
      "[38869 14597 14568 14521 14419 14392 14356 14355 14354 14353 14352 14278]\n",
      "Top recommended items for user 121:\n",
      "[38187 12507 12293 12267 12202 12102 12027 12014 12012 12010 12009 12008]\n",
      "Top recommended items for user 122:\n",
      "[38102 10968 10882 10860 10859 10857 10856 10854 10852 10851 10828 10811]\n",
      "Top recommended items for user 123:\n",
      "[38883 14062 14425 14419 14412 14354 14352 14219 14183 14166 14165 14132]\n",
      "Top recommended items for user 124:\n",
      "[38784  8205 11145 11130 11105 11093 11071 11062 11059 11053 10950 10840]\n",
      "Top recommended items for user 125:\n",
      "[38068 14166 14132 14131 14130 14062 14035 14012 14009 14008 14007 13966]\n",
      "Top recommended items for user 126:\n",
      "[38855 14839 15064 15062 14999 14967 14966 14949 14948 14947 14945 14941]\n",
      "Top recommended items for user 127:\n",
      "[38329 12429 12774 12772 12729 12728 12654 12653 12649 12643 12621 12507]\n",
      "Top recommended items for user 128:\n",
      "[38898 14608 14354 14419 14421 14512 14539 14568 14596 14597 14631 14343]\n",
      "Top recommended items for user 129:\n",
      "[38880 14597 14631 14645 14658 14807 14821 14937 14939 14941 14945 14947]\n",
      "Top recommended items for user 130:\n",
      "[38303 10625 10617 10616 10457 10440 10397 10371 10370 10368 10366 10209]\n",
      "Top recommended items for user 131:\n",
      "[38880 13880 14132 14126 14113 14110 14109 14062 14035 14008 14007 13966]\n",
      "Top recommended items for user 132:\n",
      "[37234 11389 12008 11997 11975 11831 11822 11821 11802 11645 11643 11642]\n",
      "Top recommended items for user 133:\n",
      "[38858 14597 13570 13543 13542 13541 13524 13523 13486 13418 13417 13415]\n",
      "Top recommended items for user 134:\n",
      "[38349 13296 12049 12014 12012 12010 12009 12008 11997 11975 11942 11822]\n",
      "Top recommended items for user 135:\n",
      "[38860 10738 11130 11116 11112 11105 11093 11066 11062 11059 10978 10950]\n",
      "Top recommended items for user 136:\n",
      "[38916 12012 12643 12621 12523 12508 12507 12480 12477 12474 12424 12344]\n",
      "Top recommended items for user 137:\n",
      "[38354 14945 14939 14937 14888 14807 14712 14663 14631 14608 14597 14596]\n",
      "Top recommended items for user 138:\n",
      "[38689 13418 11997 11984 11979 11975 11919 11822 11821 11820 11783 11772]\n",
      "Top recommended items for user 139:\n",
      "[38869 13933 13917 13802 13785 13655 13610 13543 13542 13541 13418 13417]\n",
      "Top recommended items for user 140:\n",
      "[38573 12684 13270 13266 13263 13206 13151 13150 13134 13133 13055 12968]\n",
      "Top recommended items for user 141:\n",
      "[37234 10356 10684 10625 10619 10617 10616 10603 10482 10415 10399 10371]\n",
      "Top recommended items for user 142:\n",
      "[38625 12649 13041 13012 13008 12978 12974 12973 12925 12782 12777 12774]\n",
      "Top recommended items for user 143:\n",
      "[38893 11280 11821 11782 11726 11645 11552 11426 11425 11423 11421 11362]\n",
      "Top recommended items for user 144:\n",
      "[38868 16440 16332 16342 16368 16420 16432 16433 16434 16437 16439 16442]\n",
      "Top recommended items for user 145:\n",
      "[38850 12774 13842 13837 13802 13657 13655 13649 13646 13633 13632 13631]\n",
      "Top recommended items for user 146:\n",
      "[38846 20367 20300 20153 20131 20100 20099 20098 20095 20088 20085 20082]\n",
      "Top recommended items for user 147:\n",
      "[38869 14145 14007 14008 14035 14062 14114 14132 14165 38640 14166 14219]\n",
      "Top recommended items for user 148:\n",
      "[38886 14166 15192 15135 15133 15064 15062 14999 14967 14966 14949 14948]\n",
      "Top recommended items for user 149:\n",
      "[38878 38827 13417 13415 13414 13392 13391 13385 13381 13303 13134 13088]\n",
      "Top recommended items for user 150:\n",
      "[38918 16432 16098 16103 16141 16233 16260 16261 16262 16288 16294 16368]\n",
      "Top recommended items for user 151:\n",
      "[38484 11062 10833 10865 10882 10950 10977 11048 11059 11093 10811 11105]\n",
      "Top recommended items for user 152:\n",
      "[38541 11113 11730 11717 11645 11552 11407 11280 11219 11210 11170 11160]\n",
      "Top recommended items for user 153:\n",
      "[38770 12591 13655 13610 13543 13542 13541 13486 13485 13454 13418 13417]\n",
      "Top recommended items for user 154:\n",
      "[38689 12966 13539 13447 13418 13417 13415 13414 13381 13301 13291 13116]\n",
      "Top recommended items for user 155:\n",
      "[38917 18020 17577 17628 17664 17665 17667 17689 17690 17692 17740 17741]\n",
      "Top recommended items for user 156:\n",
      "[38898 13605 13542 13541 13500 13418 13417 13416 13415 13414 13381 13288]\n",
      "Top recommended items for user 157:\n",
      "[38886 13379 13262 12925 12848 12841 12839 12818 12774 12654 12653 12649]\n",
      "Top recommended items for user 158:\n",
      "[38317 11093 12014 12012 12010 12009 12008 11997 11975 11849 11822 11821]\n",
      "Top recommended items for user 159:\n",
      "[38571 11552 12010 12009 12008 12003 11997 11975 11913 11822 11821 11793]\n",
      "Top recommended items for user 160:\n",
      "[38783 12192 12654 12653 12649 12643 12621 12611 12600 12507 12495 12202]\n",
      "Top recommended items for user 161:\n",
      "[38901 11909 11814 11816 11821 11822 11853 11854 11910 13542 11975 11987]\n",
      "Top recommended items for user 162:\n",
      "[38908 15928 16652 16510 16449 16442 16439 16437 16434 16433 16432 16368]\n",
      "Top recommended items for user 163:\n",
      "[38904 13418 13933 13926 13902 13802 13724 13655 13610 13543 13542 13541]\n",
      "Top recommended items for user 164:\n",
      "[38524 10853 11130 11120 11119 11118 11116 11105 11093 11062 11059 10982]\n",
      "Top recommended items for user 165:\n",
      "[38364 13414 11975 11925 11924 11872 11822 11821 11740 11645 11634 11552]\n",
      "Top recommended items for user 166:\n",
      "[38899 16192 15229 15233 15272 15291 15341 15343 15482 15529 15530 15624]\n",
      "Top recommended items for user 167:\n",
      "[38869 14941 15103 15064 15062 14999 14979 14970 14967 14966 14949 14948]\n",
      "Top recommended items for user 168:\n",
      "[38800 12012 12009 12008 11997 11975 11974 11945 11822 11821 11752 11751]\n",
      "Top recommended items for user 169:\n",
      "[38907 11645 11490 11546 11549 11552 11565 11634 11662  7738 11713 11771]\n",
      "Top recommended items for user 170:\n",
      "[38890 13548 13933 13926 13844 13843 13840 13838 13802 13762 13661 13655]\n",
      "Top recommended items for user 171:\n",
      "[38693 38640 14947 14945 14943 14942 14941 14940 14939 14937 14909 14807]\n",
      "Top recommended items for user 172:\n",
      "[38580 13022 12653 12654 12774 12873 12925 12982 13023 13397 13024 13044]\n",
      "Top recommended items for user 173:\n",
      "[38891 17357 16855 16887 16888 17065 17080 17112 17156 17157 17231 17254]\n",
      "Top recommended items for user 174:\n",
      "[38918 13162 13728 13655 13610 13543 13542 13541 13533 13418 13417 13415]\n",
      "Top recommended items for user 175:\n",
      "[38570 13417 13066 13148 13367 13381 13414 13415 13418 14631 13451 13534]\n",
      "Top recommended items for user 176:\n",
      "[38907 14597 14992 14967 14966 14949 14948 14947 14945 14941 14939 14937]\n",
      "Top recommended items for user 177:\n",
      "[38576 13674 14007 13974 13969 13967 13966 13933 13926 13877 13802 13761]\n",
      "Top recommended items for user 178:\n",
      "[37234 11009 11219 11210 11130 11105 11096 11093 11062 11059 11036 11033]\n",
      "Top recommended items for user 179:\n",
      "[38389 38387 10209 10210 10213 10223 10224 10230 10337 10348 10349 10358]\n",
      "Top recommended items for user 180:\n",
      "[38079  8419 10100 10049  9991  9990  9952  9943  9929  9927  9671  9579]\n",
      "Top recommended items for user 181:\n",
      "[38887 13418 13415 13414 13403 13381 13365 13204 12925 12826 12778 12774]\n",
      "Top recommended items for user 182:\n",
      "[38725 14928 13381 13414 13415 13417 13418 13485 13501 13541 13542 13543]\n",
      "Top recommended items for user 183:\n",
      "[38869 14528 14939 14937 14843 14807 14631 14608 14597 14596 14588 14568]\n",
      "Top recommended items for user 184:\n",
      "[38908 14948 14937 14938 14939 14941 14945 14947 14949 14807 14966 14967]\n",
      "Top recommended items for user 185:\n",
      "[38876 15192 13933 13966 14007 14008 14035 14056 14062 14089 14132 14165]\n",
      "Top recommended items for user 186:\n",
      "[38689 13610 14419 14354 14352 14294 14280 14259 14219 14202 14166 14165]\n",
      "Top recommended items for user 187:\n",
      "[38613  9645 11062 11059 10950 10846 10811 10739 10686 10684 10625 10619]\n",
      "Top recommended items for user 188:\n",
      "[38895 12316 12854 12774 12734 12733 12654 12653 12649 12643 12621 12530]\n",
      "Top recommended items for user 189:\n",
      "[38821 12661 13160 13067 12925 12834 12802 12774 12724 12698 12664 12663]\n",
      "Top recommended items for user 190:\n",
      "[38738 12624 12602 12603 12604 12606 12620 12621 12643 12582 12649 12653]\n",
      "Top recommended items for user 191:\n",
      "[38142  8669  8603  8582  8575  8544  8542  8541  8528  8516  8508  8491]\n",
      "Top recommended items for user 192:\n",
      "[38909 14914 14992 14994 14995 14999 15000 15062 15064 15192 15193 15194]\n",
      "Top recommended items for user 193:\n",
      "[38826 13802 13677 13659 13655 13610 13602 13543 13542 13541 13445 13418]\n",
      "Top recommended items for user 194:\n",
      "[38916 12866 13403 13401 13396 13381 13373 13372 13292 13275 13189 13042]\n",
      "Top recommended items for user 195:\n",
      "[38810 12621 12570 12520 12507 12414 12413 12202 12014 12012 12010 12009]\n",
      "Top recommended items for user 196:\n",
      "[37700 12621 12786 12782 12775 12774 12655 12654 12653 12649 12643 12507]\n",
      "Top recommended items for user 197:\n",
      "[38555 12704 13668 13655 13650 13645 13632 13616 13610 13572 13569 13568]\n",
      "Top recommended items for user 198:\n",
      "[38671 12658 12656 12654 12653 12649 12647 12643 12621 12561 12507 12468]\n",
      "Top recommended items for user 199:\n",
      "[38769 11552 12593 12582 12507 12422 12419 12390 12389 12368 12352 12320]\n"
     ]
    }
   ],
   "source": [
    "for user_id in range(user_item_matrix.shape[0]):\n",
    "    user_candidate_pool = user_candidates[user_id]\n",
    "    item_probs = lgb_model.predict(user_candidate_pool)\n",
    "    top_items = user_candidate_pool.iloc[item_probs.argsort()[::-1][:12]]\n",
    "    print(f\"Top recommended items for user {user_id}:\")\n",
    "    print(top_items['item_index'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
