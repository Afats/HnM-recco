{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<a id=\"Content\">HnM RecSys Notebook 9417</a>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": false,
    "tags": []
   },
   "source": [
    "## **<a id=\"Content\">Table of Contents</a>**\n",
    "* [**<span>1. Imports</span>**](#Imports)  \n",
    "* [**<span>2. Helper Functions/Decorators</span>**](#Helper-Functions)\n",
    "* [**<span>5. LightGBM Model</span>**](#LightGBM-Model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# only use last x weeks of transactions data since data is too large\n",
    "def filter_transactions_last_x_weeks(transactions, x = 10):\n",
    "    # Convert date strings to datetime objects\n",
    "    transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "\n",
    "    # Calculate the date x weeks ago from the latest transaction date\n",
    "    latest_date = transactions['t_dat'].max()\n",
    "    cutoff_date = latest_date - timedelta(weeks=x)\n",
    "\n",
    "    # Filter transactions to only include those in the last x weeks\n",
    "    filtered_transactions = transactions.loc[transactions['t_dat'] >= cutoff_date].copy()\n",
    "\n",
    "    return filtered_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_customers_and_articles(customers, articles, filtered_transactions):\n",
    "    # Get unique customer and article IDs from filtered transactions\n",
    "    customer_ids = filtered_transactions['customer_id'].unique()\n",
    "    article_ids = filtered_transactions['article_id'].unique()\n",
    "\n",
    "    # Filter customers and articles to only include those in filtered transactions\n",
    "    customers_filtered = customers.loc[customers['customer_id'].isin(customer_ids)].copy()\n",
    "    articles_filtered = articles.loc[articles['article_id'].isin(article_ids)].copy()\n",
    "\n",
    "    return customers_filtered, articles_filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison of the top GBDT models today. LightGBM is the fastest to train."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|LightGBM|XGBoost|CatBoost|\n",
    "|:----|:----|:----|:----|\n",
    "|Categoricals|Supports categorical features via one-hot encoding|Supports categorical features via one-hot encoding|Automatically handles categorical features using embeddings|\n",
    "|Speed|Very fast training and prediction|Fast training and prediction|Slower than LightGBM and XGBoost|\n",
    "|Handling Bias|Handles unbalanced classes via 'is_unbalance'|Handles unbalanced classes via 'scale_pos_weight'|Automatically handles unbalanced classes|\n",
    "|Handling NaNs|Handles NaN values natively|Requires manual handling of NaNs|Automatically handles NaN values using special category|\n",
    "|Custom Loss|Supports custom loss functions|Supports custom loss functions|Supports custom loss functions|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LightGBM for a ranking problem, we treat this as a binary classification problem where the target variable is whether an item is relevant or not to the user.\n",
    "\n",
    "Alternatively, we can use LightGBM's ranking API, which is designed for ranking problems. Instead of optimizing for accuracy, the ranking API optimizes for ranking metric MAP (MAP support deprecated however). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM imports\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# open user_item_matrix_200\n",
    "with open('user_item_matrix_200.pkl', 'rb') as f:\n",
    "    user_item_matrix = pickle.load(f)\n",
    "\n",
    "# open customer and articels incides map\n",
    "with open('lightgbm/customer_id_indices_map.pkl', 'rb') as f:\n",
    "    customer_id_indices_map = pickle.load(f)\n",
    "\n",
    "with open('lightgbm/article_id_indices_map.pkl', 'rb') as f:\n",
    "    article_id_indices_map = pickle.load(f)\n",
    "\n",
    "# load df from pickle file for time-based split\n",
    "with open('lightgbm/df.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# load final_df from pickle file for clean processing\n",
    "with open('lightgbm/final_df_with_binary_targets.pkl', 'rb') as f:\n",
    "    final_df = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_1</th>\n",
       "      <th>sales_channel_2</th>\n",
       "      <th>quantity</th>\n",
       "      <th>article_engagement_ratio</th>\n",
       "      <th>user_index</th>\n",
       "      <th>item_index</th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>club_member_status</th>\n",
       "      <th>...</th>\n",
       "      <th>garment_group_no_1019.0</th>\n",
       "      <th>garment_group_no_1020.0</th>\n",
       "      <th>garment_group_no_1021.0</th>\n",
       "      <th>garment_group_no_1023.0</th>\n",
       "      <th>garment_group_no_1025.0</th>\n",
       "      <th>index_group_no_1.0</th>\n",
       "      <th>index_group_no_2.0</th>\n",
       "      <th>index_group_no_3.0</th>\n",
       "      <th>index_group_no_4.0</th>\n",
       "      <th>index_group_no_26.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042358</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>11563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050842</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>9899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067810</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>14438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>10307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>10</td>\n",
       "      <td>13608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  sales_channel_1  sales_channel_2  quantity   \n",
       "0  0.042358            False             True       1.0  \\\n",
       "1  0.050842            False             True       1.0   \n",
       "2  0.067810            False             True       1.0   \n",
       "3  0.016937            False             True       1.0   \n",
       "4  0.016937            False             True       1.0   \n",
       "\n",
       "   article_engagement_ratio  user_index  item_index   FN  Active   \n",
       "0                  1.000000           5       11563  1.0     1.0  \\\n",
       "1                  1.000000           5        9899  1.0     1.0   \n",
       "2                  1.000000           5       14438  1.0     1.0   \n",
       "3                  0.500000          10       10307  0.0     0.0   \n",
       "4                  0.166667          10       13608  0.0     0.0   \n",
       "\n",
       "   club_member_status  ...  garment_group_no_1019.0  garment_group_no_1020.0   \n",
       "0                 2.0  ...                    False                    False  \\\n",
       "1                 2.0  ...                    False                    False   \n",
       "2                 2.0  ...                    False                    False   \n",
       "3                 2.0  ...                    False                    False   \n",
       "4                 2.0  ...                    False                    False   \n",
       "\n",
       "   garment_group_no_1021.0  garment_group_no_1023.0  garment_group_no_1025.0   \n",
       "0                    False                    False                    False  \\\n",
       "1                    False                    False                    False   \n",
       "2                    False                    False                    False   \n",
       "3                    False                    False                    False   \n",
       "4                    False                     True                    False   \n",
       "\n",
       "   index_group_no_1.0  index_group_no_2.0  index_group_no_3.0   \n",
       "0                True               False               False  \\\n",
       "1                True               False               False   \n",
       "2                True               False               False   \n",
       "3               False                True               False   \n",
       "4                True               False               False   \n",
       "\n",
       "   index_group_no_4.0  index_group_no_26.0  \n",
       "0               False                False  \n",
       "1               False                False  \n",
       "2               False                False  \n",
       "3               False                False  \n",
       "4               False                False  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # target encoding\n",
    "# from category_encoders import TargetEncoder\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# # Define columns to target encode\n",
    "# cols_to_encode = ['department_no', 'product_type_no', 'section_no', 'graphical_appearance_no']\n",
    "\n",
    "# # Define number of folds for cross-validation\n",
    "# n_splits = 5\n",
    "\n",
    "# # Create KFold object for cross-validation\n",
    "# kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# # Perform target encoding with cross-validation\n",
    "# for col in cols_to_encode:\n",
    "#     final_df[f'{col}_te'] = 0\n",
    "#     te = TargetEncoder(cols=[col])\n",
    "#     for train_idx, val_idx in kf.split(final_df):\n",
    "#         te.fit(final_df.iloc[train_idx][[col]], final_df.iloc[train_idx]['target'])\n",
    "#         final_df.loc[val_idx, f'{col}_te'] = te.transform(final_df.iloc[val_idx][[col]]).values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- memory optimizations -------------\n",
    "\n",
    "# reference: https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\n",
    "\n",
    "# iterate through all the columns of a dataframe and reduce the int and float data types to the smallest possible size, ex. customer_id should not be reduced from int64 to a samller value as it would have collisions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"Iterate over all the columns of a DataFrame and modify the data type\n",
    "    to reduce memory usage, handling ordered Categoricals\"\"\"\n",
    "    \n",
    "    # check the memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type == 'category':\n",
    "            if df[col].cat.ordered:\n",
    "                # Convert ordered Categorical to an integer\n",
    "                df[col] = df[col].cat.codes.astype('int16')\n",
    "            else:\n",
    "                # Convert unordered Categorical to a string\n",
    "                df[col] = df[col].astype('str')\n",
    "        \n",
    "        elif col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    # check the memory usage after optimization\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "\n",
    "    # calculate the percentage of the memory usage reduction\n",
    "    mem_reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(\"Memory usage decreased by {:.1f}%\".format(mem_reduction))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1952211, 56)\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# only get top 50 customers by number of total pruchase quantity from final_df\n",
    "\n",
    "# Compute the total quantity for each user_index\n",
    "user_quantity = final_df.groupby('user_index')['quantity'].sum()\n",
    "\n",
    "# Get the top 50 user_indices by total quantity\n",
    "top_50_users = user_quantity.nlargest(50).index\n",
    "\n",
    "# Filter the final_df to include only the data for the top 50 users\n",
    "final_df_top_50 = final_df[final_df['user_index'].isin(top_50_users)].copy()\n",
    "# print the shape of final_df_top_50\n",
    "print(final_df_top_50.shape)\n",
    "\n",
    "print(final_df_top_50['user_index'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_train_test_split(final_df, test_size=0.2):\n",
    "\n",
    "    # Convert days, months, and years columns to datetime object\n",
    "    final_df['date'] = pd.to_datetime(final_df[['day', 'month', 'year']])\n",
    "\n",
    "    # Sort dataframe by date in ascending order\n",
    "    final_df = final_df.sort_values(by='date')\n",
    "\n",
    "    # Calculate cutoff index\n",
    "    cutoff_index = int(len(final_df) * (1-test_size))\n",
    "\n",
    "    # Create train and test dataframes\n",
    "    train_df = final_df[:cutoff_index]\n",
    "    test_df = final_df[cutoff_index:]\n",
    "\n",
    "    # Drop date column from train and test dataframes\n",
    "    train_df = train_df.drop('date', axis=1)\n",
    "    test_df = test_df.drop('date', axis=1)\n",
    "\n",
    "    # split train_df into X_train and y_train\n",
    "    X_train = train_df.drop('target', axis=1)\n",
    "    y_train = train_df['target']\n",
    "\n",
    "    # split test_df into X_test and y_test\n",
    "    X_test = test_df.drop('target', axis=1)\n",
    "    y_test = test_df['target']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1393.06 MB\n",
      "Memory usage after optimization is: 726.30 MB\n",
      "Memory usage decreased by 47.9%\n",
      "Memory usage of dataframe is 348.27 MB\n",
      "Memory usage after optimization is: 401.84 MB\n",
      "Memory usage decreased by -15.4%\n",
      "(6242440, 55)\n",
      "(1560611, 55)\n",
      "(6242440,)\n",
      "(1560611,)\n"
     ]
    }
   ],
   "source": [
    "# 80/20 time-based split to curb data leakage\n",
    "X_train, X_test, y_train, y_test = time_based_train_test_split(final_df, test_size=0.2)\n",
    "# final_df_top_50 = final_df_top_50.drop('date', axis=1)\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(final_df.drop(['target'], axis=1), final_df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# redcue memory usage\n",
    "X_train = reduce_mem_usage(X_train)\n",
    "X_test = reduce_mem_usage(X_test)\n",
    "\n",
    "# print the shape of X_train, X_test, y_train, y_test\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "import joblib\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = final_df.columns.tolist()\n",
    "features.remove('target')\n",
    "target = 'target'\n",
    "\n",
    "# Group data by user -- so that LightGBM knows which data points belong to each user and can compute the metrics correctly\n",
    "grouped_data_train = X_train.groupby('user_index')\n",
    "grouped_data_test = X_test.groupby('user_index')\n",
    "groups_train = [grouped_data_train.groups[user] for user in grouped_data_train.groups.keys()]\n",
    "groups_train_flat = np.concatenate(groups_train)\n",
    "groups_test = [grouped_data_test.groups[user] for user in grouped_data_test.groups.keys()]\n",
    "\n",
    "# Create LightGBM datasets with group query information\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=grouped_data_train.groups.values())\n",
    "test_data = lgb.Dataset(X_test, label=y_test, group=grouped_data_test.groups.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RFECV(cv=5, estimator=LGBMClassifier(max_depth=7),\n",
       "      scoring=make_scorer(average_precision_score, needs_threshold=True),\n",
       "      verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RFECV</label><div class=\"sk-toggleable__content\"><pre>RFECV(cv=5, estimator=LGBMClassifier(max_depth=7),\n",
       "      scoring=make_scorer(average_precision_score, needs_threshold=True),\n",
       "      verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(max_depth=7)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(max_depth=7)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RFECV(cv=5, estimator=LGBMClassifier(max_depth=7),\n",
       "      scoring=make_scorer(average_precision_score, needs_threshold=True),\n",
       "      verbose=1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Feature selection using RFECV\n",
    "selector = RFECV(\n",
    "    estimator=lgb.LGBMClassifier(n_jobs=-1,\n",
    "        num_leaves=31, max_depth=7, learning_rate=0.1\n",
    "    ),\n",
    "    cv=5, scoring=get_scorer('average_precision'),\n",
    "    verbose=1, step=1\n",
    ")\n",
    "selector.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['price'], dtype='object')\n",
      "[('price', 1), ('user_index', 2), ('item_index', 3), ('article_engagement_ratio', 4), ('user_purchase_quant', 5), ('department_no', 6), ('mean_purchase_age', 7), ('time_diff_days', 8), ('quantity', 9), ('item_avg_price_level', 10), ('product_type_no', 11), ('club_member_status', 12), ('section_no', 13), ('age', 14), ('item_purchase_frequency', 15), ('min_purchase_age', 16), ('day', 17), ('Active', 18), ('FN', 19), ('sales_channel_2', 20), ('sales_channel_1', 21), ('fashion_news_frequency', 22), ('RFM_Score', 23), ('garment_group_no_1001.0', 24), ('garment_group_no_1002.0', 25), ('garment_group_no_1003.0', 26), ('garment_group_no_1005.0', 27), ('garment_group_no_1006.0', 28), ('garment_group_no_1007.0', 29), ('garment_group_no_1008.0', 30), ('garment_group_no_1009.0', 31), ('graphical_appearance_no', 32), ('garment_group_no_1010.0', 33), ('garment_group_no_1011.0', 34), ('garment_group_no_1012.0', 35), ('garment_group_no_1013.0', 36), ('garment_group_no_1014.0', 37), ('age_diff', 38), ('garment_group_no_1016.0', 39), ('garment_group_no_1017.0', 40), ('garment_group_no_1018.0', 41), ('max_purchase_age', 42), ('garment_group_no_1019.0', 43), ('garment_group_no_1020.0', 44), ('garment_group_no_1021.0', 45), ('article_preference', 46), ('garment_group_no_1023.0', 47), ('garment_group_no_1025.0', 48), ('index_group_no_1.0', 49), ('index_group_no_2.0', 50), ('index_group_no_3.0', 51), ('year', 52), ('index_group_no_4.0', 53), ('month', 54), ('index_group_no_26.0', 55)]\n"
     ]
    }
   ],
   "source": [
    "# Get selected features\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "print(selected_features)\n",
    "\n",
    "ranks = selector.ranking_\n",
    "feat_ranks = {feat:rank for feat, rank in zip(X_train.columns, ranks)}\n",
    "sorted_ranks = sorted(feat_ranks.items(), key=lambda x: x[1])\n",
    "print(sorted_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['price', 'quantity', 'article_engagement_ratio', 'user_index',\n",
      "       'item_index', 'time_diff_days', 'user_purchase_quant', 'department_no',\n",
      "       'mean_purchase_age', 'item_avg_price_level'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# # Get selected features\n",
    "# selected_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "selected_features = X_train.columns[(ranks == 1) | (ranks == 2) | ( ranks == 3)]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min date: 2018-09-20 00:00:00\n",
      "Max date: 2020-09-22 00:00:00\n"
     ]
    }
   ],
   "source": [
    "min_date = df['t_dat'].min()\n",
    "max_date = df['t_dat'].max()\n",
    "\n",
    "print('Min date:', min_date)\n",
    "print('Max date:', max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_split_date(df, split_percentage):\n",
    "    # Get min and max dates\n",
    "    min_date = df['t_dat'].min()\n",
    "    max_date = df['t_dat'].max()\n",
    "\n",
    "    # Calculate split date\n",
    "    split_date = min_date + pd.DateOffset(days=int((max_date - min_date).days * split_percentage))\n",
    "\n",
    "    return split_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split date: 2019-09-21 00:00:00\n"
     ]
    }
   ],
   "source": [
    "split_date = calculate_split_date(df, 0.50)\n",
    "print('Split date:', split_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# print number of unique target values in final_df after split date\n",
    "print(final_df[final_df['date'] >= split_date]['target'].nunique())\n",
    "print(final_df[final_df['date'] <= split_date]['target'].nunique())\n",
    "\n",
    "# print unique target values in final_df\n",
    "print(final_df['target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2182045566.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[124], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"Len of final df before split: \"len(final_df[final_df['date'] < split_date]))\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "print(final_df['target'].value_counts())\n",
    "\n",
    "# print len of final_df \n",
    "print(\"Len of final df: \", len(final_df))\n",
    "\n",
    "# print len of final_df before split date\n",
    "print(\"Len of final df before split: \", len(final_df[final_df['date'] < split_date]))\n",
    "\n",
    "# print len of final_df after split date\n",
    "print(\"Len of final df after split: \", len(final_df[final_df['date'] >= split_date]))\n",
    "\n",
    "\n",
    "print('Before split date:')\n",
    "print(final_df[final_df['date'] < split_date]['target'].value_counts())\n",
    "\n",
    "print('After split date:')\n",
    "print(final_df[final_df['date'] >= split_date]['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_1</th>\n",
       "      <th>sales_channel_2</th>\n",
       "      <th>quantity</th>\n",
       "      <th>article_engagement_ratio</th>\n",
       "      <th>user_index</th>\n",
       "      <th>item_index</th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>...</th>\n",
       "      <th>article_preference</th>\n",
       "      <th>item_purchase_frequency</th>\n",
       "      <th>item_avg_price_level</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>recency</th>\n",
       "      <th>frequency</th>\n",
       "      <th>monetary_value</th>\n",
       "      <th>RFM Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.042358</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>11563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042358</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>861</td>\n",
       "      <td>37.391350</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.030487</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>139</td>\n",
       "      <td>6231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>646</td>\n",
       "      <td>22.751427</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.025406</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>139</td>\n",
       "      <td>14568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.024521</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>646</td>\n",
       "      <td>22.751427</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>147</td>\n",
       "      <td>11460</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.031311</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>688</td>\n",
       "      <td>29.959660</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.031067</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>147</td>\n",
       "      <td>8942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.029251</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>688</td>\n",
       "      <td>29.959660</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         t_dat     price  sales_channel_1  sales_channel_2  quantity   \n",
       "0   2018-09-20  0.042358            False             True       1.0  \\\n",
       "134 2018-09-20  0.030487            False             True       1.0   \n",
       "135 2018-09-20  0.025406            False             True       1.0   \n",
       "136 2018-09-20  0.031052            False             True       1.0   \n",
       "137 2018-09-20  0.031067            False             True       1.0   \n",
       "\n",
       "     article_engagement_ratio  user_index  item_index   FN  Active  ...   \n",
       "0                    1.000000           5       11563  1.0     1.0  ...  \\\n",
       "134                  0.500000         139        6231  0.0     0.0  ...   \n",
       "135                  0.052632         139       14568  0.0     0.0  ...   \n",
       "136                  0.111111         147       11460  1.0     1.0  ...   \n",
       "137                  0.200000         147        8942  1.0     1.0  ...   \n",
       "\n",
       "     article_preference  item_purchase_frequency  item_avg_price_level  year   \n",
       "0                   0.0                 0.000000              0.042358  2018  \\\n",
       "134                 1.0                 1.000000              0.028198  2018   \n",
       "135                 1.0                 0.583333              0.024521  2018   \n",
       "136                 1.0                 1.200000              0.031311  2018   \n",
       "137                 1.0                 1.250000              0.029251  2018   \n",
       "\n",
       "     month  day  recency  frequency monetary_value  RFM Score  \n",
       "0        9   20        6        861      37.391350        244  \n",
       "134      9   20        6        646      22.751427        233  \n",
       "135      9   20        6        646      22.751427        233  \n",
       "136      9   20       10        688      29.959660        234  \n",
       "137      9   20       10        688      29.959660        234  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = df.sort_values(by='t_dat')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_1</th>\n",
       "      <th>sales_channel_2</th>\n",
       "      <th>quantity</th>\n",
       "      <th>article_engagement_ratio</th>\n",
       "      <th>user_index</th>\n",
       "      <th>item_index</th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>club_member_status</th>\n",
       "      <th>...</th>\n",
       "      <th>garment_group_no_1020.0</th>\n",
       "      <th>garment_group_no_1021.0</th>\n",
       "      <th>garment_group_no_1023.0</th>\n",
       "      <th>garment_group_no_1025.0</th>\n",
       "      <th>index_group_no_1.0</th>\n",
       "      <th>index_group_no_2.0</th>\n",
       "      <th>index_group_no_3.0</th>\n",
       "      <th>index_group_no_4.0</th>\n",
       "      <th>index_group_no_26.0</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042358</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>11563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067810</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>14438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>10307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>10</td>\n",
       "      <td>13608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.016937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>10</td>\n",
       "      <td>12935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  sales_channel_1  sales_channel_2  quantity   \n",
       "0  0.042358            False             True       1.0  \\\n",
       "2  0.067810            False             True       1.0   \n",
       "3  0.016937            False             True       1.0   \n",
       "4  0.016937            False             True       1.0   \n",
       "5  0.016937            False             True       1.0   \n",
       "\n",
       "   article_engagement_ratio  user_index  item_index   FN  Active   \n",
       "0                  1.000000           5       11563  1.0     1.0  \\\n",
       "2                  1.000000           5       14438  1.0     1.0   \n",
       "3                  0.500000          10       10307  0.0     0.0   \n",
       "4                  0.166667          10       13608  0.0     0.0   \n",
       "5                  0.333333          10       12935  0.0     0.0   \n",
       "\n",
       "   club_member_status  ...  garment_group_no_1020.0  garment_group_no_1021.0   \n",
       "0                 2.0  ...                    False                    False  \\\n",
       "2                 2.0  ...                    False                    False   \n",
       "3                 2.0  ...                    False                    False   \n",
       "4                 2.0  ...                    False                    False   \n",
       "5                 2.0  ...                    False                    False   \n",
       "\n",
       "   garment_group_no_1023.0  garment_group_no_1025.0  index_group_no_1.0   \n",
       "0                    False                    False                True  \\\n",
       "2                    False                    False                True   \n",
       "3                    False                    False               False   \n",
       "4                     True                    False                True   \n",
       "5                    False                    False               False   \n",
       "\n",
       "   index_group_no_2.0  index_group_no_3.0  index_group_no_4.0   \n",
       "0               False               False               False  \\\n",
       "2               False               False               False   \n",
       "3                True               False               False   \n",
       "4               False               False               False   \n",
       "5                True               False               False   \n",
       "\n",
       "   index_group_no_26.0       date  \n",
       "0                False 2018-09-20  \n",
       "2                False 2018-09-20  \n",
       "3                False 2018-09-20  \n",
       "4                False 2018-09-20  \n",
       "5                False 2018-09-20  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['date'] = pd.to_datetime(final_df[['day', 'month', 'year']])\n",
    "final_df = final_df.sort_values(by='date')\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_train_test_split3(final_df, test_date):\n",
    "    # Convert days, months, and years columns to datetime object\n",
    "    final_df['date'] = pd.to_datetime(final_df[['day', 'month', 'year']])\n",
    "\n",
    "    # Sort dataframe by date in ascending order\n",
    "    final_df = final_df.sort_values(by='date')\n",
    "\n",
    "    # Split dataframe into training and testing data\n",
    "    train_df = final_df[final_df['date'] < test_date]\n",
    "    test_df = final_df[final_df['date'] >= test_date]\n",
    "\n",
    "    # Drop date column from train and test dataframes\n",
    "    train_df = train_df.drop('date', axis=1)\n",
    "    test_df = test_df.drop('date', axis=1)\n",
    "\n",
    "    # split train_df into X_train and y_train\n",
    "    X_train = train_df.drop('target', axis=1)\n",
    "    y_train = train_df['target']\n",
    "\n",
    "    # split test_df into X_test and y_test\n",
    "    X_test = test_df.drop('target', axis=1)\n",
    "    y_test = test_df['target']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3, X_test3, y_train3, y_test3 = time_based_train_test_split3(final_df, split_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "[1. 0.]\n",
      "[0.]\n",
      "---\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "---\n",
      "[1.]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# X_train1, X_test1, y_train1, y_test1 = time_based_train_test_split(final_df, test_size=0.3)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(final_df.drop(['target'], axis=1), final_df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\"---\")\n",
    "print(y_train1.unique())\n",
    "print(y_test1.unique())\n",
    "\n",
    "print(\"---\")\n",
    "print(y_train2.unique())\n",
    "print(y_test2.unique())\n",
    "\n",
    "print(\"---\")\n",
    "print(y_train3.unique())\n",
    "print(y_test3.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0.0    7676429\n",
      "1.0     126622\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "0.0    6115818\n",
      "1.0     126622\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "0.0    1560611\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "0.0    6141050\n",
      "1.0     101390\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "0.0    1535379\n",
      "1.0      25232\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(final_df['target'].value_counts())\n",
    "print(y_train1.value_counts())\n",
    "print(y_test1.value_counts())\n",
    "print(y_train2.value_counts())\n",
    "print(y_test2.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: -0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmdoh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get integer indices of selected features\n",
    "selected_feature_indices = [X_train.columns.get_loc(col) for col in selected_features]\n",
    "\n",
    "# Train the LGBMClassifier using the selected features\n",
    "lgbm = lgb.LGBMClassifier(num_boost_round=100)\n",
    "lgbm.fit(X_train_scaled[:, selected_feature_indices], y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lgbm.predict_proba(X_test_scaled[:, selected_feature_indices])[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "average_precision = average_precision_score(y_test, y_pred)\n",
    "print(\"Average Precision Score:\", average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define hyperparameters space\n",
    "# param_dist = {\n",
    "#     'lgbm__learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'lgbm__num_leaves': [15, 31, 63],\n",
    "#     'lgbm__bagging_fraction': [0.6, 0.8, 1.0],\n",
    "#     'lgbm__feature_fraction': [0.6, 0.8, 1.0]\n",
    "#     # 'n_estimators': [100],\n",
    "#     # 'max_depth': [3, 5, -1]\n",
    "# }\n",
    "\n",
    "# # Perform grid search on the pipeline\n",
    "# clf = GridSearchCV(estimator=pipeline, param_grid=param_dist, cv=2, scoring=get_scorer('average_precision'), n_jobs=-1, verbose=2, refit=True, error_score='raise')\n",
    "# clf.fit(X_train, y_train, groups=groups_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best intermediate model\n",
    "joblib.dump(clf.best_estimator_, 'best_model.pkl')\n",
    "print(f'Best hyperparameters: {clf.best_params_}')\n",
    "print(f'Best map score: {clf.best_score_}')\n",
    "\n",
    "# Save the selected features\n",
    "selected_features = X_train_sel.columns.tolist()\n",
    "joblib.dump(selected_features, 'lightgbm/selected_features.pkl')\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = clf.best_estimator_.predict(X_test_sel, num_iteration=clf.best_estimator_.best_iteration_)\n",
    "ndcg = ndcg_score(y_test, y_pred, group_scores=True, verbose=1)\n",
    "print(f'NDCG score on test set: {ndcg}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, it can be used to predict the probability of purchase for new user-product pairs, which can be used to generate recommendations for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_popular_products(df, n_products=500):\n",
    "    # Group the dataframe by product and sum the quantity for each product\n",
    "    product_quantities = df.groupby('item_index')['quantity'].sum()\n",
    "    # Sort the products by quantity in descending order and select the top n_products\n",
    "    popular_products = product_quantities.sort_values(ascending=False).index.tolist()[:n_products]\n",
    "    # Filter the dataframe to only include the popular products\n",
    "    df = df[df['item_index'].isin(popular_products)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat this as a binary classification problem: After training the model, we can then get the probability that each user is likely to purchase an item from a candidate set of items. We can then sort these by descending probability to get the top 12 products as done below. <br>\n",
    "\n",
    "A heuristic apparoach that we use to enhance LighGBM predictions here: <br>\n",
    "1. Get a candidate set of top 500 most popular articles (by total purchase quanitity). <br>\n",
    "2. Include the customer's predicitons to this set. <br>\n",
    "3. Use lightGBM to predict the probability of purchases, and get the top 12. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_products' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Assume X is the input data for the LightGBM model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# X has a row for each user-product pair and a binary target indicating whether the user purchased the product or not\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m# This can be done using a combination of popular products and user purchase history\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# Let's assume we have a dictionary 'user_products' that maps each user ID to a list of products they've purchased\u001b[39;00m\n\u001b[0;32m     11\u001b[0m user_candidates \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfor\u001b[39;00m user_id \u001b[39min\u001b[39;00m user_products:\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Select the 600 most popular products\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     popular_products \u001b[39m=\u001b[39m select_popular_products(\u001b[39m500\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[39m# Add user purchase history to candidate list\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user_products' is not defined"
     ]
    }
   ],
   "source": [
    "# Assume X is the input data for the LightGBM model\n",
    "# X has a row for each user-product pair and a binary target indicating whether the user purchased the product or not\n",
    "\n",
    "# Train the LightGBM model on X\n",
    "# lgb_model = lgb.LGBMClassifier(**best_params)\n",
    "# lgb_model.fit(X, y)\n",
    "\n",
    "# Generate candidate products for each user\n",
    "# This can be done using a combination of popular products and user purchase history\n",
    "# Let's assume we have a dictionary 'user_products' that maps each user ID to a list of products they've purchased\n",
    "user_candidates = {}\n",
    "for user_id in user_products:\n",
    "    # Select the 600 most popular products\n",
    "    popular_products = select_popular_products(500)\n",
    "    \n",
    "    # Add user purchase history to candidate list\n",
    "    user_history = user_products[user_id]\n",
    "    candidate_products = list(set(popular_products + user_history))\n",
    "    \n",
    "    # Store candidate products for this user\n",
    "    user_candidates[user_id] = candidate_products\n",
    "\n",
    "# Predict probabilities of purchase for each candidate product for each user\n",
    "user_scores = {}\n",
    "for user_id, candidates in user_candidates.items():\n",
    "    # Create input data for this user\n",
    "    user_data = create_user_data(user_id, candidates)\n",
    "    \n",
    "    # Predict probabilities using the LightGBM model\n",
    "    scores = lgbm.predict_proba(user_data)[:, 1]\n",
    "    \n",
    "    # Store scores for this user\n",
    "    user_scores[user_id] = scores\n",
    "\n",
    "# Rank candidate products for each user and return top 12 as recommendations\n",
    "recommendations = {}\n",
    "for user_id, scores in user_scores.items():\n",
    "    # Sort candidate products by descending score\n",
    "    candidate_products = user_candidates[user_id]\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_products = [candidate_products[i] for i in sorted_indices]\n",
    "    \n",
    "    # Select top 12 products\n",
    "    top_products = sorted_products[:12]\n",
    "    \n",
    "    # Add user purchase history to top products\n",
    "    top_products += user_products[user_id]\n",
    "    \n",
    "    # Remove duplicates and return as recommendations\n",
    "    recommendations[user_id] = list(set(top_products))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using MAP as the evaluation metric, we could also use the LightGBM ranking API instead of the binary classification API. The code to rank article_ids using the lightgbm ranking API is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import make_scorer\n",
    "# from sklearn.metrics import average_precision_score\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# target = 'item_index'\n",
    "# features = final_df.columns.tolist()\n",
    "# features.remove(target)\n",
    "\n",
    "# # split the data into training and test sets -- can also do time-based split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(final_df[features], final_df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# # for number of items to rank for each user (group param for ordered ranking)\n",
    "# num_items_per_user = 12\n",
    "# user_indices = X_test.index.unique()\n",
    "# query = [num_items_per_user] * len(user_indices)\n",
    "# query_ids = []\n",
    "# for user_index in user_indices:\n",
    "#     user_indices_repeated = [user_index] * num_items_per_user\n",
    "#     query_ids.extend(user_indices_repeated)\n",
    "\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, group=query_ids)\n",
    "\n",
    "# # MAP@12 metric\n",
    "# def mean_average_precision(y_true, y_score, k=12):\n",
    "#     # get the indices of the top k scores\n",
    "#     top_k_indices = np.argsort(y_score)[::-1][:k]\n",
    "\n",
    "#     # calculate average precision at k\n",
    "#     return average_precision_score(y_true[top_k_indices], y_score[top_k_indices])\n",
    "\n",
    "# # define hyperparameters for tuning\n",
    "# params = {\n",
    "# 'objective': 'lambdarank', #using lightgbm ranking API\n",
    "# 'metric': 'MAP',\n",
    "# 'learning_rate': 0.05,\n",
    "# 'num_leaves': 31,\n",
    "# 'max_depth': 5,\n",
    "# 'min_data_in_leaf': 50,\n",
    "# 'feature_fraction': 0.8,\n",
    "# 'bagging_fraction': 0.8,\n",
    "# 'bagging_freq': 5\n",
    "# }\n",
    "\n",
    "# # create LightGBM model\n",
    "# model = lgb.LGBMRanker()\n",
    "\n",
    "# # perform grid search with cross-validation\n",
    "# param_grid = {\n",
    "# 'num_leaves': [31, 50, 75],\n",
    "# 'max_depth': [5, 7, -1],\n",
    "# 'min_data_in_leaf': [20, 50, 100],\n",
    "# 'feature_fraction': [0.6, 0.8, 1],\n",
    "# 'bagging_fraction': [0.6, 0.8, 1],\n",
    "# 'bagging_freq': [1, 3, 5],\n",
    "# 'lambda_l1': [0, 1, 2],\n",
    "# 'lambda_l2': [0, 1, 2]\n",
    "# }\n",
    "\n",
    "# best_map_score = 0.0\n",
    "# best_model = None\n",
    "\n",
    "# for params_dict in ParameterGrid(param_grid):\n",
    "#     params.update(params_dict)\n",
    "#     model = lgb.train(params, train_data)\n",
    "#     y_pred = model.predict(X_test, group=query)\n",
    "#     map_score = mean_average_precision(y_test, y_pred, k=12)\n",
    "#     if map_score > best_map_score:\n",
    "#         best_map_score = map_score\n",
    "#         best_model = model\n",
    "#         with open(f\"lightgbm/grid_search_model_{map_score:.4f}.pickle\", 'wb') as f:\n",
    "#             pickle.dump(model, f)\n",
    "\n",
    "# # save the best model\n",
    "# if not os.path.exists('lightgbm'):\n",
    "#     os.makedirs('lightgbm')\n",
    "# with open('lightgbm/best_model.pickle', 'wb') as f:\n",
    "#     pickle.dump(best_model, f)\n",
    "\n",
    "# # print the best MAP score\n",
    "# print(f\"Best mean average precision: {best_map_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
